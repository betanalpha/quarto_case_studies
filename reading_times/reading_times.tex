% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Stan

Program}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Clausal Inference},
  pdfauthor={Michael Betancourt},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Clausal Inference}
\author{Michael Betancourt}
\date{January 2025}

\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{3}
\tableofcontents
}
How do we parse, and ultimately understand, language? One way of
studying comprehension of language is to measure how long it takes for
individuals to read different types of writing. Distinct language
patterns that are particularly easy, or difficult, to parse will
manifest as systematic differences in those reading times. In this case
study we'll develop a Bayesian analysis of reading time data using
narratively generative modeling.

Our goal here is not to construct estimators of particular quantities or
make decisions about different quantities being the same. Rather our
objective is to model the entire data generating process, from the
latent cognitive behaviors of interest to the final, processed
observations. Given that model we can then use Bayes' Theorem to
directly infer which cognitive behaviors are most consistent with
observed data.

This case study will assume familiarity with
\href{https://betanalpha.github.io/assets/case_studies/modeling_and_inference.html}{probabilistic
modeling and Bayesian inference},
\href{https://betanalpha.github.io/assets/case_studies/generative_modeling.html}{narratively
generative modeling},
\href{https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html}{model
evaluation}, and
\href{https://betanalpha.github.io/assets/case_studies/stan_intro.html}{Stan}.
If your experience is limited then this analysis will likely be a bit
overwhelming, \emph{and that is absolutely okay}. In that case I
recommend treating this case study more as a demonstration of what is
possible given more experience with these tools and, ideally, a
motivation to study them.

\section{Modeling Reading Times}\label{modeling-reading-times}

As with most scientific endeavors, designing and modeling reading time
experiments requires care. For example, some individuals in in
experimental cohort might read faster than others. If faster readers are
given one text and slower readers another, then the differences we
observe may be due to not difference in the texts themselves but rather
these differences in the individuals. Separating individual pace, and
other sources of heterogeneity, from language complexity requires a
careful analysis.

\subsection{Pairwise Comparison
Models}\label{pairwise-comparison-models}

One particularly productive way to account for the complexities in an
experiment is to directly model the entire data generating process.
While this might appear to be an overwhelming task, it becomes much more
manageable if we start as simply as possible and then expand as
necessary.

How should we model reading times? Let's assume that each observation is
the result of one reader parsing a short piece of text, such as a word
or phrase, from a longer piece of text, such as a sentence or paragraph.
I will refer to this short piece as the \textbf{target text}.

The more complex the target text is in the context of the longer text,
the longer we would expect reading times to be. At the same time, a more
experienced reader would typically be able to achieve shorter reading
times than a less experienced reader.

We can model this contrast between an individual's reading skill and the
complexity of the assigned text with a bipartite
\href{https://betanalpha.github.io/assets/chapters_html/pairwise_comparison_modeling.html}{pairwise
comparison process}. Those unfamiliar with pairwise comparison modeling
more generally may have encountered the specific application of this
technique in \textbf{item response theory}.

A pairwise comparison model begins with an appropriate probabilistic
location model for the reading times, \[
p(t \mid \mu, \phi ).
\] Here the location parameter \(\mu\) configures the \emph{centrality}
of the reading times while \(\phi\) configures any other behaviors, such
as how the model concentrates around that central value. Some common
models that are compatible with positive, real-valued reading times
include the log normal model, the gamma model, and the inverse gamma
model. If you're curious about these models then take a look at
\href{https://betanalpha.github.io/assets/case_studies/probability_densities.html}{this
chapter}.

Once we have chosen a location model, we replace the location parameter
\(\mu\) by the output of a deterministic function, \[
\mu_{ij} = f( \alpha_{i} - \beta_{j} ).
\] The parameter \(\alpha_{i}\) quantifies the complexity of the \(i\)th
target text, the parameter \(\beta_{j}\) quantifies the experience of
\(j\)th reader, and the coupling function \(f\) determines exactly how
the difference in these parameters moderates \(\mu\). Different coupling
functions can be more or less useful in different applications.

To demonstrate this construction let's start with the log normal reading
time model, \[
\text{log-normal}(t \mid \log(\mu), \phi ),
\] with the location parameter \(\mu\) and scale parameter \(\phi\). We
can then elevate the log normal model to a pairwise comparison model by
introducing a coupling function, \begin{align*}
\mu_{ij}
&=
f( \alpha_{i} - \beta_{j} )
\\
&=
\exp \left(\eta + \alpha_{i} - \beta_{j} \right)
\\
&=
\exp \left(  \eta \right) \,
\exp \left( \alpha_{i} \right) \,
\frac{1}{ \exp \left( \beta_{j} \right) }.
\end{align*}

Conveniently, this coupling function factors into three multiplicative
terms. The leading term \(\exp(\eta)\) defines a baseline reading time
behavior which the complexity and experience parameters then scale up
and down depending on their sign. Given a particular reader, the more
complex a target text is the larger the difference \[
\alpha_{i} - \beta_{j}
\] will be. As the difference increases, the location \(\mu_{ij}\)
scales up from the baseline and the reading time model concentrates on
longer reading times.

\subsection{Capturing Systematic
Heterogeneity}\label{capturing-systematic-heterogeneity}

The goal of many linguistic experiments is to infer not the behavior of
individual texts but rather any behaviors that are \emph{shared} across
groups of texts, and the relationships between those shared behaviors.
For example, one common objective is to understand how different
language patterns influence reader comprehension regardless of
particular word choice.

Shared behaviors manifest as \emph{systematic} differences in the
observed reading times. These differences, however, take different forms
depending on the assumed data generating process. Because our
interaction with language is so complex, the possibilities here can be
overwhelming.

To make the discussion more manageable we'll have to narrow our scope to
a specific language pattern. Then we can consider how variants of that
pattern might result in heterogeneous reading time behaviors.

\subsubsection{Relative Clauses}\label{relative-clauses}

Consider a simple sentence with a subject, a verb, and an object, such
as

\begin{quote}
``The scientist built a model.''
\end{quote}

This sentence becomes more complex with the introduction of a
\textbf{relative clause} that endows the subject with additional
information. For example we might write

\begin{quote}
``The scientist who ignored the statistician built a model.''
\end{quote}

or

\begin{quote}
``The scientist whom the statistician ignored built model.''
\end{quote}

In the first sentence the subject of the sentence is also the subject of
the relative clause, making the relative clause
\textbf{subject-extracted}. Because the subject of the second sentence
is the \emph{object} of the relative clause, however, the relative
clause is \textbf{object-extracted}.

For more discussion of these different types of relative clauses see
Gibson and Wu (2013), Vasishth et al. (2017), Nicenboim and Vasishth
(2018), and Nicenboim, Schad, and Vasishth (2025).

We should be able to learn the relative difficulty in parsing these two
types of relative clauses by first engineering sentences that are the
same up to the behavior of the relative clause and then examining any
difference in the subsequent reading time behaviors. That said, even in
this narrow scope there are a myriad of ways in which the reading times
might behave differently.

Here we will consider two classes of models discussed in Vasishth et al.
(2017).

\subsubsection{Dependency Locality
Model}\label{dependency-locality-model}

The dependency locality model hypothesizes that the difficulty in
parsing a sentence is influenced by the distance between linguistically
related elements. Sentences in which related elements are separated by
unrelated elements are predicted to take longer to read than sentences
where the related elements appear next to each other.

If subject-extracted and object-extracted relative clauses exhibit
different localities then we would expect sentences with one to be
systematically more difficult to parse than sentences with the other.
Interestingly the type of relative clause that should be easier to parse
is not universal, but rather specific to particular languages.

We can incorporate dependency locality into our base reading time model
by introducing sentence complexity parameters for each type of relative
clause, \(\alpha_{i}^{\text{SR}}\) and \(\alpha_{i}^{\text{OR}}\). If \[
\alpha_{i}^{\text{SR}} > \alpha_{i}^{\text{OR}}
\] in a particular language, then subject-extracted relative clauses
will be more difficult to parse than object-extracted relative clauses.

When the influence of the relative clause structure is independent of
the rest of the sentence, we can expand these parameters into
text-specific baselines \(\alpha_{i}\) that are modified by relative
clause parameters, \begin{align*}
\alpha_{i}^{\text{SR}}
&=
\alpha_{i} + \gamma^{\text{SR}}
\\
\alpha_{i}^{\text{OR}}
&=
\alpha_{i} + \gamma^{\text{OR}}.
\end{align*}

In this case the relative clause parameters \(\gamma^{\text{SR}}\) and
\(\gamma^{\text{OR}}\) scale the location of the base reading time
model, \begin{align*}
\mu_{ij}^{\text{SR}}
&=
f( \alpha_{i}^{\text{SR}} - \beta_{j} )
\\
&=
\exp \left(  \eta
           + \alpha_{i}^{\text{SR}} - \beta_{j} \right)
\\
&=
\exp \left(  \gamma^{\text{SR}} + \eta
           + \alpha_{i} - \beta_{j} \right)
\\
&=
\exp \left( \gamma^{\text{SR}} \right) \,
\exp \left( \eta + \alpha_{i} - \beta_{j} \right)
\\
&=
\exp \left( \gamma^{\text{SR}} \right) \, \mu_{ij}
\end{align*} and \begin{align*}
\mu_{ij}^{\text{OR}}
&=
f( \alpha_{i}^{\text{OR}} - \beta_{j} )
\\
&=
\exp \left(  \gamma^{\text{OR}} + \eta
           + \alpha_{i} - \beta_{j} \right)
\\
&=
\exp \left( \gamma^{\text{OR}} \right) \,
\exp \left( \eta + \alpha_{i} - \beta_{j} \right)
\\
&=
\exp \left( \gamma^{\text{OR}} \right) \, \mu_{ij}
\end{align*}

\subsubsection{Direct-Access Model}\label{direct-access-model}

The direct-access model (Nicenboim and Vasishth 2018) posits that
reading comprehension is more of a trial and error process. In this
model readers parse text by forming an initial hypothesis for the
relationships between all of its elements. If these hypothesized
relationships are inconsistent, then the reader needs to spend time to
correct it. Text can take longer to parse not only because forming the
initial hypothesis might take longer but also because that initial
hypothesis might be less likely to be correct.

We can model the time it takes for a reader to form an initial
hypothesis using the same pairwise comparison modeling techniques that
we've already considered, \[
p_{1}( t_{ij} \mid \mu_{1, ij}, \phi_{1})
\] with \[
\mu_{1, ij}
=
\exp \left( \eta + \alpha_{i} - \beta_{j} \right).
\] Again more complex text slows the time of initial hypothesis
formation, while increased reader experience accelerates it.

Correcting an erroneous initial hypothesis takes more time. Here we'll
assume that the time to fix an initial hypothesis is proportional to the
time taken to form it in the first place, \[
p_{2}( t_{ij} \mid \mu_{2, ij}, \phi_{2})
\] with \begin{align*}
\mu_{2, ij}
=
\exp(\omega) \, \mu_{1, ij}
=
\exp \left( \eta + \omega + \alpha_{i} - \beta_{j} \right).
\end{align*} To ensure that an initial failure results in longer reading
times we'll need to restrict \(\omega\) to be positive.

When we observe only reading times, however, we will not know if a
reader's initial hypothesis was actually successful or not. Consequently
neither of these models is sufficient on their own!

To quantify our uncertainty about the success of the initial hypothesis
we have to combine both of these models into a single
\href{https://betanalpha.github.io/assets/chapters_html/mixture_modeling.html}{mixture
model}, \[
       \lambda  \, p_{1}( t_{ij} \mid \mu_{1, ij}, \phi_{1})
+ (1 - \lambda) \, p_{2}( t_{ij} \mid \mu_{2, ij}, \phi_{2}),
\] where \(\lambda\) quantifies the probability that the initial
hypothesis is correct. The more challenging a text is to parse
correctly, the smaller \(\lambda\) will be and the more weight will be
given to the longer reading times of the second component model.

In general \(\lambda\) could vary across readers and texts. Here,
however, we will assume that that any systematic heterogeneity in
\(\lambda\) is entirely due to the structure of the relative clause, \[
  \lambda_{\text{SR}}  \,
  p_{1}( t_{ij} \mid \mu_{1, ij}, \phi_{1})
+ (1 - \lambda_{\text{SR}}) \,
  p_{2}( t_{ij} \mid \mu_{2, ij}, \phi_{2})
\] and \[
  \lambda_{\text{OR}}  \,
  p_{1}( t_{ij} \mid \mu_{1, ij}, \phi_{1})
+ (1 - \lambda_{\text{OR}}) \,
  p_{2}( t_{ij} \mid \mu_{2, ij}, \phi_{2}).
\] If \[
\lambda_{\text{SR}} > \lambda_{\text{OR}}
\] then adding a subject-extracted relative clause to a sentence makes
it more difficult to parse correctly than adding an equivalent
object-extracted relative clause.

\subsubsection{¿Por Qué No Los Dos?}\label{sec:joint}

One of the beautiful features of probabilistic modeling is that we can
often combine separate models together into a consistent \emph{joint
model}. This allows us to, for example, directly infer which behaviors
from each model are more or less consistent with any observed data.

The dependency locality and direct-access models are not mutually
exclusive; indeed integrating them into a consistent joint model is
relatively straightforward. All we need to do is introduce the
dependency locality model's scaling of the locality parameter to our
direct-access mixture model.

For passages with subject-extracted relative clauses this gives \[
  \lambda_{\text{SR}}  \,
p_{1}( t_{ij} \mid
       \exp \left( \gamma^{\text{SR}} \right) \, \mu_{1, ij}, \phi_{1})
+ (1 - \lambda_{\text{SR}}) \,
p_{2}( t_{ij} \mid
       \exp \left( \gamma^{\text{SR}} \right) \, \mu_{2, ij}, \phi_{2}).
\] Equivalently for object-extracted relative clauses we have \[
  \lambda_{\text{OR}}  \,
p_{1}( t_{ij} \mid
       \exp \left( \gamma^{\text{OR}} \right) \, \mu_{1, ij}, \phi_{1})
+ (1 - \lambda_{\text{OR}}) \,
p_{2}( t_{ij} \mid
       \exp \left( \gamma^{\text{OR}} \right) \, \mu_{2, ij}, \phi_{2}).
\]

Note that we completely recover the dependency locality model when \[
\lambda_{SR} = \lambda_{OR} = 1.
\] Similarly we recover the direct-access model when \[
\gamma^{\text{SR}} = \gamma^{\text{OR}}.
\] In other words both of the proposed models are \emph{special cases}
of this larger joint model.

We can compare how useful these pieces are for explaining any observed
data by studying how much our inferences for \(\lambda_{SR}\),
\(\lambda_{OR}\), \(\gamma^{\text{SR}}\), and \(\gamma^{\text{OR}}\)
deviate from these special cases. For example if the observed data is
consistent with \[
\gamma^{\text{SR}} = \gamma^{\text{OR}}
\] then the role of dependency locality will be negligible.

\section{Environment Setup}\label{environment-setup}

To prepare for implementing these models let's first set up our local
\texttt{R} environment.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{family=}\StringTok{"serif"}\NormalTok{, }\AttributeTok{las=}\DecValTok{1}\NormalTok{, }\AttributeTok{bty=}\StringTok{"l"}\NormalTok{,}
    \AttributeTok{cex.axis=}\DecValTok{1}\NormalTok{, }\AttributeTok{cex.lab=}\DecValTok{1}\NormalTok{, }\AttributeTok{cex.main=}\DecValTok{1}\NormalTok{,}
    \AttributeTok{xaxs=}\StringTok{"i"}\NormalTok{, }\AttributeTok{yaxs=}\StringTok{"i"}\NormalTok{, }\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\FunctionTok{library}\NormalTok{(rstan)}
\FunctionTok{rstan\_options}\NormalTok{(}\AttributeTok{auto\_write =} \ConstantTok{TRUE}\NormalTok{)            }\CommentTok{\# Cache compiled Stan programs}
\FunctionTok{options}\NormalTok{(}\AttributeTok{mc.cores =}\NormalTok{ parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{()) }\CommentTok{\# Parallelize chains}
\NormalTok{parallel}\SpecialCharTok{:::}\FunctionTok{setDefaultClusterOptions}\NormalTok{(}\AttributeTok{setup\_strategy =} \StringTok{"sequential"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To facilitate the implementation of the Bayesian analysis we'll take
advantage of my recommended diagnostic and visualization tools.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{util }\OtherTok{\textless{}{-}} \FunctionTok{new.env}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

First we have a suite of Markov chain Monte Carlo diagnostics and
estimation tools; this code and supporting documentation are both
available on
\href{https://github.com/betanalpha/mcmc_diagnostics}{GitHub}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{source}\NormalTok{(}\StringTok{\textquotesingle{}mcmc\_analysis\_tools\_rstan.R\textquotesingle{}}\NormalTok{, }\AttributeTok{local=}\NormalTok{util)}
\end{Highlighting}
\end{Shaded}

Second we have a suite of probabilistic visualization functions based on
Markov chain Monte Carlo estimation. Again the code and supporting
documentation are available on
\href{https://github.com/betanalpha/mcmc_visualization_tools}{GitHub}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{source}\NormalTok{(}\StringTok{\textquotesingle{}mcmc\_visualization\_tools.R\textquotesingle{}}\NormalTok{, }\AttributeTok{local=}\NormalTok{util)}
\end{Highlighting}
\end{Shaded}

\section{Data Exploration}\label{sec:data-exploration}

A model is only speculation until we can study its interplay with data.
For this case study we will analyze data from Gibson and Wu (2013).
These data were collected by measuring how quickly readers could parse
Chinese sentences with subject-extracted and object-extracted relative
clauses.

\subsection{The Experimental Design}\label{the-experimental-design}

Gibson and Wu designed sixteen \textbf{items}, each of which defined a
writing template consisting of two sentences. The first sentence in each
item established context, while the last sentence introduced space for a
relative clause. Individual written passages were then constructed by
taking an item, selecting a noun to be the subject of the relative
clause, and then finally choosing whether the relative clause would
subject-extracted or object-extracted. The remaining word choice was
carefully engineered to minimize interactions with the subject of the
relative clause.

Forty native Chinese speakers, or \textbf{subjects}, each read some of
these constructed passages on computers. To measure the pace of each
subject's reading, the passages were not initially presented in their
entirety. Instead each subject had to press a button to reveal fragments
sequentially, with the time between presses quantifying the time taken
to parse each fragment. The subject of the relative clause was always
revealed by itself, and the final reading times were given by the time
between the proceeding and subsequent button presses.

This procedure was implemented using the \texttt{Linger}
\href{https://web.archive.org/web/20191220181934/http://tedlab.mit.edu/~dr/Linger/}{software
package}.

To verify that each subject correctly understood the structure of the
relative clause in each passage, the trials concluded with a binary
question testing the subject's comprehension. Gibson and Wu (2013)
quotes a comprehension accuracy of about 90\% for both the
subject-extracted and object-extracted passages, indicating that in both
cases the subjects were usually taking the time to parse what they were
reading.

Gibson graciously shared the data from that experiment in a text file
formatted with space-separated values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{raw\_data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{\textquotesingle{}data/gibsonwu2012data.txt\textquotesingle{}}\NormalTok{,}
                     \AttributeTok{sep=}\StringTok{""}\NormalTok{, }\AttributeTok{header=}\ConstantTok{TRUE}\NormalTok{)}

\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\StringTok{\textquotesingle{}N\textquotesingle{}} \OtherTok{=} \FunctionTok{nrow}\NormalTok{(raw\_data),}
             \StringTok{\textquotesingle{}N\_items\textquotesingle{}} \OtherTok{=} \FunctionTok{max}\NormalTok{(raw\_data}\SpecialCharTok{$}\NormalTok{item),}
             \StringTok{\textquotesingle{}item\textquotesingle{}} \OtherTok{=}\NormalTok{ raw\_data}\SpecialCharTok{$}\NormalTok{item,}
             \StringTok{\textquotesingle{}N\_subjects\textquotesingle{}} \OtherTok{=} \FunctionTok{max}\NormalTok{(raw\_data}\SpecialCharTok{$}\NormalTok{subj),}
             \StringTok{\textquotesingle{}subject\textquotesingle{}} \OtherTok{=}\NormalTok{ raw\_data}\SpecialCharTok{$}\NormalTok{subj,}
             \StringTok{\textquotesingle{}reading\_time\textquotesingle{}} \OtherTok{=}\NormalTok{ raw\_data}\SpecialCharTok{$}\NormalTok{rt,}
             \StringTok{\textquotesingle{}subj\_rel\textquotesingle{}} \OtherTok{=}\NormalTok{ raw\_data}\SpecialCharTok{$}\NormalTok{type }\SpecialCharTok{==} \StringTok{\textquotesingle{}subj{-}ext\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Altogether the data includes \texttt{2735} observations. Consistent with
the experiment design in \textbf{ref}, these observations spanned
\texttt{16} items and \texttt{40} subjects, both of which are indexed
sequentially.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\FunctionTok{sprintf}\NormalTok{(}\StringTok{\textquotesingle{}\%i observations}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, data}\SpecialCharTok{$}\NormalTok{N))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
2735 observations
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\FunctionTok{sprintf}\NormalTok{(}\StringTok{\textquotesingle{}\%i items}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, data}\SpecialCharTok{$}\NormalTok{N\_items))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
16 items
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\FunctionTok{sprintf}\NormalTok{(}\StringTok{\textquotesingle{}\%i subjects}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{, data}\SpecialCharTok{$}\NormalTok{N\_subjects))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
40 subjects
\end{verbatim}

Not every subject in the experiment was given every constructed passage
to read. Instead pairings were determined through a Latin square design
(Box, Hunter, and Hunter 2005) which, if implemented correctly, would
have ensured good coverage of the possible subject-item pairings.

That said, we don't analyze how an experiment \emph{was supposed} to be
implemented. We analyze how it how it \emph{was actually} implemented.
Here that requires carefully investigating the observed pairings.

One nice way to quantify the observed pairings is with a little bit of
graph theory; for more on these techniques see Sections 4.3.2 and 6.2 of
my
\href{https://betanalpha.github.io/assets/chapters_html/pairwise_comparison_modeling.html}{pairwise
comparison modeling chapter}. Specifically, we can encode the
observations in a graph where the items and subjects are nodes and each
observed pairing is an edge. The properties of this graph then allow us
to study the properties of the observed pairings.

For example, the connected components of a graph determine which subsets
of items and subjects have been directly or indirectly compared to each
other. Items and subjects in different connected components cannot be
related to each other.

In conflict with the experimental design not every item has been paired
with at least one subject, and not every subject has been paired with at
least one item. Item 12 and Subjects 10, 13, and 25 all appear as their
own isolated components in the graph.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{source}\NormalTok{(}\StringTok{\textquotesingle{}graph\_utility.R\textquotesingle{}}\NormalTok{, }\AttributeTok{local=}\NormalTok{util)}

\NormalTok{adj }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{build\_adj\_matrix}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{N\_items }\SpecialCharTok{+}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{N\_subjects,}
\NormalTok{                             data}\SpecialCharTok{$}\NormalTok{N,}
\NormalTok{                             data}\SpecialCharTok{$}\NormalTok{item,}
\NormalTok{                             data}\SpecialCharTok{$}\NormalTok{N\_items }\SpecialCharTok{+}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{subject)}

\NormalTok{components }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{compute\_connected\_components}\NormalTok{(adj)}
\FunctionTok{length}\NormalTok{(components)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \FunctionTok{seq\_along}\NormalTok{(components)) \{}
  \FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{\textquotesingle{}Component\textquotesingle{}}\NormalTok{, k))}
  \FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}  \textquotesingle{}}\NormalTok{)}
  \FunctionTok{cat}\NormalTok{(}\FunctionTok{sapply}\NormalTok{(components[[k]],}
             \ControlFlowTok{function}\NormalTok{(i)}
               \FunctionTok{ifelse}\NormalTok{(i }\SpecialCharTok{\textgreater{}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{N\_items,}
                      \FunctionTok{paste0}\NormalTok{(}\StringTok{\textquotesingle{}Subject \textquotesingle{}}\NormalTok{, i }\SpecialCharTok{{-}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{N\_items, }\StringTok{\textquotesingle{},\textquotesingle{}}\NormalTok{),}
                      \FunctionTok{paste0}\NormalTok{(}\StringTok{\textquotesingle{}Item \textquotesingle{}}\NormalTok{, i, }\StringTok{\textquotesingle{},\textquotesingle{}}\NormalTok{))))}
  \FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
  \FunctionTok{print}\NormalTok{(}\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Component 1"
  Item 1, Subject 1, Item 2, Subject 2, Item 3, Subject 3, Item 4, Subject 4, Item 5, Subject 5, Item 6, Subject 6, Item 7, Subject 7, Item 8, Subject 8, Item 9, Subject 9, Item 10, Subject 11, Item 11, Subject 12, Item 13, Subject 14, Item 14, Subject 15, Item 15, Subject 16, Item 16, Subject 17, Subject 18, Subject 19, Subject 20, Subject 21, Subject 22, Subject 23, Subject 24, Subject 26, Subject 27, Subject 28, Subject 29, Subject 30, Subject 31, Subject 32, Subject 33, Subject 34, Subject 35, Subject 36, Subject 37, Subject 38, Subject 39, Subject 40,
[1] ""
[1] "Component 2"
  Item 12,
[1] ""
[1] "Component 3"
  Subject 10,
[1] ""
[1] "Component 4"
  Subject 13,
[1] ""
[1] "Component 5"
  Subject 25,
[1] ""
\end{verbatim}

Indeed this item and these subjects have not been involved in any
observations at all. Item 12 was not read by any subjects, and Subjects
10, 13, and 25 did not read any items.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \FunctionTok{seq\_along}\NormalTok{(components)) \{}
  \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{length}\NormalTok{(components[[k]]) }\SpecialCharTok{\textgreater{}} \DecValTok{1}\NormalTok{) }\ControlFlowTok{next}

  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in}\NormalTok{ components[[k]]) \{}
    \ControlFlowTok{if}\NormalTok{ (i }\SpecialCharTok{\textgreater{}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{N\_items) \{}
\NormalTok{      N\_obs }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{subject }\SpecialCharTok{==}\NormalTok{ (i }\SpecialCharTok{{-}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{N\_items))}
      \FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{\textquotesingle{}Subject\textquotesingle{}}\NormalTok{, i }\SpecialCharTok{\%\%}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{N\_items))}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{      N\_obs }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{item }\SpecialCharTok{==}\NormalTok{ i)}
      \FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{\textquotesingle{}Item\textquotesingle{}}\NormalTok{, i))}
\NormalTok{    \}}
    \FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{\textquotesingle{}  N\_obs =\textquotesingle{}}\NormalTok{, N\_obs))}
    \FunctionTok{print}\NormalTok{(}\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{)}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Item 12"
[1] "  N_obs = 0"
[1] ""
[1] "Subject 10"
[1] "  N_obs = 0"
[1] ""
[1] "Subject 13"
[1] "  N_obs = 0"
[1] ""
[1] "Subject 9"
[1] "  N_obs = 0"
[1] ""
\end{verbatim}

To be fair these gaps are documented in Gibson and Wu (2013). Item 12
was not sampled by the experimental software due to a coding error.
Moreover, the three subjects not included here were intentionally
excluded due to exceptionally low sentence comprehension accuracy.

Had the data from all of the subjects been included, then we would have
had the opportunity to analyze sentence comprehension jointly with
reading times. For now, however, the best we can do is acknowledge this
processing of the data and analyze the observations that were included.

\subsection{Summarizing Reading Times}\label{summarizing-reading-times}

With a thorough understanding of the item/subject pairings, we can
finally consider the resulting reading times. Because we cannot
productively visualize the data in its entirely, we will have to
engineer interpretable \textbf{summary statistics} that can be
visualized.

My personal favorite summary statistic is the lowly histogram. Across
all observations we see a rapid rise in instances of small reading times
and then a slower decay as we move up to larger reading times.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_line\_hist}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{reading\_time, }\DecValTok{0}\NormalTok{, }\DecValTok{7000}\NormalTok{, }\DecValTok{100}\NormalTok{,}
                    \AttributeTok{xlab=}\StringTok{\textquotesingle{}Reading Time (ms)\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, values): 1 value (0.0%) fell
above the binning.
\end{verbatim}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-9-1.pdf}

Sometimes different choices of units can better connect a summary to our
available domain expertise. By moving from milliseconds to seconds, for
example, we see that most readers can parse the target noun in under a
second. Sometimes, however, readers take over five seconds!

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_line\_hist}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{reading\_time }\SpecialCharTok{*} \FloatTok{1e{-}3}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{7}\NormalTok{, }\FloatTok{0.1}\NormalTok{,}
                    \AttributeTok{xlab=}\StringTok{\textquotesingle{}Reading Time (s)\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, values): 1 value (0.0%) fell
above the binning.
\end{verbatim}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-10-1.pdf}

Because of this heavy tailed behavior, it can be difficult to perceive
all the relevant features of the reading times at the same time. One way
to make the observed reading times a bit more compact is to histogram
their \emph{logarithms}. Note that we can do this here only because
there are no vanishing reading times; \texttt{log(1\ +\ epsilon)}
heuristics are \emph{never} a good idea. Yes, even for you.

Interestingly, the histogram of the log reading times indicate that
there might be some multimodal behavior in the observations. That said,
this is only a speculation because of the inherent variability in the
measurements.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_line\_hist}\NormalTok{(}\FunctionTok{log}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{reading\_time), }\DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.1}\NormalTok{,}
                    \AttributeTok{xlab=}\StringTok{\textquotesingle{}log(Reading Time / 1 ms)\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-11-1.pdf}

Summarizing all of the data in aggregate doesn't tell the entire story
here. Remember that the scientific question of interest concerns the
\emph{difference} in reading time behavior across the two types of
relative clause. Fortunately, we can readily isolate those behaviors
with two separate histograms.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_line\_hists}\NormalTok{(}\FunctionTok{log}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{reading\_time[data}\SpecialCharTok{$}\NormalTok{subj\_rel }\SpecialCharTok{==} \ConstantTok{FALSE}\NormalTok{]),}
                     \FunctionTok{log}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{reading\_time[data}\SpecialCharTok{$}\NormalTok{subj\_rel }\SpecialCharTok{==} \ConstantTok{TRUE}\NormalTok{]),}
                     \DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.1}\NormalTok{,}
                     \AttributeTok{xlab=}\StringTok{\textquotesingle{}log(Reading Time / 1 ms)\textquotesingle{}}\NormalTok{)}

\FunctionTok{text}\NormalTok{(}\FloatTok{7.5}\NormalTok{, }\DecValTok{80}\NormalTok{, }\AttributeTok{labels=}\StringTok{\textquotesingle{}Object Relative\textquotesingle{}}\NormalTok{, }\AttributeTok{col=}\StringTok{"black"}\NormalTok{)}
\FunctionTok{text}\NormalTok{(}\FloatTok{8.5}\NormalTok{, }\DecValTok{20}\NormalTok{, }\AttributeTok{labels=}\StringTok{\textquotesingle{}Subject Relative\textquotesingle{}}\NormalTok{, }\AttributeTok{col=}\NormalTok{util}\SpecialCharTok{$}\NormalTok{c\_mid\_teal)}
\end{Highlighting}
\end{Shaded}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-12-1.pdf}

The peaks of both histograms look similar, with the reading times of
subject-extracted relative clauses exhibiting a slightly heavier tail.
Note that the similarity of the two histograms doesn't necessarily say
anything about the differences in complexity for subject-extracted and
objected-extracted relative clauses. For example, if the assignment of
passages with the two types of clauses is not balanced across readers of
similar experience then the differences in reader behavior could wash
out differences in the text behavior.

We also see that the multimodal structure persists in both of these
histograms. This behavior will be useful to keep in mind as we develop
our models.

The type of relative clause is not the only way that we can categorize
the observations. We can also examine any systematic heterogeneity in
reading time behavior across items and subjects by plotting log reading
time histograms for each group. This is also known as
\textbf{stratifying} the histograms.

To make the visualization a bit more manageable, I will show the
stratified histograms for only nine items and nine subjects.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{483833992}\NormalTok{)}

\NormalTok{display\_items }\OtherTok{\textless{}{-}} \FunctionTok{sort}\NormalTok{(}\FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{data}\SpecialCharTok{$}\NormalTok{N\_items, }\AttributeTok{size=}\DecValTok{9}\NormalTok{))}

\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in}\NormalTok{ display\_items) \{}
\NormalTok{  reading\_times }\OtherTok{\textless{}{-}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{reading\_time[data}\SpecialCharTok{$}\NormalTok{item }\SpecialCharTok{==}\NormalTok{ i]}
\NormalTok{  util}\SpecialCharTok{$}\FunctionTok{plot\_line\_hist}\NormalTok{(}\FunctionTok{log}\NormalTok{(reading\_times), }\DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.25}\NormalTok{,}
                      \AttributeTok{xlab=}\StringTok{\textquotesingle{}log(Reading Time / 1 ms)\textquotesingle{}}\NormalTok{,}
                      \AttributeTok{main=}\FunctionTok{paste}\NormalTok{(}\StringTok{\textquotesingle{}Item\textquotesingle{}}\NormalTok{, i))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-13-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{display\_subjects }\OtherTok{\textless{}{-}} \FunctionTok{sort}\NormalTok{(}\FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{data}\SpecialCharTok{$}\NormalTok{N\_subjects, }\AttributeTok{size=}\DecValTok{9}\NormalTok{))}

\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\ControlFlowTok{for}\NormalTok{ (s }\ControlFlowTok{in}\NormalTok{ display\_subjects) \{}
\NormalTok{  reading\_times }\OtherTok{\textless{}{-}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{reading\_time[data}\SpecialCharTok{$}\NormalTok{subject }\SpecialCharTok{==}\NormalTok{ s]}
\NormalTok{  util}\SpecialCharTok{$}\FunctionTok{plot\_line\_hist}\NormalTok{(}\FunctionTok{log}\NormalTok{(reading\_times), }\DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.25}\NormalTok{,}
                      \AttributeTok{xlab=}\StringTok{\textquotesingle{}log(Reading Time / 1 ms)\textquotesingle{}}\NormalTok{,}
                      \AttributeTok{main=}\FunctionTok{paste}\NormalTok{(}\StringTok{\textquotesingle{}Subject\textquotesingle{}}\NormalTok{, s))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-14-1.pdf}

While informative, these ensemble visualizations can be a bit
overwhelming to process when there are a large number of elements. In
order to scale to larger groups we would have to consider alternative
summaries.

For example we might reduce the reading times in each group to a
\emph{scalar} summary, such as an empirical mean or variance, and then
histogram all of the scalar summaries into a single, compact
visualization. If you are curious about how this might be implemented in
practice, then take a look at this case study where I analyze
\href{https://betanalpha.github.io/assets/chapters_html/ratings.html\#data-exploration}{movie
reviews}.

\subsection{Discretization}\label{discretization}

A well-constructed summary statistic isolates interpretable features of
the entire data set. That interpretability ensures that we can compare
and contrast the observed behavior to any available domain expertise.
Because domain expertise varies from person to person, however, so too
will the insights we can pull from any visualization.

Those familiar with histograms of continuous data might have already
noticed a ``spikeness'' in the stratified histograms. As a crude rule of
thumb, the expected variation in each bin can be approximated by the
square root of the observed counts. Consequently if we're looking at
data drawn from a continuous model we would expect neighboring bin
counts to be similar up to their square roots.

Here, however, there are a decent number of isolated bins whose counts
spike up above their neighboring. At this point my concern is entirely
speculative. That said, it does motivate me to dig into the data a
little bit more carefully.

Observed reading times can, in theory, take any positive real value. In
practice we are limited to computers and floating point numbers. Unless
our observations are extraordinarily precise, however, floating numbers
should well-approximate exact real numbers.

For example, repetitions of particular values is mathematically
improbable when working with exactly real-valued observations.
Repetitions of particular floating point values is not impossible, but
they will be rare if the data generating process is continuous.

This is not, however, the behavior we see in our observed reading times.
Most observed values repeat at least once, and some repeat as many as
thirty times!

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_line\_hist}\NormalTok{(}\FunctionTok{table}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{reading\_time),}
                    \FloatTok{0.5}\NormalTok{, }\FloatTok{34.5}\NormalTok{, }\DecValTok{1}\NormalTok{,}
                    \AttributeTok{xlab=}\StringTok{\textquotesingle{}Reading Time Frequencies\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-15-1.pdf}

Looking at the most frequent values reveals an even more striking
behavior: the observed reading times are all integer multiples of 1
millisecond.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{reading\_time)}
\NormalTok{t[t }\SpecialCharTok{\textgreater{}} \DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

 189  222  229  230  238  243  245  246  253  254  262  270  277  278  285  286 
   7    6    7    9   12    6    9   16    8   16   18   19    7   19    8   23 
 287  293  294  298  301  302  309  310  317  318  326  333  334  340  342  344 
   6    6   21    8    7   23    6   19    6   34   25    6   24    7   29    6 
 349  350  357  358  366  373  374  382  390  398  406  414  422  428  430  438 
   9   25    7   23   26   10   24   24   23   13   22   17   13    6   16   18 
 446  454  462  470  478  483  484  486  502  508  510  515  524  526  531  532 
  10   11    7   12    6    7    6    7   10    8    7    7    8    8    8    6 
 534  550  566  582  588  596  603  628  652  660  668  676  684  691  700  708 
   8    6    7    8    9    6    6    6    7    8   11    9    6    6    8   11 
 716  731  756  780  788  812  820  828  860  876 1028 1044 
   6    6    8    7    8    8    7    7    8    6    7    6 
\end{verbatim}

Indeed \emph{all} of the observed reading times are integer multiples of
1 millisecond.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{reading\_time }\SpecialCharTok{\%\%} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

   0 
2735 
\end{verbatim}

In other words, the observed reading times are not as continuous as we
had assumed. Instead they are fundamentally \emph{discrete}. The details
of this discretization, however, are not immediately clear. How we
proceed depends on the available domain expertise.

Fortunately the computer program used to measure the reading times in
this experiment is open source. If one were experienced with computer
programming then they might look into the \texttt{Linger} source code to
see that it uses the built-in \texttt{Tcl} function \texttt{clock} to
approximate the time of each button press.

The \texttt{clock} function uses CPU cycles to track intervals of time,
assuming that each cycle is approximately uniform in time. This isn't
always a great approximation, for example excess computation can slow
the cycles and introduce awkward timing artifacts, but for now let's
assume that it is. While the exact computer hardware used in the
experiments was not documented, personal computers of the time typically
featured gigahertz processor speeds; at these speeds the resolution of
the \texttt{clock} function should be closer to microseconds than
milliseconds.

Consequently the millisecond discretization is more likely implemented
in the code itself, either as truncation or rounding when floating point
values are converted to integer values. Because \texttt{Tcl} is
dynamically typed, however, it can be tricky to track down exactly where
in the code this conversion happens, let alone determine whether the
reading times are rounded to the nearest integer or truncated to the
next lowest or highest integer.

At this point my domain expertise has been exhausted. In order to
understand the discretization further I would reach out to collaborators
who are more familiar with \texttt{Tcl}, if not the \texttt{Linger} code
itself.

I appreciate that all of this might come across as irrelevant detail. If
we can engineer a continuous model that adequately captures the relevant
features of the observed data then it will be. On the other hand, if we
cannot engineer an adequate continuous model then we will have to
incorporate this discretization into our models in order to extract
meaningful insights.

In \href{@sec:mod_disc}{Section 4.6} I demonstrate a few modeling
technique that directly account for discretization.

\section{Model Development}\label{model-development}

We have reviewed the relevant theory and carefully explored the observed
data. Now we are finally ready to start building some probabilistic
models of the data generating process, and then use Bayesian inference
to learn what data generating behaviors are consistent with the observed
data.

To make model development as manageable as possible we will work
iteratively, starting with a simple model and then looking for any
inadequacies. If we can learn not only that a model is inadequate but
also \emph{why} it is inadequate, then we can productively improve the
model. Iterating this process of critique and improvement then allows us
to quickly develop a model sophisticated enough to capture all of the
relevant features of the observed data.

\subsection{Log-Normal Dependency Locality
Model}\label{log-normal-dependency-locality-model}

Let's begin with a log-normal dependency locality model for the observed
reading times. While this model is relatively straightforward, it still
needs a bit of work before we can implement it effectively.

\subsubsection{Observational Model}\label{observational-model}

The construction of the observational model begins with a log-normal
model for the reading times \[
\text{log-normal}(t \mid \log(\mu), \phi ).
\] We will then allow the location parameter to vary depending on the
particular passage and reader being paired together.

When subject \(j\) has been assigned a passage derived from item \(i\)
with a subject-extracted relative clause we use the location model
\begin{align*}
\mu_{ij}^{\text{SR}}
&=
f( \alpha_{i}^{\text{SR}} - \beta_{j} )
\\
&=
\exp \left(  \eta
           + \alpha_{i}^{\text{SR}} - \beta_{j} \right)
\\
&=
\exp \left(  \eta + \gamma^{\text{SR}}
           + \alpha_{i} - \beta_{j} \right)
\\
&=
\exp \left( \gamma^{\text{SR}} \right) \,
\exp \left( \eta + \alpha_{i} - \beta_{j} \right)
\\
&=
\exp \left( \gamma^{\text{SR}} \right) \, \mu_{ij}
\end{align*}

Similarly for a passage derived from item \(i\) but with an
object-extracted relative clause we use \begin{align*}
\mu_{ij}^{\text{OR}}
&=
f( \alpha_{i}^{\text{OR}} - \beta_{j} )
\\
&=
\exp \left(  \eta + \gamma^{\text{OR}}
           + \alpha_{i} - \beta_{j} \right)
\\
&=
\exp \left( \gamma^{\text{OR}} \right) \,
\exp \left( \eta + \alpha_{i} - \beta_{j} \right)
\\
&=
\exp \left( \gamma^{\text{OR}} \right) \, \mu_{ij}
\end{align*}

\subsubsection{Anchors}\label{anchors}

One immediate issue with this model is it is inherently
\textbf{redundant}. There are many distinct configurations of \(\eta\),
\(\alpha_{i}\), \(\beta_{j}\), \(\gamma^{\text{SR}}\), and
\(\gamma^{\text{OR}}\) that give exactly the same location configuration
\emph{for all observations}.

Redundant models always lead to redundant inferences. If one model
configuration is reasonably consistent with the observed data then so
too will \emph{all} of the other equivalent model configurations.
Because of this inferential consequently model redundancy is also known
as statistical \textbf{non-identifiability}.

In order to quantify inferential uncertainties from a redundant model we
will have to consider all of these equivalent model configurations. This
is expensive at best and impossible at worst. For much more on
identifiability see my
\href{https://betanalpha.github.io/assets/case_studies/identifiability.html}{chapter
on the topic}.

Fortunately we can completely eliminate the redundancy in this model
with the careful use of \textbf{anchors}. To anchor the model we select
an arbitrary anchor item, \(i'\), and then consider item complexities
only \emph{relative} to the complexity of that item, \[
\delta_{i} = \alpha_{i} - \alpha_{i'}.
\] If \(\delta_{i}\) is greater than zero then the \(i\)th item is more
complex than the anchor item. By construction the relatively complexity
of the anchor item is fixed, or anchored, to zero, \[
\delta_{i'} = \alpha_{i'} - \alpha_{i'} = 0.
\] This eliminates one degree of freedom from the model, reducing its
overall redundancy.

In addition to anchoring the item complexities we can also anchor the
subject reading skills, \[
\zeta_{j} = \beta_{j} - \beta_{j'}.
\] Together these anchors allow us to write the location model as
\begin{align*}
\mu_{ij}^{\text{SR}}
&=
\exp \left(  \eta + \gamma^{\text{SR}}
           + \alpha_{i} - \beta_{j} \right)
\\
&=
\exp \left(  \eta + \gamma^{\text{SR}} + \alpha_{i'} - \beta_{j'}
           + \delta_{i} - \zeta_{j} \right)
           \\
&=
\exp \left(  \kappa
           + \delta_{i} - \zeta_{j} \right),
\end{align*} where \[
\kappa = \eta + \gamma^{\text{SR}} + \alpha_{i'} - \beta_{j'}
\] quantifies the reading time behavior of the anchor subject parsing a
written passage derived from the anchor item using a subject-extracted
relative clause. By modeling \(\kappa\) as a single parameter we replace
four degrees of freedom in our initial implementation of the model with
only one.

For object-extracted relative clauses we have \begin{align*}
\mu_{ij}^{\text{OR}}
&=
\exp \left(  \eta + \gamma^{\text{OR}}
           + \alpha_{i} - \beta_{j} \right)
\\
&=
\exp \left(  \eta + \gamma^{\text{SR}} + \alpha_{i'} - \beta_{j'}
           + \gamma^{\text{OR}} - \gamma^{\text{SR}}
           + \delta_{i} - \zeta_{j} \right)
           \\
&=
\exp \left(  \kappa + \chi
           + \delta_{i} - \zeta_{j} \right).
\end{align*} Here \[
\chi = \gamma^{\text{OR}} - \gamma^{\text{SR}}
\] quantifies the complexity of object-extracted relative clauses
\emph{relative} to subject-extracted relative clauses.

Because all of the included pairings are at least indirectly related to
each other, the anchoring that we have implemented completely eliminates
the redundancy of the observational model. For much more on anchoring in
general, and how to manage pairings that decompose into multiple
connected components, see the aforementioned
\href{https://betanalpha.github.io/assets/chapters_html/pairwise_comparison_modeling.html}{pairwise
comparison modeling chapter}.

In theory we can anchor any item and subject, even if we have not
observed that particular pairing. That said anchoring and item and
subject that have been paired as many times as possible can reduce
inferential degeneracies, which then reduces computational costs.
Because the pairings of the items and subjects in this data set are
relatively uniform, any choice of anchors should perform as well as any
others. We'll anchor Item 1 and Subject 1 just out of convenience.

\subsubsection{Prior Model}\label{prior-model}

Before we can perform Bayesian inference we need to elevate our
observational model to a
\href{https://betanalpha.github.io/assets/case_studies/modeling_and_inference.html\#314_the_complete_bayesian_model}{full
Bayesian model} by specifying a
\href{https://betanalpha.github.io/assets/case_studies/prior_modeling.html}{prior
model} over all of the model configuration variables.

A prior model allows us to incorporates additional domain expertise into
our inferences. Translating implicit, and often qualitative domain
expertise into an explicit, quantitative prior model, however, takes
time and effort. In practice we have only finite resources, so our goal
isn't to build a prior model that perfectly incorporates \emph{all}
available domain expertise, but rather just the domain expertise that we
think might improve any inferences of interest.

Critically we don't have to be precious here. If the domain expertise
that we use initially proves to be insufficient, then we can always
incorporate more into an updated prior model. In other words prior
modeling it itself often iterative.

In general a prior model is defined by a joint probability distribution
over all of the model configuration variables. Here we will make a
common assumption that our domain expertise for each model configuration
variable is independent. This allows the joint prior model to decompose
into separate component prior models for each parameter.

To motivate one-dimensional component prior models I like to think in
terms of \textbf{thresholds} that separate more reasonable behaviors
from more extreme behaviors. For example, to avoid unrealistically fast
and slow reading times for our baseline scenario we might want to
constrain \(\kappa\) to \begin{alignat*}{5}
& 100 \text{ milliseconds} &
& \lessapprox &
& \, \exp(\kappa) \cdot 1 \text{ millisecond} \, &
& \lessapprox &
& \, 1000 \text{ milliseconds}
\\
& 100 &
& \lessapprox &
& \quad\quad\quad\; \exp(\kappa) &
& \lessapprox &
& \, 1000
\\
& \log 100 &
& \lessapprox &
& \quad\quad\quad\quad\; \kappa &
& \lessapprox &
& \log 1000.
\end{alignat*} Note this isn't a constraint on the actual reading times
but rather just the baseline location of the reading time distribution.

Model configurations outside of these thresholds are not impossible,
just more extreme relative to our available domain expertise. We can
capture this with a prior model that concentrates most, but not all, of
its probability within these thresholds, \begin{align*}
1 - \epsilon
&=
\pi( \, [ \log 100 , \log 1000 ] \, )
\\
&=
\int_{\log 100 }^{ \log 1000} \mathrm{d} \kappa \,
                   \text{normal}(\kappa \mid m, s).
\end{align*} We don't have to be too precise value about the definition
of ``most'' here. I use \(\epsilon = 0.02\) when considering two
thresholds.

Conveniently we can analytically derive a normal prior model that
satisfies this condition, \begin{align*}
p( \kappa )
&=
\text{normal} \left(
\kappa \;\, \bigg| \;\, \frac{\log 1000 + \log 100}{2},
                        \frac{1}{2.32}
                        \frac{\log 1000 - \log 100}{2} \right)
\\
&= \text{normal}( \kappa \mid 5.76, 0.50).
\end{align*}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{xs }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.01}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(xs, }\FunctionTok{dnorm}\NormalTok{(xs, }\FloatTok{5.76}\NormalTok{, }\FloatTok{0.50}\NormalTok{),}
     \AttributeTok{type=}\StringTok{\textquotesingle{}l\textquotesingle{}}\NormalTok{, }\AttributeTok{yaxt=}\StringTok{\textquotesingle{}n\textquotesingle{}}\NormalTok{,}
     \AttributeTok{lwd=}\DecValTok{3}\NormalTok{, }\AttributeTok{col=}\NormalTok{util}\SpecialCharTok{$}\NormalTok{c\_dark,}
     \AttributeTok{xlab=}\StringTok{\textquotesingle{}kappa\textquotesingle{}}\NormalTok{, }\AttributeTok{ylab=}\StringTok{\textquotesingle{}p(kappa)\textquotesingle{}}\NormalTok{)}

\FunctionTok{abline}\NormalTok{(}\AttributeTok{v=}\FunctionTok{log}\NormalTok{(}\FloatTok{1e2}\NormalTok{), }\AttributeTok{lwd=}\DecValTok{2}\NormalTok{, }\AttributeTok{lty=}\DecValTok{3}\NormalTok{, }\AttributeTok{col=}\StringTok{"\#DDDDDD"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v=}\FunctionTok{log}\NormalTok{(}\FloatTok{1e3}\NormalTok{), }\AttributeTok{lwd=}\DecValTok{2}\NormalTok{, }\AttributeTok{lty=}\DecValTok{3}\NormalTok{, }\AttributeTok{col=}\StringTok{"\#DDDDDD"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-18-1.pdf}

To motivate component prior models for the other parameters let's
consider their proportional influences on the baseline behavior. If our
domain expertise suggests that the item complexities are within an order
of magnitude of each other then we would want our prior model to enforce
the constraint \begin{alignat*}{5}
& \quad\quad\;\; \frac{1}{10} &
& \lessapprox &
& \; \exp( \delta_{i} ) \, &
& \lessapprox &
& \, 10
\\
& \, \hphantom{-} \log \frac{1}{10} &
& \lessapprox &
& \quad\;\; \delta_{i} &
& \lessapprox &
& \, \log 10
\\
& -\log 10 &
& \lessapprox &
& \quad\;\; \delta_{i} &
& \lessapprox &
& \log 10.
\end{alignat*}

We can achieve this soft containment with \begin{align*}
p( \delta_i )
&=
\text{normal} \left(
\delta_i \;\, \bigg| \;\, \frac{\log 10 + (-\log 10)}{2},
                          \frac{1}{2.32}
                          \frac{\log 10 - (-\log 10)}{2} \right)
\\
&=
\text{normal} \left(
\delta_i \;\, \bigg| \;\, 0, \frac{1}{2.32} \, \log 10\right)
\\
&= \text{normal}( \delta_i \mid 0, 0.99).
\end{align*} Note that this is a prior model for the \emph{relative}
item complexities. It does \emph{not} imply the same prior model for the
\emph{absolute} item qualities, \[
p( \alpha_i ) \ne \text{normal}( \alpha_i \mid 0, 0.99)!
\]

Here we'll assume similar domain expertise for the relative reader
skills and relative complexity between subject-extracted and
object-extracted relative clauses, \begin{align*}
p( \zeta_j ) &= \text{normal}( \zeta_j \mid 0, 0.99)
\\
p( \chi ) &= \text{normal}( \chi \mid 0, 0.99).
\end{align*} If have more precise domain expertise for any of these
quantities then we should absolutely consider using it to motivate a
more informative component prior model.

Lastly we need a component prior model for the scale parameter \(\phi\).
For the log normal model, like most probabilistic models over positive
real values, the scale parameter amplifies a baseline variance. When
\(\phi = 0\) the model collapses to a point and as
\(\phi \rightarrow \infty\) the model becomes infinitely diffuse.

Let's embrace order of magnitude again and use the soft constraints \[
0 \lessapprox \phi \lessapprox 10,
\] which we can enforce with the prior model \[
p( \psi )
= \text{half-normal} \left( \psi \;\, \bigg| \;\,
                            0, \frac{10}{2.57} \right)
= \text{half-normal} \left( \psi \mid 0, 3.89 \right).
\]

One the main limitations of component prior models is that they ignore
the interactions between the parameters as the data generating process
evolves. Each component prior model might appear reasonable on their
own, but together admit undesired behaviors. Fortunately we can check
for any undesired interactions by performing a
\href{https://betanalpha.github.io/assets/case_studies/prior_modeling.html\#415_Good_Do(o)g}{prior
predictive check}. In a prior predictive check we sample model
configurations from the prior model and then simulate the data
generating process, comparing the resulting outcomes to our domain
expertise.

For example, what reading times are reasonable? Well, human reaction
times are not far from \(10^{2}\) milliseconds; observed reading times
an order of magnitude faster than this would definitely be a bit
suspicious. On the other hand taking longer than \[
  10^{5} \, \text{milliseconds}
= 10^{2} \, \text{seconds}
\approx 2 \, \text{minutes}
\] to read a single word would also raise questions. Critically we are
basing these thresholds on our domain expertise and \emph{not} what we
might have seen when we explored the observed data!

We can now use \texttt{Stan}'s \texttt{Fixed\_param} configuration to
generate prior predictive samples.

\begin{codelisting}

\caption{\texttt{dlt1\textbackslash\_prior.stan}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{ \{}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N; }\CommentTok{// Number of observations}

  \CommentTok{// Item configuration}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N\_items;}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{, }\KeywordTok{upper}\NormalTok{=N\_items\textgreater{} item;}

  \CommentTok{// Subject configuration}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N\_subjects;}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{, }\KeywordTok{upper}\NormalTok{=N\_subjects\textgreater{} subject;}

  \CommentTok{// Item variant}
  \CommentTok{//   Object Relative:  subj\_rel = 0}
  \CommentTok{//   Subject Relative: subj\_rel = 1}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} subj\_rel;}

  \DataTypeTok{vector}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{}[N] reading\_time; }\CommentTok{// Reading times (ms)}
\NormalTok{\}}


\KeywordTok{generated quantities}\NormalTok{ \{}
  \CommentTok{// Log Reading Time Baseline}
  \DataTypeTok{real}\NormalTok{ kappa = normal\_rng(}\FloatTok{5.76}\NormalTok{, }\FloatTok{0.50}\NormalTok{);}

  \CommentTok{// Relative item difficulties}
  \DataTypeTok{vector}\NormalTok{[N\_items {-} }\DecValTok{1}\NormalTok{] delta\_free}
\NormalTok{    = to\_vector(normal\_rng(zeros\_vector(N\_items {-} }\DecValTok{1}\NormalTok{), }\FloatTok{0.99}\NormalTok{));}

  \CommentTok{// Relative subject skills}
  \DataTypeTok{vector}\NormalTok{[N\_subjects {-} }\DecValTok{1}\NormalTok{] zeta\_free}
\NormalTok{    = to\_vector(normal\_rng(zeros\_vector(N\_subjects {-} }\DecValTok{1}\NormalTok{), }\FloatTok{0.99}\NormalTok{));}

  \CommentTok{// Subject Relative Difference}
  \DataTypeTok{real}\NormalTok{ chi = normal\_rng(}\DecValTok{0}\NormalTok{, }\FloatTok{0.99}\NormalTok{);}

  \CommentTok{// Measurement scale}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} phi = abs(normal\_rng(}\DecValTok{0}\NormalTok{, }\FloatTok{3.89}\NormalTok{));}

  \CommentTok{// Relative skills for all items and subjects}
  \DataTypeTok{vector}\NormalTok{[N\_items] delta}
\NormalTok{    = append\_row([}\DecValTok{0}\NormalTok{]\textquotesingle{}, delta\_free);}
  \DataTypeTok{vector}\NormalTok{[N\_subjects] zeta}
\NormalTok{    = append\_row([}\DecValTok{0}\NormalTok{]\textquotesingle{}, zeta\_free);}

  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{real}\NormalTok{ log\_reading\_time\_pred;}

  \ControlFlowTok{for}\NormalTok{ (n }\ControlFlowTok{in} \DecValTok{1}\NormalTok{:N) \{}
    \DataTypeTok{int}\NormalTok{ i = item[n];}
    \DataTypeTok{int}\NormalTok{ s = subject[n];}

    \DataTypeTok{real}\NormalTok{ log\_mu = kappa + delta[i] {-} zeta[s];}
    \ControlFlowTok{if}\NormalTok{ (subj\_rel[n] == }\DecValTok{0}\NormalTok{) log\_mu += chi;}

\NormalTok{    log\_reading\_time\_pred[n] = log(lognormal\_rng(log\_mu, phi));}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{stan}\NormalTok{(}\AttributeTok{file=}\StringTok{"stan\_programs/dlt1\_prior.stan"}\NormalTok{,}
            \AttributeTok{data=}\NormalTok{data, }\AttributeTok{seed=}\DecValTok{8438338}\NormalTok{, }\AttributeTok{algorithm=}\StringTok{"Fixed\_param"}\NormalTok{,}
            \AttributeTok{warmup=}\DecValTok{0}\NormalTok{, }\AttributeTok{iter=}\DecValTok{1024}\NormalTok{, }\AttributeTok{refresh=}\DecValTok{0}\NormalTok{)}

\NormalTok{samples }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{extract\_expectand\_vals}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

Then we can histogram the results and compare to our elicited
thresholds.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_hist\_quantiles}\NormalTok{(samples, }\StringTok{\textquotesingle{}log\_reading\_time\_pred\textquotesingle{}}\NormalTok{,}
                         \DecValTok{0}\NormalTok{, }\DecValTok{20}\NormalTok{, }\FloatTok{0.25}\NormalTok{,}
                         \AttributeTok{xlab=}\StringTok{\textquotesingle{}Log Reading Time / 1 ms\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 715455 predictive values (6.4%) fell below the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 52859 predictive values (0.5%) fell above the binning.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v=}\FunctionTok{log}\NormalTok{(}\FloatTok{1e1}\NormalTok{), }\AttributeTok{lwd=}\DecValTok{2}\NormalTok{, }\AttributeTok{lty=}\DecValTok{3}\NormalTok{, }\AttributeTok{col=}\StringTok{"\#DDDDDD"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v=}\FunctionTok{log}\NormalTok{(}\FloatTok{1e5}\NormalTok{), }\AttributeTok{lwd=}\DecValTok{2}\NormalTok{, }\AttributeTok{lty=}\DecValTok{3}\NormalTok{, }\AttributeTok{col=}\StringTok{"\#DDDDDD"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-20-1.pdf}

The prior predictive reading times weakly concentrate within our
thresholds; if anything the prior predictive behavior is too
conservative, especially towards smaller reading times. For now we will
push ahead, aware that our prior model is relatively weakly informative
and could be improved if needed.

In addition to the aggregate histogram we could also examine the prior
predictive behavior in the stratified histograms that we considered in
\href{@sec:data-exploration}{Section 3}. I will leave that as an
exercise for the ambitious reader!

\subsubsection{Inferential Computation}\label{inferential-computation}

At this point we implement our full Bayesian model as a \texttt{Stan}
program. The \texttt{Stan} package can then take this program, evaluate
it on the observed data to construct a posterior density function, and
then use Hamiltonian Monte Carlo to explore the implied posterior
distribution.

\begin{codelisting}

\caption{\texttt{dlt1.stan}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{ \{}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N; }\CommentTok{// Number of observations}

  \CommentTok{// Item configuration}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N\_items;}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{, }\KeywordTok{upper}\NormalTok{=N\_items\textgreater{} item;}

  \CommentTok{// Subject configuration}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N\_subjects;}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{, }\KeywordTok{upper}\NormalTok{=N\_subjects\textgreater{} subject;}

  \CommentTok{// Item variant}
  \CommentTok{//   Object Relative:  subj\_rel = 0}
  \CommentTok{//   Subject Relative: subj\_rel = 1}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} subj\_rel;}

  \DataTypeTok{vector}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{}[N] reading\_time; }\CommentTok{// Reading times (ms)}
\NormalTok{\}}

\KeywordTok{parameters}\NormalTok{ \{}
  \CommentTok{// Log Reading Time Baseline}
  \DataTypeTok{real}\NormalTok{ kappa;}

  \CommentTok{// Relative item difficulties}
  \DataTypeTok{vector}\NormalTok{[N\_items {-} }\DecValTok{1}\NormalTok{] delta\_free;}

  \CommentTok{// Relative subject skills}
  \DataTypeTok{vector}\NormalTok{[N\_subjects {-} }\DecValTok{1}\NormalTok{] zeta\_free;}

  \CommentTok{// Subject Relative Difference}
  \DataTypeTok{real}\NormalTok{ chi;}

  \CommentTok{// Measurement scale}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} phi;}
\NormalTok{\}}

\KeywordTok{transformed parameters}\NormalTok{ \{}
  \CommentTok{// Relative skills for all items and subjects}
  \DataTypeTok{vector}\NormalTok{[N\_items] delta}
\NormalTok{    = append\_row([}\DecValTok{0}\NormalTok{]\textquotesingle{}, delta\_free);}
  \DataTypeTok{vector}\NormalTok{[N\_subjects] zeta}
\NormalTok{    = append\_row([}\DecValTok{0}\NormalTok{]\textquotesingle{}, zeta\_free);}
\NormalTok{\}}

\KeywordTok{model}\NormalTok{ \{}
  \CommentTok{// Location configurations}
  \DataTypeTok{vector}\NormalTok{[N] log\_mu =   kappa}
\NormalTok{                     + delta[item] {-} zeta[subject]}
\NormalTok{                     + (}\DecValTok{1}\NormalTok{ {-} to\_vector(subj\_rel)) * chi;}

  \CommentTok{// Prior model}

  \CommentTok{// 100 \textless{}\textasciitilde{} exp(kappa) \textless{}\textasciitilde{} 1000}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(kappa | }\FloatTok{5.76}\NormalTok{, }\FloatTok{0.50}\NormalTok{);}

  \CommentTok{// 0.1 \textless{}\textasciitilde{} exp(delta) \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(delta\_free | }\DecValTok{0}\NormalTok{, }\FloatTok{0.99}\NormalTok{);}

  \CommentTok{// 0.1 \textless{}\textasciitilde{} exp(zeta) \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(zeta\_free| }\DecValTok{0}\NormalTok{, }\FloatTok{0.99}\NormalTok{);}

  \CommentTok{// 0.1 \textless{}\textasciitilde{} exp(chi) \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(chi | }\DecValTok{0}\NormalTok{, }\FloatTok{0.99}\NormalTok{);}

  \CommentTok{// 0 \textless{}\textasciitilde{} phi \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(phi | }\DecValTok{0}\NormalTok{, }\FloatTok{3.89}\NormalTok{);}

  \CommentTok{// Observational model}
  \KeywordTok{target +=}\NormalTok{ lognormal\_lpdf(reading\_time | log\_mu, phi);}
\NormalTok{\}}

\KeywordTok{generated quantities}\NormalTok{ \{}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{real}\NormalTok{ log\_reading\_time\_pred;}

  \ControlFlowTok{for}\NormalTok{ (n }\ControlFlowTok{in} \DecValTok{1}\NormalTok{:N) \{}
    \DataTypeTok{int}\NormalTok{ i = item[n];}
    \DataTypeTok{int}\NormalTok{ s = subject[n];}

    \DataTypeTok{real}\NormalTok{ log\_mu = kappa + delta[i] {-} zeta[s];}
    \ControlFlowTok{if}\NormalTok{ (subj\_rel[n] == }\DecValTok{0}\NormalTok{) log\_mu += chi;}

\NormalTok{    log\_reading\_time\_pred[n] = log(lognormal\_rng(log\_mu, phi));}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{stan}\NormalTok{(}\AttributeTok{file=}\StringTok{"stan\_programs/dlt1.stan"}\NormalTok{,}
            \AttributeTok{data=}\NormalTok{data, }\AttributeTok{seed=}\DecValTok{8438338}\NormalTok{,}
            \AttributeTok{warmup=}\DecValTok{1000}\NormalTok{, }\AttributeTok{iter=}\DecValTok{2024}\NormalTok{, }\AttributeTok{refresh=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

First and foremost, we have to check for any signs that the posterior
computation might be inaccurate.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diagnostics1 }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{extract\_hmc\_diagnostics}\NormalTok{(fit)}
\NormalTok{util}\SpecialCharTok{$}\FunctionTok{check\_all\_hmc\_diagnostics}\NormalTok{(diagnostics1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  All Hamiltonian Monte Carlo diagnostics are consistent with reliable
Markov chain Monte Carlo.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{samples\_dlt1 }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{extract\_expectand\_vals}\NormalTok{(fit)}
\NormalTok{base\_samples }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{filter\_expectands}\NormalTok{(samples\_dlt1,}
                                       \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}kappa\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}delta\_free\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}zeta\_free\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}chi\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}phi\textquotesingle{}}\NormalTok{),}
                                       \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{util}\SpecialCharTok{$}\FunctionTok{summarize\_expectand\_diagnostics}\NormalTok{(base\_samples)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The expectands kappa triggered diagnostic warnings.

The expectands kappa triggered hat{ESS} warnings.

Small empirical effective sample sizes result in imprecise Markov chain
Monte Carlo estimators.
\end{verbatim}

The empirical effective sample size warning are not fatal on their own;
they just indicate that the Markov chains are exploring the posterior
distribution slowly. This is especially true given that most of the
small empirical effective sample sizes are coming from only one Markov
chains, which indicates that this slow exploration is likely due to an
unlucky adaptation of the respective Markov transition.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_ess\_hats}\NormalTok{(base\_samples, }\AttributeTok{B=}\DecValTok{50}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-23-1.pdf}

\subsubsection{Posterior Retrodictive
Checks}\label{posterior-retrodictive-checks}

Before investigating any posterior inferences we need to evaluate how
well our full Bayesian model captures the relevant features of the
observed data. Inferences drawn from a model that poorly fits the data
are fragile at best and completely meaningless at worst.

I strongly recommend evaluating Bayesian models by comparing the
behavior of the observed data to the behavior of the posterior
predictive distribution with a
\href{https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html\#143_Posterior_Retrodiction_Checks}{posterior
retrodictive check}. Note the terminology here -- we're implementing a
\emph{retrodictive} check because we're evaluating how well the model
retrodicts data that it has already seen. \emph{Predictive} checks
require comparisons to new observations.

Each posterior retrodictive check starts with a summary statistic that
isolates an interpretable, relevant feature of the observational space.
Conveniently the summary statistics that are useful for data exploration
are also useful for retrodictive checking, so we can build off of the
work that we've already done.

For example, we might use the aggregate histogram summary statistic.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_hist\_quantiles}\NormalTok{(samples\_dlt1, }\StringTok{\textquotesingle{}log\_reading\_time\_pred\textquotesingle{}}\NormalTok{,}
                         \DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.1}\NormalTok{,}
                         \AttributeTok{baseline\_values=}\FunctionTok{log}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{reading\_time),}
                         \AttributeTok{xlab=}\StringTok{\textquotesingle{}Log Reading Time / 1 ms\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 1386 predictive values (0.0%) fell below the binning.
\end{verbatim}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-24-1.pdf}

Unfortunately this comparison reveals substantial retrodictive tension.
The posterior predictive behavior is much more symmetric than the
observed behavior; specifically it exhibits a heavier left tail and
lighter right tail. At the same time the posterior predictive behavior
doesn't exhibit the same dip at moderate values that the observed data
does.

This aggregate tension could be due to our initial model poorly fitting
reading times from passages with either of the relative clauses.
Fortunately we can readily investigate this by implementing posterior
retrodictive checks for the histograms stratified by relative clause
type.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{names }\OtherTok{\textless{}{-}} \FunctionTok{sapply}\NormalTok{(}\FunctionTok{which}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{subj\_rel }\SpecialCharTok{==} \DecValTok{1}\NormalTok{),}
                \ControlFlowTok{function}\NormalTok{(n)}
                \FunctionTok{paste0}\NormalTok{(}\StringTok{\textquotesingle{}log\_reading\_time\_pred[\textquotesingle{}}\NormalTok{, n, }\StringTok{\textquotesingle{}]\textquotesingle{}}\NormalTok{))}
\NormalTok{filtered\_samples }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{filter\_expectands}\NormalTok{(samples\_dlt1, names)}

\NormalTok{obs\_reading\_time }\OtherTok{\textless{}{-}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{reading\_time[data}\SpecialCharTok{$}\NormalTok{subj\_rel }\SpecialCharTok{==} \DecValTok{1}\NormalTok{]}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_hist\_quantiles}\NormalTok{(filtered\_samples, }\StringTok{\textquotesingle{}log\_reading\_time\_pred\textquotesingle{}}\NormalTok{,}
                         \DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.1}\NormalTok{,}
                         \AttributeTok{baseline\_values=}\FunctionTok{log}\NormalTok{(obs\_reading\_time),}
                         \AttributeTok{xlab=}\StringTok{\textquotesingle{}Log Reading Time / 1 ms\textquotesingle{}}\NormalTok{,}
                         \AttributeTok{main=}\StringTok{\textquotesingle{}Subject Relative\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 527 predictive values (0.0%) fell below the binning.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{names }\OtherTok{\textless{}{-}} \FunctionTok{sapply}\NormalTok{(}\FunctionTok{which}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{subj\_rel }\SpecialCharTok{==} \DecValTok{0}\NormalTok{),}
                \ControlFlowTok{function}\NormalTok{(n)}
                \FunctionTok{paste0}\NormalTok{(}\StringTok{\textquotesingle{}log\_reading\_time\_pred[\textquotesingle{}}\NormalTok{, n, }\StringTok{\textquotesingle{}]\textquotesingle{}}\NormalTok{))}
\NormalTok{filtered\_samples }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{filter\_expectands}\NormalTok{(samples\_dlt1, names)}

\NormalTok{obs\_reading\_time }\OtherTok{\textless{}{-}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{reading\_time[data}\SpecialCharTok{$}\NormalTok{subj\_rel }\SpecialCharTok{==} \DecValTok{0}\NormalTok{]}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_hist\_quantiles}\NormalTok{(filtered\_samples, }\StringTok{\textquotesingle{}log\_reading\_time\_pred\textquotesingle{}}\NormalTok{,}
                         \DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.1}\NormalTok{,}
                         \AttributeTok{baseline\_values=}\FunctionTok{log}\NormalTok{(obs\_reading\_time),}
                         \AttributeTok{xlab=}\StringTok{\textquotesingle{}Log Reading Time / 1 ms\textquotesingle{}}\NormalTok{,}
                         \AttributeTok{main=}\StringTok{\textquotesingle{}Object Relative\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 859 predictive values (0.0%) fell below the binning.
\end{verbatim}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-25-1.pdf}

The aggregate retrodictive tension strongly persists into the
object-extracted relative clause histogram. On the other hand, the
disagreement appears to be a bit weaker in the subject-extracted
relative clause observations.

At this point we could perform more retrodictive checks, for instance
over the item-stratified and subject-stratified histograms, to build
more and more understanding for \emph{why} our initial model is
inadequate, and hence how we can make it better. That said we already
have a promising hypothesis -- the shape of the tails.

\subsection{Inverse Gamma Dependency Locality Model}\label{sec:dlt2}

The retrodictive checks suggest that a more asymmetric measurement
variability model, with a lighter lower tail and heavier upper tail,
might better fit the observed reading times. Fortunately the
mean-dispersion inverse gamma model exhibits exactly this behavior
relative to the log normal model.

Perhaps you have never heard of an inverse gamma model, let alone a
mean-dispersion inverse gamma model, before? That is entirely
reasonable.

Building probabilistic models is similar to writing a story in a foreign
language. You may know what you want to write, but not have the
vocabulary to express that in the new language. The log normal model,
inverse gamma model, as well as pairwise comparison and mixture models
more generally, are all examples of probabilistic vocabulary that expand
the kind of data generating stories you can tell.

Just as when learning a foreign language, the stories you can tell when
first getting starting with probabilistic modeling will be simple and
awkward. As you build your vocabulary by learning new modeling
techniques, you will be able to tell richer and more sophisticated
stories. Moreover, collaborating with a statistician fluent in this
language can gives you a pretty good head start.

The mean-dispersion inverse gamma model uses all of the same parameters
as the log normal model. All of these parameters maintain the exact same
interpretation except for one: \(\phi\). While qualitatively similar in
the two models, the precise influence of the scale parameter is slightly
different.

For the persistent parameters we can use the same component prior
models. Because the scale parameters behave similarly we can also use
the same component prior model for \(\phi\). We just need to be careful
to verify reasonable prior predictive behavior.

\begin{codelisting}

\caption{\texttt{dlt2\textbackslash\_prior.stan}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{functions}\NormalTok{ \{}
  \CommentTok{// Mean{-}dispersion parameterization of inverse gamma family}
  \DataTypeTok{real}\NormalTok{ inv\_gamma\_md\_lpdf(}\DataTypeTok{real}\NormalTok{ x, }\DataTypeTok{real}\NormalTok{ log\_mu, }\DataTypeTok{real}\NormalTok{ psi) \{}
    \ControlFlowTok{return}\NormalTok{ inv\_gamma\_lpdf(x | inv(psi) + }\DecValTok{2}\NormalTok{,}
\NormalTok{                              exp(log\_mu) * (inv(psi) + }\DecValTok{1}\NormalTok{));}
\NormalTok{  \}}

  \DataTypeTok{real}\NormalTok{ inv\_gamma\_md\_rng(}\DataTypeTok{real}\NormalTok{ log\_mu, }\DataTypeTok{real}\NormalTok{ psi) \{}
    \ControlFlowTok{return}\NormalTok{ inv\_gamma\_rng(inv(psi) + }\DecValTok{2}\NormalTok{,}
\NormalTok{                         exp(log\_mu) * (inv(psi) + }\DecValTok{1}\NormalTok{));}
\NormalTok{  \}}
\NormalTok{\}}

\KeywordTok{data}\NormalTok{ \{}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N; }\CommentTok{// Number of observations}

  \CommentTok{// Item configuration}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N\_items;}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{, }\KeywordTok{upper}\NormalTok{=N\_items\textgreater{} item;}

  \CommentTok{// Subject configuration}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N\_subjects;}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{, }\KeywordTok{upper}\NormalTok{=N\_subjects\textgreater{} subject;}

  \CommentTok{// Item variant}
  \CommentTok{//   Object Relative:  subj\_rel = 0}
  \CommentTok{//   Subject Relative: subj\_rel = 1}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} subj\_rel;}

  \DataTypeTok{vector}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{}[N] reading\_time; }\CommentTok{// Reading times (ms)}
\NormalTok{\}}

\KeywordTok{generated quantities}\NormalTok{ \{}
  \CommentTok{// Log Reading Time Baseline}
  \DataTypeTok{real}\NormalTok{ kappa = normal\_rng(}\FloatTok{5.76}\NormalTok{, }\FloatTok{0.50}\NormalTok{);}

  \CommentTok{// Relative item difficulties}
  \DataTypeTok{vector}\NormalTok{[N\_items {-} }\DecValTok{1}\NormalTok{] delta\_free}
\NormalTok{    = to\_vector(normal\_rng(zeros\_vector(N\_items {-} }\DecValTok{1}\NormalTok{), }\FloatTok{0.99}\NormalTok{));}

  \CommentTok{// Relative subject skills}
  \DataTypeTok{vector}\NormalTok{[N\_subjects {-} }\DecValTok{1}\NormalTok{] zeta\_free}
\NormalTok{    = to\_vector(normal\_rng(zeros\_vector(N\_subjects {-} }\DecValTok{1}\NormalTok{), }\FloatTok{0.99}\NormalTok{));}

  \CommentTok{// Subject Relative Difference}
  \DataTypeTok{real}\NormalTok{ chi = normal\_rng(}\DecValTok{0}\NormalTok{, }\FloatTok{0.99}\NormalTok{);}

  \CommentTok{// Measurement scale}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} phi = abs(normal\_rng(}\DecValTok{0}\NormalTok{, }\FloatTok{3.89}\NormalTok{));}

  \CommentTok{// Relative skills for all items and subjects}
  \DataTypeTok{vector}\NormalTok{[N\_items] delta}
\NormalTok{    = append\_row([}\DecValTok{0}\NormalTok{]\textquotesingle{}, delta\_free);}
  \DataTypeTok{vector}\NormalTok{[N\_subjects] zeta}
\NormalTok{    = append\_row([}\DecValTok{0}\NormalTok{]\textquotesingle{}, zeta\_free);}

  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{real}\NormalTok{ log\_reading\_time\_pred;}

  \ControlFlowTok{for}\NormalTok{ (n }\ControlFlowTok{in} \DecValTok{1}\NormalTok{:N) \{}
    \DataTypeTok{int}\NormalTok{ i = item[n];}
    \DataTypeTok{int}\NormalTok{ s = subject[n];}

    \DataTypeTok{real}\NormalTok{ log\_mu = kappa + delta[i] {-} zeta[s];}
    \ControlFlowTok{if}\NormalTok{ (subj\_rel[n] == }\DecValTok{0}\NormalTok{) log\_mu += chi;}

\NormalTok{    log\_reading\_time\_pred[n] = log(inv\_gamma\_md\_rng(log\_mu, phi));}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{stan}\NormalTok{(}\AttributeTok{file=}\StringTok{"stan\_programs/dlt2\_prior.stan"}\NormalTok{,}
            \AttributeTok{data=}\NormalTok{data, }\AttributeTok{seed=}\DecValTok{8438338}\NormalTok{, }\AttributeTok{algorithm=}\StringTok{"Fixed\_param"}\NormalTok{,}
            \AttributeTok{warmup=}\DecValTok{0}\NormalTok{, }\AttributeTok{iter=}\DecValTok{1024}\NormalTok{, }\AttributeTok{refresh=}\DecValTok{0}\NormalTok{)}

\NormalTok{samples }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{extract\_expectand\_vals}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_hist\_quantiles}\NormalTok{(samples, }\StringTok{\textquotesingle{}log\_reading\_time\_pred\textquotesingle{}}\NormalTok{,}
                         \DecValTok{0}\NormalTok{, }\DecValTok{20}\NormalTok{, }\FloatTok{0.25}\NormalTok{,}
                         \AttributeTok{xlab=}\StringTok{\textquotesingle{}Log Reading Time / 1 ms\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 12239 predictive values (0.1%) fell below the binning.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v=}\FunctionTok{log}\NormalTok{(}\FloatTok{1e1}\NormalTok{), }\AttributeTok{lwd=}\DecValTok{2}\NormalTok{, }\AttributeTok{lty=}\DecValTok{3}\NormalTok{, }\AttributeTok{col=}\StringTok{"\#DDDDDD"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v=}\FunctionTok{log}\NormalTok{(}\FloatTok{1e5}\NormalTok{), }\AttributeTok{lwd=}\DecValTok{2}\NormalTok{, }\AttributeTok{lty=}\DecValTok{3}\NormalTok{, }\AttributeTok{col=}\StringTok{"\#DDDDDD"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-27-1.pdf}

Conveniently the prior predictive behavior has become even more
consistent with our conservatively-elicited reading time thresholds!

This is all very exciting, but what really matters is the posterior
retrodictive performance of this new model. Before we can investigate
that, however, we first have to quantify a new posterior distribution.

\begin{codelisting}

\caption{\texttt{dlt2.stan}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{functions}\NormalTok{ \{}
  \CommentTok{// Mean{-}dispersion parameterization of inverse gamma family}
  \DataTypeTok{real}\NormalTok{ inv\_gamma\_md\_lpdf(}\DataTypeTok{real}\NormalTok{ x, }\DataTypeTok{real}\NormalTok{ log\_mu, }\DataTypeTok{real}\NormalTok{ psi) \{}
    \ControlFlowTok{return}\NormalTok{ inv\_gamma\_lpdf(x | inv(psi) + }\DecValTok{2}\NormalTok{,}
\NormalTok{                              exp(log\_mu) * (inv(psi) + }\DecValTok{1}\NormalTok{));}
\NormalTok{  \}}

  \DataTypeTok{real}\NormalTok{ inv\_gamma\_md\_rng(}\DataTypeTok{real}\NormalTok{ log\_mu, }\DataTypeTok{real}\NormalTok{ psi) \{}
    \ControlFlowTok{return}\NormalTok{ inv\_gamma\_rng(inv(psi) + }\DecValTok{2}\NormalTok{,}
\NormalTok{                         exp(log\_mu) * (inv(psi) + }\DecValTok{1}\NormalTok{));}
\NormalTok{  \}}
\NormalTok{\}}

\KeywordTok{data}\NormalTok{ \{}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N; }\CommentTok{// Number of observations}

  \CommentTok{// Item configuration}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N\_items;}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{, }\KeywordTok{upper}\NormalTok{=N\_items\textgreater{} item;}

  \CommentTok{// Subject configuration}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N\_subjects;}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{, }\KeywordTok{upper}\NormalTok{=N\_subjects\textgreater{} subject;}

  \CommentTok{// Item variant}
  \CommentTok{//   Object Relative:  subj\_rel = 0}
  \CommentTok{//   Subject Relative: subj\_rel = 1}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} subj\_rel;}

  \DataTypeTok{vector}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{}[N] reading\_time; }\CommentTok{// Reading times (ms)}
\NormalTok{\}}

\KeywordTok{parameters}\NormalTok{ \{}
  \CommentTok{// Log Reading Time Baseline}
  \DataTypeTok{real}\NormalTok{ kappa;}

  \CommentTok{// Relative item difficulties}
  \DataTypeTok{vector}\NormalTok{[N\_items {-} }\DecValTok{1}\NormalTok{] delta\_free;}

  \CommentTok{// Relative subject skills}
  \DataTypeTok{vector}\NormalTok{[N\_subjects {-} }\DecValTok{1}\NormalTok{] zeta\_free;}

  \CommentTok{// Subject Relative Difference}
  \DataTypeTok{real}\NormalTok{ chi;}

  \CommentTok{// Measurement scale}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} phi;}
\NormalTok{\}}

\KeywordTok{transformed parameters}\NormalTok{ \{}
  \CommentTok{// Relative skills for all items and subjects}
  \DataTypeTok{vector}\NormalTok{[N\_items] delta}
\NormalTok{    = append\_row([}\DecValTok{0}\NormalTok{]\textquotesingle{}, delta\_free);}
  \DataTypeTok{vector}\NormalTok{[N\_subjects] zeta}
\NormalTok{    = append\_row([}\DecValTok{0}\NormalTok{]\textquotesingle{}, zeta\_free);}
\NormalTok{\}}

\KeywordTok{model}\NormalTok{ \{}
  \CommentTok{// Prior model}

  \CommentTok{// 100 \textless{}\textasciitilde{} exp(kappa) \textless{}\textasciitilde{} 1000}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(kappa | }\FloatTok{5.76}\NormalTok{, }\FloatTok{0.50}\NormalTok{);}

  \CommentTok{// 0.1 \textless{}\textasciitilde{} exp(delta) \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(delta\_free | }\DecValTok{0}\NormalTok{, }\FloatTok{0.99}\NormalTok{);}

  \CommentTok{// 0.1 \textless{}\textasciitilde{} exp(zeta) \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(zeta\_free| }\DecValTok{0}\NormalTok{, }\FloatTok{0.99}\NormalTok{);}

  \CommentTok{// 0.1 \textless{}\textasciitilde{} exp(chi) \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(chi | }\DecValTok{0}\NormalTok{, }\FloatTok{0.99}\NormalTok{);}

  \CommentTok{// 0 \textless{}\textasciitilde{} phi \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(phi | }\DecValTok{0}\NormalTok{, }\FloatTok{3.89}\NormalTok{);}


  \CommentTok{// Observational model}
  \ControlFlowTok{for}\NormalTok{ (n }\ControlFlowTok{in} \DecValTok{1}\NormalTok{:N) \{}
    \DataTypeTok{int}\NormalTok{ i = item[n];}
    \DataTypeTok{int}\NormalTok{ s = subject[n];}

    \DataTypeTok{real}\NormalTok{ log\_mu = kappa + delta[i] {-} zeta[s];}
    \ControlFlowTok{if}\NormalTok{ (subj\_rel[n] == }\DecValTok{0}\NormalTok{) log\_mu += chi;}

    \KeywordTok{target +=}\NormalTok{ inv\_gamma\_md\_lpdf(reading\_time[n] | log\_mu, phi);}
\NormalTok{  \}}
\NormalTok{\}}

\KeywordTok{generated quantities}\NormalTok{ \{}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{real}\NormalTok{ log\_reading\_time\_pred;}

  \ControlFlowTok{for}\NormalTok{ (n }\ControlFlowTok{in} \DecValTok{1}\NormalTok{:N) \{}
    \DataTypeTok{int}\NormalTok{ i = item[n];}
    \DataTypeTok{int}\NormalTok{ s = subject[n];}

    \DataTypeTok{real}\NormalTok{ log\_mu = kappa + delta[i] {-} zeta[s];}
    \ControlFlowTok{if}\NormalTok{ (subj\_rel[n] == }\DecValTok{0}\NormalTok{) log\_mu += chi;}

\NormalTok{    log\_reading\_time\_pred[n] = log(inv\_gamma\_md\_rng(log\_mu, phi));}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{stan}\NormalTok{(}\AttributeTok{file=}\StringTok{\textquotesingle{}stan\_programs/dlt2.stan\textquotesingle{}}\NormalTok{,}
            \AttributeTok{data=}\NormalTok{data, }\AttributeTok{seed=}\DecValTok{8438338}\NormalTok{,}
            \AttributeTok{warmup=}\DecValTok{1000}\NormalTok{, }\AttributeTok{iter=}\DecValTok{2024}\NormalTok{, }\AttributeTok{refresh=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Fortunately the diagnostics are now much cleaner, indicating that our
posterior quantification should be trustworthy.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diagnostics }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{extract\_hmc\_diagnostics}\NormalTok{(fit)}
\NormalTok{util}\SpecialCharTok{$}\FunctionTok{check\_all\_hmc\_diagnostics}\NormalTok{(diagnostics)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  All Hamiltonian Monte Carlo diagnostics are consistent with reliable
Markov chain Monte Carlo.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{samples\_dlt2 }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{extract\_expectand\_vals}\NormalTok{(fit)}
\NormalTok{base\_samples }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{filter\_expectands}\NormalTok{(samples\_dlt2,}
                                       \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}kappa\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}delta\_free\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}zeta\_free\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}chi\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}phi\textquotesingle{}}\NormalTok{),}
                                       \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{util}\SpecialCharTok{$}\FunctionTok{summarize\_expectand\_diagnostics}\NormalTok{(base\_samples)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The expectands kappa, zeta_free[2], zeta_free[3], zeta_free[8],
zeta_free[13], zeta_free[16], zeta_free[20], zeta_free[22],
zeta_free[27], zeta_free[29], zeta_free[36], zeta_free[37],
zeta_free[38], zeta_free[39] triggered diagnostic warnings.

The expectands kappa, zeta_free[2], zeta_free[3], zeta_free[8],
zeta_free[13], zeta_free[16], zeta_free[20], zeta_free[22],
zeta_free[27], zeta_free[29], zeta_free[36], zeta_free[37],
zeta_free[38], zeta_free[39] triggered hat{ESS} warnings.

Small empirical effective sample sizes result in imprecise Markov chain
Monte Carlo estimators.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_ess\_hats}\NormalTok{(base\_samples, }\AttributeTok{B=}\DecValTok{50}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-30-1.pdf}

So did we do any better? The retrodictive tension in the upper tail of
the aggregate histogram is now slightly better. Frustratingly, however,
tension in the lower tail and the peak remain.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_hist\_quantiles}\NormalTok{(samples\_dlt2, }\StringTok{\textquotesingle{}log\_reading\_time\_pred\textquotesingle{}}\NormalTok{,}
                         \DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.1}\NormalTok{,}
                         \AttributeTok{baseline\_values=}\FunctionTok{log}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{reading\_time),}
                         \AttributeTok{xlab=}\StringTok{\textquotesingle{}Log Reading Time / 1 ms\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 10 predictive values (0.0%) fell below the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 172 predictive values (0.0%) fell above the binning.
\end{verbatim}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-31-1.pdf}

The tension in the histograms stratified by relative clause is similar.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{names }\OtherTok{\textless{}{-}} \FunctionTok{sapply}\NormalTok{(}\FunctionTok{which}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{subj\_rel }\SpecialCharTok{==} \DecValTok{1}\NormalTok{),}
                \ControlFlowTok{function}\NormalTok{(n) }\FunctionTok{paste0}\NormalTok{(}\StringTok{\textquotesingle{}log\_reading\_time\_pred[\textquotesingle{}}\NormalTok{, n, }\StringTok{\textquotesingle{}]\textquotesingle{}}\NormalTok{))}
\NormalTok{filtered\_samples }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{filter\_expectands}\NormalTok{(samples\_dlt2, names)}

\NormalTok{obs\_reading\_time }\OtherTok{\textless{}{-}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{reading\_time[data}\SpecialCharTok{$}\NormalTok{subj\_rel }\SpecialCharTok{==} \DecValTok{1}\NormalTok{]}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_hist\_quantiles}\NormalTok{(filtered\_samples, }\StringTok{\textquotesingle{}log\_reading\_time\_pred\textquotesingle{}}\NormalTok{,}
                         \DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.1}\NormalTok{,}
                         \AttributeTok{baseline\_values=}\FunctionTok{log}\NormalTok{(obs\_reading\_time),}
                         \AttributeTok{xlab=}\StringTok{\textquotesingle{}Log Reading Time / 1 ms\textquotesingle{}}\NormalTok{,}
                         \AttributeTok{main=}\StringTok{\textquotesingle{}Subject Relative\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 5 predictive values (0.0%) fell below the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 97 predictive values (0.0%) fell above the binning.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{names }\OtherTok{\textless{}{-}} \FunctionTok{sapply}\NormalTok{(}\FunctionTok{which}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{subj\_rel }\SpecialCharTok{==} \DecValTok{0}\NormalTok{),}
                \ControlFlowTok{function}\NormalTok{(n) }\FunctionTok{paste0}\NormalTok{(}\StringTok{\textquotesingle{}log\_reading\_time\_pred[\textquotesingle{}}\NormalTok{, n, }\StringTok{\textquotesingle{}]\textquotesingle{}}\NormalTok{))}
\NormalTok{filtered\_samples }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{filter\_expectands}\NormalTok{(samples\_dlt2, names)}

\NormalTok{obs\_reading\_time }\OtherTok{\textless{}{-}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{reading\_time[data}\SpecialCharTok{$}\NormalTok{subj\_rel }\SpecialCharTok{==} \DecValTok{0}\NormalTok{]}


\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_hist\_quantiles}\NormalTok{(filtered\_samples, }\StringTok{\textquotesingle{}log\_reading\_time\_pred\textquotesingle{}}\NormalTok{,}
                         \DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.1}\NormalTok{,}
                         \AttributeTok{baseline\_values=}\FunctionTok{log}\NormalTok{(obs\_reading\_time),}
                         \AttributeTok{xlab=}\StringTok{\textquotesingle{}Log Reading Time / 1 ms\textquotesingle{}}\NormalTok{,}
                         \AttributeTok{main=}\StringTok{\textquotesingle{}Object Relative\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 5 predictive values (0.0%) fell below the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 75 predictive values (0.0%) fell above the binning.
\end{verbatim}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-32-1.pdf}

At this point it's not clear what we can do to save the dependency
locality model. The observed data seems to be exhibiting two peaks that
the dependency locality model just can't reproduce on its own. Speaking
of multiple peaks, however, how about that direct-access model?

\subsection{Log Normal Direct-Access
Model}\label{log-normal-direct-access-model}

So long as there is a chance of initial failure, the direct-access model
always results in bimodal reading time behaviors. That might be just
want we need to resolve our retrodictive tension.

\subsubsection{Observational Model}\label{observational-model-1}

Recall that the direct-access model implies the observational models \[
  \lambda_{\text{SR}}  \,
  p_{1}( t_{ij} \mid \mu_{1, ij}, \phi_{1})
+ (1 - \lambda_{\text{SR}}) \,
  p_{2}( t_{ij} \mid \mu_{2, ij}, \phi_{2})
\] and \[
  \lambda_{\text{OR}}  \,
  p_{1}( t_{ij} \mid \mu_{1, ij}, \phi_{1})
+ (1 - \lambda_{\text{OR}}) \,
  p_{2}( t_{ij} \mid \mu_{2, ij}, \phi_{2}).
\] where \[
\mu_{1, ij}
=
\exp \left( \eta + \alpha_{i} - \beta_{j} \right)
\] and \[
\mu_{2, ij}
=
\exp \left( \eta + \omega + \alpha_{i} - \beta_{j} \right).
\]

As we did with the dependency locality model, let's start with a log
normal model for both components, \[
  \lambda_{\text{SR}}  \,
  \text{log-normal}( t_{ij} \mid \mu_{1, ij}, \phi_{1})
+ (1 - \lambda_{\text{SR}}) \,
  \text{log-normal}( t_{ij} \mid \mu_{2, ij}, \phi_{2})
\] and \[
  \lambda_{\text{OR}}  \,
  \text{log-normal}( t_{ij} \mid \mu_{1, ij}, \phi_{1})
+ (1 - \lambda_{\text{OR}}) \,
  \text{log-normal}( t_{ij} \mid \mu_{2, ij}, \phi_{2}).
\]

\subsubsection{Anchoring}\label{anchoring}

The pairwise-comparison structure of the direct-access model exhibits
the same redundancies as the pairwise-comparison structure of the
dependency locality model. Fortunately that means that we can eliminate
the redundancy with the same anchoring strategy.

Selecting a distinguished anchor item \(i'\) allows us to define
relative complexities \[
\delta_{i} = \alpha_{i} - \alpha_{i'},
\] while choosing a distinguished anchor subject \(j'\) allows us to
define relative skills, \[
\zeta_{j} = \beta_{j} - \beta_{j'}.
\]

This allows us to write \begin{align*}
\mu_{1, ij}
&=
\exp \left( \eta + \alpha_{i} - \beta_{j} \right)
\\
&=
\exp \left( \eta + \alpha_{i'} - \beta_{j'}
            + \delta_{i} - \zeta_{j} \right)
\\
&\equiv
\exp \left( \nu + \delta_{i} - \zeta_{j} \right)
\end{align*} and \begin{align*}
\mu_{2, ij}
&=
\exp \left( \eta + \omega + \alpha_{i} - \beta_{j} \right)
\\
&=
\exp \left(  \eta + \alpha_{i'} - \beta_{j'} + \omega
           + \delta_{i} - \zeta_{j} \right)
\\
&=
\exp \left(  \nu + \omega
           + \delta_{i} - \zeta_{j} \right).
\end{align*} with \[
\nu = \eta + \alpha_{i'} - \beta_{j'}.
\]

Once again we'll anchor Item 1 and Subject 1.

\subsubsection{Prior Model}\label{prior-model-1}

The relative complexity and skill parameters in the direct-access model
and dependency locality model are equivalent. Consequently we can use
the same component prior models for both, \begin{align*}
p( \delta_i ) &= \text{normal}( \delta_i \mid 0, 0.99)
\\
p( \zeta_j ) &= \text{normal}( \zeta_j \mid 0, 0.99).
\end{align*} Similarly we will use the same component prior models for
the scale parameters as we did before, \begin{align*}
p( \psi_{1} )
&= \text{half-normal} \left( \psi_{1} \mid 0, 3.89 \right)
\\
p( \psi_{2} )
&= \text{half-normal} \left( \psi_{2} \mid 0, 3.89 \right).
\end{align*}

This leaves the model configuration variables \(\nu\) and \(\omega\)
which are similar, but not exactly the same, as the variables \(\kappa\)
and \(\chi\) from the dependency locality model. Because our domain
expertise elicitation was not particularly precise, however, we would
likely end up with equivalent component prior models. Consequently we'll
immediately take \[
p( \nu ) = \text{normal}( \nu \mid 5.76, 0.50)
\]

That said, we have to be a bit more careful about the component prior
model for \(\omega\) which, unlike \(\chi\), is constrained to positive
values. Instead we'll need \begin{alignat*}{5}
& \quad 1 &
& \lessapprox &
& \; \exp( \omega ) \, &
& \lessapprox &
& \, 10
\\
& \log 1 &
& \lessapprox &
& \quad\;\; \omega &
& \lessapprox &
& \, \log 10
\\
& \quad 0 &
& \lessapprox &
& \quad\;\; \omega &
& \lessapprox &
& \log 10.
\end{alignat*} We can accomplish this the half-normal component prior
model \begin{align*}
p( \omega )
&=
\text{half-normal} \left(
\omega \mid 0, \frac{1}{2.57} \log 10 \right)
\\
&=
\text{half-normal} \left( \omega \mid 0, 0.90 \right)
\end{align*}

Lastly we need to consider component prior models for the probabilities
\(\lambda_{\text{SR}}\) and \(\lambda_{\text{OR}}\). While there is
surely wealth of knowledge about the probability of initial hypotheses
failing, that knowledge is not available to me at the moment. Given my
limited domain expertise I will consider uniform prior models for both
probabilities. If we want to be explicit about choosing uniform
component prior models, then we can write them as \begin{align*}
p( \lambda_{\text{SR}} )
&=
\text{beta}( \lambda_{\text{SR}} \mid 1, 1)
\\
p( \lambda_{\text{OR}} )
&=
\text{beta}( \lambda_{\text{OR}} \mid 1, 1).
\end{align*}

To double check that this mostly copy-and-pasted prior model behaves
reasonably well when passed through the direct-access theory model,
let's implement another prior predictive check.

\begin{codelisting}

\caption{\texttt{dat1\textbackslash\_prior.stan}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{ \{}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N; }\CommentTok{// Number of observations}

  \CommentTok{// Item configuration}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N\_items;}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{, }\KeywordTok{upper}\NormalTok{=N\_items\textgreater{} item;}

  \CommentTok{// Subject configuration}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N\_subjects;}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{, }\KeywordTok{upper}\NormalTok{=N\_subjects\textgreater{} subject;}

  \CommentTok{// Item variant}
  \CommentTok{//   Object Relative:  subj\_rel = 0}
  \CommentTok{//   Subject Relative: subj\_rel = 1}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} subj\_rel;}

  \DataTypeTok{vector}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{}[N] reading\_time; }\CommentTok{// Reading times (ms)}
\NormalTok{\}}

\KeywordTok{generated quantities}\NormalTok{ \{}
  \CommentTok{// Log reading time baseline}
  \DataTypeTok{real}\NormalTok{ nu = normal\_rng(}\FloatTok{5.76}\NormalTok{, }\FloatTok{0.50}\NormalTok{);}

  \CommentTok{// Relative item difficulties}
  \DataTypeTok{vector}\NormalTok{[N\_items {-} }\DecValTok{1}\NormalTok{] delta\_free}
\NormalTok{    = to\_vector(normal\_rng(zeros\_vector(N\_items {-} }\DecValTok{1}\NormalTok{), }\FloatTok{0.99}\NormalTok{));}

  \CommentTok{// Relative subject skills}
  \DataTypeTok{vector}\NormalTok{[N\_subjects {-} }\DecValTok{1}\NormalTok{] zeta\_free}
\NormalTok{    = to\_vector(normal\_rng(zeros\_vector(N\_subjects {-} }\DecValTok{1}\NormalTok{), }\FloatTok{0.99}\NormalTok{));}

  \CommentTok{// Initial failure difference}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} omega = abs(normal\_rng(}\DecValTok{0}\NormalTok{, }\FloatTok{0.90}\NormalTok{));}

  \CommentTok{// Measurement scales}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} phi1 = abs(normal\_rng(}\DecValTok{0}\NormalTok{, }\FloatTok{3.89}\NormalTok{));}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} phi2 = abs(normal\_rng(}\DecValTok{0}\NormalTok{, }\FloatTok{3.89}\NormalTok{));}

  \CommentTok{// Initial failure probabilities}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} lambda\_SR = beta\_rng(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{);}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} lambda\_OR = beta\_rng(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{);}

  \CommentTok{// Relative skills for all items and subjects}
  \DataTypeTok{vector}\NormalTok{[N\_items] delta}
\NormalTok{    = append\_row([}\DecValTok{0}\NormalTok{]\textquotesingle{}, delta\_free);}
  \DataTypeTok{vector}\NormalTok{[N\_subjects] zeta}
\NormalTok{    = append\_row([}\DecValTok{0}\NormalTok{]\textquotesingle{}, zeta\_free);}

  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{real}\NormalTok{ log\_reading\_time\_pred;}

  \ControlFlowTok{for}\NormalTok{ (n }\ControlFlowTok{in} \DecValTok{1}\NormalTok{:N) \{}
    \DataTypeTok{int}\NormalTok{ i = item[n];}
    \DataTypeTok{int}\NormalTok{ s = subject[n];}
    \DataTypeTok{real}\NormalTok{ log\_mu = nu + delta[i] {-} zeta[s];}

    \ControlFlowTok{if}\NormalTok{ (subj\_rel[n] == }\DecValTok{1}\NormalTok{) \{}
      \ControlFlowTok{if}\NormalTok{ (bernoulli\_rng(lambda\_SR)) \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(lognormal\_rng(log\_mu,         phi1));}
\NormalTok{      \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(lognormal\_rng(log\_mu + omega, phi2));}
\NormalTok{      \}}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
       \ControlFlowTok{if}\NormalTok{ (bernoulli\_rng(lambda\_OR)) \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(lognormal\_rng(log\_mu,         phi1));}
\NormalTok{      \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(lognormal\_rng(log\_mu + omega, phi2));}
\NormalTok{      \}}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{stan}\NormalTok{(}\AttributeTok{file=}\StringTok{"stan\_programs/dat1\_prior.stan"}\NormalTok{,}
            \AttributeTok{data=}\NormalTok{data, }\AttributeTok{seed=}\DecValTok{8438338}\NormalTok{, }\AttributeTok{algorithm=}\StringTok{"Fixed\_param"}\NormalTok{,}
            \AttributeTok{warmup=}\DecValTok{0}\NormalTok{, }\AttributeTok{iter=}\DecValTok{1024}\NormalTok{, }\AttributeTok{refresh=}\DecValTok{0}\NormalTok{)}

\NormalTok{samples }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{extract\_expectand\_vals}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_hist\_quantiles}\NormalTok{(samples, }\StringTok{\textquotesingle{}log\_reading\_time\_pred\textquotesingle{}}\NormalTok{,}
                         \DecValTok{0}\NormalTok{, }\DecValTok{20}\NormalTok{, }\FloatTok{0.25}\NormalTok{,}
                         \AttributeTok{xlab=}\StringTok{\textquotesingle{}Log Reading Time / 1 ms\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 643174 predictive values (5.7%) fell below the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 62182 predictive values (0.6%) fell above the binning.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v=}\FunctionTok{log}\NormalTok{(}\FloatTok{1e1}\NormalTok{), }\AttributeTok{lwd=}\DecValTok{2}\NormalTok{, }\AttributeTok{lty=}\DecValTok{3}\NormalTok{, }\AttributeTok{col=}\StringTok{"\#DDDDDD"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v=}\FunctionTok{log}\NormalTok{(}\FloatTok{1e5}\NormalTok{), }\AttributeTok{lwd=}\DecValTok{2}\NormalTok{, }\AttributeTok{lty=}\DecValTok{3}\NormalTok{, }\AttributeTok{col=}\StringTok{"\#DDDDDD"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-34-1.pdf}

Fortunately there don't seem to be any surprises. The prior predictive
behavior is very similar to what we saw with the log-normal locality
dependency model.

\subsubsection{Posterior Quantification}\label{posterior-quantification}

Having taken care to both implement the observational model without
redundancy and consider all our prior modeling assumptions, we are ready
for inference.

\begin{codelisting}

\caption{\texttt{dat1.stan}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{ \{}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N; }\CommentTok{// Number of observations}

  \CommentTok{// Item configuration}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N\_items;}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{, }\KeywordTok{upper}\NormalTok{=N\_items\textgreater{} item;}

  \CommentTok{// Subject configuration}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N\_subjects;}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{, }\KeywordTok{upper}\NormalTok{=N\_subjects\textgreater{} subject;}

  \CommentTok{// Item variant}
  \CommentTok{//   Object Relative:  subj\_rel = 0}
  \CommentTok{//   Subject Relative: subj\_rel = 1}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} subj\_rel;}

  \DataTypeTok{vector}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{}[N] reading\_time; }\CommentTok{// Reading times (ms)}
\NormalTok{\}}

\KeywordTok{parameters}\NormalTok{ \{}
  \CommentTok{// Log reading time baseline for successful retrieval}
  \DataTypeTok{real}\NormalTok{ nu;}

  \CommentTok{// Relative item difficulties}
  \DataTypeTok{vector}\NormalTok{[N\_items {-} }\DecValTok{1}\NormalTok{] delta\_free;}

  \CommentTok{// Relative subject skills}
  \DataTypeTok{vector}\NormalTok{[N\_subjects {-} }\DecValTok{1}\NormalTok{] zeta\_free;}

  \CommentTok{// Log offset for initial failure before successful retrieval}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} omega;}

  \CommentTok{// Measurement scales}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} phi1;}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} phi2;}

  \CommentTok{// Mixture probabilities}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} lambda\_SR;}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} lambda\_OR;}
\NormalTok{\}}

\KeywordTok{transformed parameters}\NormalTok{ \{}
  \CommentTok{// Relative skills for all items and subjects}
  \DataTypeTok{vector}\NormalTok{[N\_items] delta}
\NormalTok{    = append\_row([}\DecValTok{0}\NormalTok{]\textquotesingle{}, delta\_free);}
  \DataTypeTok{vector}\NormalTok{[N\_subjects] zeta}
\NormalTok{    = append\_row([}\DecValTok{0}\NormalTok{]\textquotesingle{}, zeta\_free);}
\NormalTok{\}}

\KeywordTok{model}\NormalTok{ \{}
  \CommentTok{// Prior model}

  \CommentTok{// 100 \textless{}\textasciitilde{} exp(nu) \textless{}\textasciitilde{} 1000}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(nu | }\FloatTok{5.76}\NormalTok{, }\FloatTok{0.50}\NormalTok{);}

  \CommentTok{// 0.1 \textless{}\textasciitilde{} exp(delta) \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(delta\_free | }\DecValTok{0}\NormalTok{, }\FloatTok{0.99}\NormalTok{);}

  \CommentTok{// 0.1 \textless{}\textasciitilde{} exp(zeta) \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(zeta\_free| }\DecValTok{0}\NormalTok{, }\FloatTok{0.99}\NormalTok{);}

  \CommentTok{// 1 \textless{}\textasciitilde{} exp(omega) \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(omega | }\DecValTok{0}\NormalTok{, }\FloatTok{0.90}\NormalTok{);}

  \CommentTok{// 0 \textless{}\textasciitilde{} phi \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(phi1 | }\DecValTok{0}\NormalTok{, }\FloatTok{3.89}\NormalTok{);}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(phi2 | }\DecValTok{0}\NormalTok{, }\FloatTok{3.89}\NormalTok{);}

  \CommentTok{// Uniform prior density functions}
  \KeywordTok{target +=}\NormalTok{ beta\_lpdf(lambda\_SR | }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{);}
  \KeywordTok{target +=}\NormalTok{ beta\_lpdf(lambda\_OR | }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{);}

  \CommentTok{// Observational model}
  \ControlFlowTok{for}\NormalTok{ (n }\ControlFlowTok{in} \DecValTok{1}\NormalTok{:N) \{}
    \DataTypeTok{int}\NormalTok{ i = item[n];}
    \DataTypeTok{int}\NormalTok{ s = subject[n];}
    \DataTypeTok{real}\NormalTok{ log\_mu = nu + delta[i] {-} zeta[s];}

    \ControlFlowTok{if}\NormalTok{ (subj\_rel[n] == }\DecValTok{1}\NormalTok{) \{}
      \DataTypeTok{real}\NormalTok{ lpd1}
\NormalTok{        =   log(lambda\_SR)}
\NormalTok{          + lognormal\_lpdf(reading\_time[n] | log\_mu,         phi1);}
      \DataTypeTok{real}\NormalTok{ lpd2}
\NormalTok{        =   log(}\DecValTok{1}\NormalTok{ {-} lambda\_SR)}
\NormalTok{          + lognormal\_lpdf(reading\_time[n] | log\_mu + omega, phi2);}

      \KeywordTok{target +=}\NormalTok{ log\_sum\_exp(lpd1, lpd2);}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
      \DataTypeTok{real}\NormalTok{ lpd1}
\NormalTok{        =   log(lambda\_OR)}
\NormalTok{          + lognormal\_lpdf(reading\_time[n] | log\_mu,         phi1);}
      \DataTypeTok{real}\NormalTok{ lpd2}
\NormalTok{        =   log(}\DecValTok{1}\NormalTok{ {-} lambda\_OR)}
\NormalTok{          + lognormal\_lpdf(reading\_time[n] | log\_mu + omega, phi2);}

      \KeywordTok{target +=}\NormalTok{ log\_sum\_exp(lpd1, lpd2);}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\}}

\KeywordTok{generated quantities}\NormalTok{ \{}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{real}\NormalTok{ log\_reading\_time\_pred;}

  \ControlFlowTok{for}\NormalTok{ (n }\ControlFlowTok{in} \DecValTok{1}\NormalTok{:N) \{}
    \DataTypeTok{int}\NormalTok{ i = item[n];}
    \DataTypeTok{int}\NormalTok{ s = subject[n];}
    \DataTypeTok{real}\NormalTok{ log\_mu = nu + delta[i] {-} zeta[s];}

    \ControlFlowTok{if}\NormalTok{ (subj\_rel[n] == }\DecValTok{1}\NormalTok{) \{}
      \ControlFlowTok{if}\NormalTok{ (bernoulli\_rng(lambda\_SR)) \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(lognormal\_rng(log\_mu,         phi1));}
\NormalTok{      \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(lognormal\_rng(log\_mu + omega, phi2));}
\NormalTok{      \}}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
       \ControlFlowTok{if}\NormalTok{ (bernoulli\_rng(lambda\_OR)) \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(lognormal\_rng(log\_mu,         phi1));}
\NormalTok{      \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(lognormal\_rng(log\_mu + omega, phi2));}
\NormalTok{      \}}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{stan}\NormalTok{(}\AttributeTok{file=}\StringTok{\textquotesingle{}stan\_programs/dat1.stan\textquotesingle{}}\NormalTok{,}
            \AttributeTok{data=}\NormalTok{data, }\AttributeTok{seed=}\DecValTok{8438338}\NormalTok{,}
            \AttributeTok{warmup=}\DecValTok{1000}\NormalTok{, }\AttributeTok{iter=}\DecValTok{2024}\NormalTok{, }\AttributeTok{refresh=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Unfortunately, multiple diagnostics indicate that our posterior
computation is not to be trusted.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diagnostics }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{extract\_hmc\_diagnostics}\NormalTok{(fit)}
\NormalTok{util}\SpecialCharTok{$}\FunctionTok{check\_all\_hmc\_diagnostics}\NormalTok{(diagnostics)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  All Hamiltonian Monte Carlo diagnostics are consistent with reliable
Markov chain Monte Carlo.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{samples\_dat1 }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{extract\_expectand\_vals}\NormalTok{(fit)}
\NormalTok{base\_samples }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{filter\_expectands}\NormalTok{(samples\_dat1,}
                                       \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}nu\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}delta\_free\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}zeta\_free\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}omega\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}phi1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}phi2\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}lambda\_SR\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}lambda\_OR\textquotesingle{}}\NormalTok{),}
                                       \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{util}\SpecialCharTok{$}\FunctionTok{summarize\_expectand\_diagnostics}\NormalTok{(base\_samples)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The expectands nu, delta_free[6], delta_free[7], delta_free[9],
delta_free[14], delta_free[15], zeta_free[1], zeta_free[2],
zeta_free[3], zeta_free[4], zeta_free[5], zeta_free[6], zeta_free[7],
zeta_free[8], zeta_free[10], zeta_free[11], zeta_free[13],
zeta_free[14], zeta_free[15], zeta_free[16], zeta_free[17],
zeta_free[18], zeta_free[19], zeta_free[20], zeta_free[22],
zeta_free[23], zeta_free[25], zeta_free[27], zeta_free[28],
zeta_free[29], zeta_free[30], zeta_free[31], zeta_free[32],
zeta_free[33], zeta_free[34], zeta_free[35], zeta_free[36],
zeta_free[37], zeta_free[38], zeta_free[39], omega, phi1, lambda_OR
triggered diagnostic warnings.

The expectands nu, delta_free[6], delta_free[7], delta_free[9],
delta_free[14], delta_free[15], zeta_free[1], zeta_free[5],
zeta_free[8], zeta_free[13], zeta_free[14], zeta_free[16],
zeta_free[18], zeta_free[25], zeta_free[27], zeta_free[28],
zeta_free[29], zeta_free[30], zeta_free[31], zeta_free[32],
zeta_free[37], zeta_free[38], zeta_free[39], omega, phi1, lambda_OR
triggered hat{R} warnings.

Split Rhat larger than 1.1 suggests that at least one of the Markov
chains has not reached an equilibrium.

The expectands nu, zeta_free[1], zeta_free[2], zeta_free[3],
zeta_free[4], zeta_free[5], zeta_free[6], zeta_free[7], zeta_free[8],
zeta_free[10], zeta_free[11], zeta_free[14], zeta_free[15],
zeta_free[16], zeta_free[17], zeta_free[18], zeta_free[19],
zeta_free[20], zeta_free[22], zeta_free[23], zeta_free[25],
zeta_free[27], zeta_free[30], zeta_free[31], zeta_free[32],
zeta_free[33], zeta_free[34], zeta_free[35], zeta_free[36],
zeta_free[37], zeta_free[38] triggered hat{ESS} warnings.

Small empirical effective sample sizes result in imprecise Markov chain
Monte Carlo estimators.
\end{verbatim}

The immediate concern here are the split \(\hat{R}\) warnings which
suggest inferential multimodality.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_rhats}\NormalTok{(base\_samples, }\AttributeTok{B=}\DecValTok{50}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-37-1.pdf}

Indeed the first and third Markov chains appear to be exploring one set
of behaviors while the second and fourth Markov chains are exploring an
entirely different set of behaviors. The latter two Markov chains are
exploring model configurations where \(\omega\) is consistent with zero.
In this case the two mixture components are equivalent, and every value
of \(\lambda_{\text{SR}}\) and \(\lambda_{\text{OR}}\) yields the same
reading time behaviors.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_pairs\_by\_chain}\NormalTok{(samples\_dat1[[}\StringTok{\textquotesingle{}omega\textquotesingle{}}\NormalTok{]], }\StringTok{\textquotesingle{}omega\textquotesingle{}}\NormalTok{,}
\NormalTok{                         samples\_dat1[[}\StringTok{\textquotesingle{}lambda\_SR\textquotesingle{}}\NormalTok{]], }\StringTok{\textquotesingle{}lambda\_SR\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-38-1.pdf}

There is no guarantee that these are all of the posterior modes, nor
that either of these modes actually provide a non-negligible influence
to posterior inferences. Multimodal posterior distributions are
notoriously difficult to quantify, but at least Markov chain Monte Carlo
provides useful diagnostic information.

\subsubsection{Posterior Retrodictive
Checks}\label{posterior-retrodictive-checks-1}

One way to qualify the relative importance of each mode is to compare
how consistent they are with the observed data. In other words we can
perform posterior retrodictive checks separately for each pair of Markov
chains.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_hist\_quantiles}\NormalTok{(}\FunctionTok{lapply}\NormalTok{(samples\_dat1,}
                                \ControlFlowTok{function}\NormalTok{(s) s[}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{),]),}
                         \StringTok{\textquotesingle{}log\_reading\_time\_pred\textquotesingle{}}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.1}\NormalTok{,}
                         \AttributeTok{baseline\_values=}\FunctionTok{log}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{reading\_time),}
                         \AttributeTok{xlab=}\StringTok{\textquotesingle{}Log Reading Time / 1 ms\textquotesingle{}}\NormalTok{,}
                         \AttributeTok{main=}\StringTok{\textquotesingle{}Markov Chains 1 and 3\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 1437 predictive values (0.0%) fell below the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 23 predictive values (0.0%) fell above the binning.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_hist\_quantiles}\NormalTok{(}\FunctionTok{lapply}\NormalTok{(samples\_dat1,}
                                \ControlFlowTok{function}\NormalTok{(s) s[}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{),]),}
                         \StringTok{\textquotesingle{}log\_reading\_time\_pred\textquotesingle{}}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.1}\NormalTok{,}
                         \AttributeTok{baseline\_values=}\FunctionTok{log}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{reading\_time),}
                         \AttributeTok{xlab=}\StringTok{\textquotesingle{}Log Reading Time / 1 ms\textquotesingle{}}\NormalTok{,}
                         \AttributeTok{main=}\StringTok{\textquotesingle{}Markov Chains 2 and 4\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 18 predictive values (0.0%) fell below the binning.
\end{verbatim}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-39-1.pdf}

Because \(\omega\) is so close to zero in the model configurations
explored by the second and forth Markov chains, the posterior predictive
reading time behavior exhibits only a single peak which clashes with the
observed reading time behavior. On the other hand, the first and third
Markov chains appear to be exploring behaviors that are much more
consistent with the observed data. Formally quantifying the relative
consistency, however, is difficult.

All of that said, the retrodictive performance for the first and third
Markov chains still leaves something to be desired. If we zoom in we see
that, while the posterior predictive behavior is able to replicate the
observed peaks, the posterior predictive tails are not quite right.
Similar to what we saw with the log-normal dependency locality model,
the posterior predictive lower tail is too heavy and the posterior
predictive upper tail is too light.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_hist\_quantiles}\NormalTok{(}\FunctionTok{lapply}\NormalTok{(samples\_dat1,}
                                \ControlFlowTok{function}\NormalTok{(s) s[}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{),]),}
                         \StringTok{\textquotesingle{}log\_reading\_time\_pred\textquotesingle{}}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.1}\NormalTok{,}
                         \AttributeTok{baseline\_values=}\FunctionTok{log}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{reading\_time),}
                         \AttributeTok{xlab=}\StringTok{\textquotesingle{}Log Reading Time / 1 ms\textquotesingle{}}\NormalTok{,}
                         \AttributeTok{main=}\StringTok{\textquotesingle{}Markov Chains 1 and 3\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 1437 predictive values (0.0%) fell below the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 23 predictive values (0.0%) fell above the binning.
\end{verbatim}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-40-1.pdf}

Even if the mode explored by the second and forth Markov chains is
irrelevant, the posterior retrodictive tension in the mode explored by
the first and third Markov chains suggests that we can improve the
direct-access model by adding a bit more asymmetry to the component
models.

\subsection{Inverse Gamma Direct-Access
Model}\label{inverse-gamma-direct-access-model}

To improve our initial direct-access model, let's try the same strategy
that we attempted with the dependency locality model: replacing the
log-normal measurement variability models with inverse gamma models.

\subsubsection{Prior Model}\label{prior-model-2}

As we did in \href{@sec:dlt2}{Section 4.2} we will carry over our
initial prior model to this second model, even though the inverse gamma
scale parameters behave slightly differently from the log normal scale
parameters. To double check that this doesn't cause any problems we'll
run another quick prior predictive check.

Fortunately everything seems reasonably consistent with our elicited
domain expertise.

\begin{codelisting}

\caption{\texttt{dat2\textbackslash\_prior.stan}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{ \{}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N; }\CommentTok{// Number of observations}

  \CommentTok{// Item configuration}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N\_items;}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{, }\KeywordTok{upper}\NormalTok{=N\_items\textgreater{} item;}

  \CommentTok{// Subject configuration}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N\_subjects;}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{, }\KeywordTok{upper}\NormalTok{=N\_subjects\textgreater{} subject;}

  \CommentTok{// Item variant}
  \CommentTok{//   Object Relative:  subj\_rel = 0}
  \CommentTok{//   Subject Relative: subj\_rel = 1}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} subj\_rel;}

  \DataTypeTok{vector}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{}[N] reading\_time; }\CommentTok{// Reading times (ms)}
\NormalTok{\}}

\KeywordTok{generated quantities}\NormalTok{ \{}
  \CommentTok{// Log reading time baseline}
  \DataTypeTok{real}\NormalTok{ nu = normal\_rng(}\FloatTok{5.76}\NormalTok{, }\FloatTok{0.50}\NormalTok{);}

  \CommentTok{// Relative item difficulties}
  \DataTypeTok{vector}\NormalTok{[N\_items {-} }\DecValTok{1}\NormalTok{] delta\_free}
\NormalTok{    = to\_vector(normal\_rng(zeros\_vector(N\_items {-} }\DecValTok{1}\NormalTok{), }\FloatTok{0.99}\NormalTok{));}

  \CommentTok{// Relative subject skills}
  \DataTypeTok{vector}\NormalTok{[N\_subjects {-} }\DecValTok{1}\NormalTok{] zeta\_free}
\NormalTok{    = to\_vector(normal\_rng(zeros\_vector(N\_subjects {-} }\DecValTok{1}\NormalTok{), }\FloatTok{0.99}\NormalTok{));}

  \CommentTok{// Initial failure difference}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} omega = abs(normal\_rng(}\DecValTok{0}\NormalTok{, }\FloatTok{0.90}\NormalTok{));}

  \CommentTok{// Measurement scales}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} phi1 = abs(normal\_rng(}\DecValTok{0}\NormalTok{, }\FloatTok{3.89}\NormalTok{));}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} phi2 = abs(normal\_rng(}\DecValTok{0}\NormalTok{, }\FloatTok{3.89}\NormalTok{));}

  \CommentTok{// Initial failure probabilities}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} lambda\_SR = beta\_rng(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{);}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} lambda\_OR = beta\_rng(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{);}

  \CommentTok{// Relative skills for all items and subjects}
  \DataTypeTok{vector}\NormalTok{[N\_items] delta}
\NormalTok{    = append\_row([}\DecValTok{0}\NormalTok{]\textquotesingle{}, delta\_free);}
  \DataTypeTok{vector}\NormalTok{[N\_subjects] zeta}
\NormalTok{    = append\_row([}\DecValTok{0}\NormalTok{]\textquotesingle{}, zeta\_free);}

  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{real}\NormalTok{ log\_reading\_time\_pred;}

  \ControlFlowTok{for}\NormalTok{ (n }\ControlFlowTok{in} \DecValTok{1}\NormalTok{:N) \{}
    \DataTypeTok{int}\NormalTok{ i = item[n];}
    \DataTypeTok{int}\NormalTok{ s = subject[n];}
    \DataTypeTok{real}\NormalTok{ log\_mu = nu + delta[i] {-} zeta[s];}

    \ControlFlowTok{if}\NormalTok{ (subj\_rel[n] == }\DecValTok{1}\NormalTok{) \{}
      \ControlFlowTok{if}\NormalTok{ (bernoulli\_rng(lambda\_SR)) \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(lognormal\_rng(log\_mu,         phi1));}
\NormalTok{      \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(lognormal\_rng(log\_mu + omega, phi2));}
\NormalTok{      \}}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
       \ControlFlowTok{if}\NormalTok{ (bernoulli\_rng(lambda\_OR)) \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(lognormal\_rng(log\_mu,         phi1));}
\NormalTok{      \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(lognormal\_rng(log\_mu + omega, phi2));}
\NormalTok{      \}}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{stan}\NormalTok{(}\AttributeTok{file=}\StringTok{"stan\_programs/dat2\_prior.stan"}\NormalTok{,}
            \AttributeTok{data=}\NormalTok{data, }\AttributeTok{seed=}\DecValTok{8438338}\NormalTok{, }\AttributeTok{algorithm=}\StringTok{"Fixed\_param"}\NormalTok{,}
            \AttributeTok{warmup=}\DecValTok{0}\NormalTok{, }\AttributeTok{iter=}\DecValTok{1024}\NormalTok{, }\AttributeTok{refresh=}\DecValTok{0}\NormalTok{)}

\NormalTok{samples }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{extract\_expectand\_vals}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_hist\_quantiles}\NormalTok{(samples, }\StringTok{\textquotesingle{}log\_reading\_time\_pred\textquotesingle{}}\NormalTok{,}
                         \DecValTok{0}\NormalTok{, }\DecValTok{20}\NormalTok{, }\FloatTok{0.25}\NormalTok{,}
                         \AttributeTok{xlab=}\StringTok{\textquotesingle{}Log Reading Time / 1 ms\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 643174 predictive values (5.7%) fell below the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 62182 predictive values (0.6%) fell above the binning.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v=}\FunctionTok{log}\NormalTok{(}\FloatTok{1e1}\NormalTok{), }\AttributeTok{lwd=}\DecValTok{2}\NormalTok{, }\AttributeTok{lty=}\DecValTok{3}\NormalTok{, }\AttributeTok{col=}\StringTok{"\#DDDDDD"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v=}\FunctionTok{log}\NormalTok{(}\FloatTok{1e5}\NormalTok{), }\AttributeTok{lwd=}\DecValTok{2}\NormalTok{, }\AttributeTok{lty=}\DecValTok{3}\NormalTok{, }\AttributeTok{col=}\StringTok{"\#DDDDDD"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-42-1.pdf}

\subsubsection{Posterior
Quantification}\label{posterior-quantification-1}

We now set our Markov chains free and cross our fingers.

\begin{codelisting}

\caption{\texttt{dat2.stan}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{functions}\NormalTok{ \{}
  \CommentTok{// Mean{-}dispersion parameterization of inverse gamma family}
  \DataTypeTok{real}\NormalTok{ inv\_gamma\_md\_lpdf(}\DataTypeTok{real}\NormalTok{ x, }\DataTypeTok{real}\NormalTok{ log\_mu, }\DataTypeTok{real}\NormalTok{ psi) \{}
    \ControlFlowTok{return}\NormalTok{ inv\_gamma\_lpdf(x | inv(psi) + }\DecValTok{2}\NormalTok{,}
\NormalTok{                              exp(log\_mu) * (inv(psi) + }\DecValTok{1}\NormalTok{));}
\NormalTok{  \}}

  \DataTypeTok{real}\NormalTok{ inv\_gamma\_md\_rng(}\DataTypeTok{real}\NormalTok{ log\_mu, }\DataTypeTok{real}\NormalTok{ psi) \{}
    \ControlFlowTok{return}\NormalTok{ inv\_gamma\_rng(inv(psi) + }\DecValTok{2}\NormalTok{,}
\NormalTok{                         exp(log\_mu) * (inv(psi) + }\DecValTok{1}\NormalTok{));}
\NormalTok{  \}}
\NormalTok{\}}

\KeywordTok{data}\NormalTok{ \{}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N; }\CommentTok{// Number of observations}

  \CommentTok{// Item configuration}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N\_items;}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{, }\KeywordTok{upper}\NormalTok{=N\_items\textgreater{} item;}

  \CommentTok{// Subject configuration}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N\_subjects;}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{, }\KeywordTok{upper}\NormalTok{=N\_subjects\textgreater{} subject;}

  \CommentTok{// Item variant}
  \CommentTok{//   Object Relative:  subj\_rel = 0}
  \CommentTok{//   Subject Relative: subj\_rel = 1}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} subj\_rel;}

  \DataTypeTok{vector}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{}[N] reading\_time; }\CommentTok{// Reading times (ms)}
\NormalTok{\}}

\KeywordTok{parameters}\NormalTok{ \{}
  \CommentTok{// Log reading time baseline for successful retrieval}
  \DataTypeTok{real}\NormalTok{ nu;}

  \CommentTok{// Relative item difficulties}
  \DataTypeTok{vector}\NormalTok{[N\_items {-} }\DecValTok{1}\NormalTok{] delta\_free;}

  \CommentTok{// Relative subject skills}
  \DataTypeTok{vector}\NormalTok{[N\_subjects {-} }\DecValTok{1}\NormalTok{] zeta\_free;}

  \CommentTok{// Log offset for initial failure before successful retrieval}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} omega;}

  \CommentTok{// Measurement scales}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} phi1;}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} phi2;}

  \CommentTok{// Mixture probabilities}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} lambda\_SR;}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} lambda\_OR;}
\NormalTok{\}}

\KeywordTok{transformed parameters}\NormalTok{ \{}
  \CommentTok{// Relative skills for all items and subjects}
  \DataTypeTok{vector}\NormalTok{[N\_items] delta}
\NormalTok{    = append\_row([}\DecValTok{0}\NormalTok{]\textquotesingle{}, delta\_free);}
  \DataTypeTok{vector}\NormalTok{[N\_subjects] zeta}
\NormalTok{    = append\_row([}\DecValTok{0}\NormalTok{]\textquotesingle{}, zeta\_free);}
\NormalTok{\}}

\KeywordTok{model}\NormalTok{ \{}
  \CommentTok{// Prior model}

  \CommentTok{// 100 \textless{}\textasciitilde{} exp(nu) \textless{}\textasciitilde{} 1000}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(nu | }\FloatTok{5.76}\NormalTok{, }\FloatTok{0.50}\NormalTok{);}

  \CommentTok{// 0.1 \textless{}\textasciitilde{} exp(delta) \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(delta\_free | }\DecValTok{0}\NormalTok{, }\FloatTok{0.99}\NormalTok{);}

  \CommentTok{// 0.1 \textless{}\textasciitilde{} exp(zeta) \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(zeta\_free| }\DecValTok{0}\NormalTok{, }\FloatTok{0.99}\NormalTok{);}

  \CommentTok{// 1 \textless{}\textasciitilde{} exp(omega) \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(omega | }\DecValTok{0}\NormalTok{, }\FloatTok{0.90}\NormalTok{);}

  \CommentTok{// 0 \textless{}\textasciitilde{} phi \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(phi1 | }\DecValTok{0}\NormalTok{, }\FloatTok{3.89}\NormalTok{);}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(phi2 | }\DecValTok{0}\NormalTok{, }\FloatTok{3.89}\NormalTok{);}

  \CommentTok{// Uniform prior density functions}
  \KeywordTok{target +=}\NormalTok{ beta\_lpdf(lambda\_SR | }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{);}
  \KeywordTok{target +=}\NormalTok{ beta\_lpdf(lambda\_OR | }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{);}

  \CommentTok{// Observational model}
  \ControlFlowTok{for}\NormalTok{ (n }\ControlFlowTok{in} \DecValTok{1}\NormalTok{:N) \{}
    \DataTypeTok{int}\NormalTok{ i = item[n];}
    \DataTypeTok{int}\NormalTok{ s = subject[n];}
    \DataTypeTok{real}\NormalTok{ log\_mu = nu + delta[i] {-} zeta[s];}

    \ControlFlowTok{if}\NormalTok{ (subj\_rel[n] == }\DecValTok{1}\NormalTok{) \{}
      \DataTypeTok{real}\NormalTok{ lpd1}
\NormalTok{        =   log(lambda\_SR)}
\NormalTok{          + inv\_gamma\_md\_lpdf(reading\_time[n] | log\_mu,         phi1);}
      \DataTypeTok{real}\NormalTok{ lpd2}
\NormalTok{        =   log(}\DecValTok{1}\NormalTok{ {-} lambda\_SR)}
\NormalTok{          + inv\_gamma\_md\_lpdf(reading\_time[n] | log\_mu + omega, phi2);}

      \KeywordTok{target +=}\NormalTok{ log\_sum\_exp(lpd1, lpd2);}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
      \DataTypeTok{real}\NormalTok{ lpd1}
\NormalTok{        =   log(lambda\_OR)}
\NormalTok{          + inv\_gamma\_md\_lpdf(reading\_time[n] | log\_mu,         phi1);}
      \DataTypeTok{real}\NormalTok{ lpd2}
\NormalTok{        =   log(}\DecValTok{1}\NormalTok{ {-} lambda\_OR)}
\NormalTok{          + inv\_gamma\_md\_lpdf(reading\_time[n] | log\_mu + omega, phi2);}

      \KeywordTok{target +=}\NormalTok{ log\_sum\_exp(lpd1, lpd2);}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\}}

\KeywordTok{generated quantities}\NormalTok{ \{}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{real}\NormalTok{ log\_reading\_time\_pred;}

  \ControlFlowTok{for}\NormalTok{ (n }\ControlFlowTok{in} \DecValTok{1}\NormalTok{:N) \{}
    \DataTypeTok{int}\NormalTok{ i = item[n];}
    \DataTypeTok{int}\NormalTok{ s = subject[n];}
    \DataTypeTok{real}\NormalTok{ log\_mu = nu + delta[i] {-} zeta[s];}

    \ControlFlowTok{if}\NormalTok{ (subj\_rel[n] == }\DecValTok{1}\NormalTok{) \{}
      \ControlFlowTok{if}\NormalTok{ (bernoulli\_rng(lambda\_SR)) \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(inv\_gamma\_md\_rng(log\_mu,         phi1));}
\NormalTok{      \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(inv\_gamma\_md\_rng(log\_mu + omega, phi2));}
\NormalTok{      \}}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
       \ControlFlowTok{if}\NormalTok{ (bernoulli\_rng(lambda\_OR)) \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(inv\_gamma\_md\_rng(log\_mu,         phi1));}
\NormalTok{      \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(inv\_gamma\_md\_rng(log\_mu + omega, phi2));}
\NormalTok{      \}}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{stan}\NormalTok{(}\AttributeTok{file=}\StringTok{\textquotesingle{}stan\_programs/dat2.stan\textquotesingle{}}\NormalTok{,}
            \AttributeTok{data=}\NormalTok{data, }\AttributeTok{seed=}\DecValTok{8438338}\NormalTok{,}
            \AttributeTok{warmup=}\DecValTok{1000}\NormalTok{, }\AttributeTok{iter=}\DecValTok{2024}\NormalTok{, }\AttributeTok{refresh=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Has posterior quantification become any better with this new model?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diagnostics }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{extract\_hmc\_diagnostics}\NormalTok{(fit)}
\NormalTok{util}\SpecialCharTok{$}\FunctionTok{check\_all\_hmc\_diagnostics}\NormalTok{(diagnostics)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  All Hamiltonian Monte Carlo diagnostics are consistent with reliable
Markov chain Monte Carlo.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{samples\_dat2 }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{extract\_expectand\_vals}\NormalTok{(fit)}
\NormalTok{base\_samples }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{filter\_expectands}\NormalTok{(samples\_dat2,}
                                       \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}nu\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}delta\_free\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}zeta\_free\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}omega\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}phi1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}phi2\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}lambda\_SR\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}lambda\_OR\textquotesingle{}}\NormalTok{),}
                                       \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{util}\SpecialCharTok{$}\FunctionTok{check\_all\_expectand\_diagnostics}\NormalTok{(base\_samples)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
nu:
  Chain 1: hat{ESS} (95.858) is smaller than desired (100).

zeta_free[10]:
  Chain 1: hat{ESS} (90.622) is smaller than desired (100).

zeta_free[15]:
  Chain 1: hat{ESS} (94.220) is smaller than desired (100).

zeta_free[33]:
  Chain 1: hat{ESS} (86.969) is smaller than desired (100).

zeta_free[35]:
  Chain 1: hat{ESS} (97.021) is smaller than desired (100).


Small empirical effective sample sizes result in imprecise Markov chain
Monte Carlo estimators.
\end{verbatim}

The split \(\hat{R}\) warnings are nowhere to be seen, and only a few
empirical effective sample size warnings persist. Because those low
empirical effective sample sizes are all close to the desired threshold,
and no other diagnostics are indicating concern, we can be relatively
confident in the accuracy of our posterior computation.

\subsubsection{Retrodictive Checks}\label{retrodictive-checks}

The diagnostics indicate that our estimated posterior predictive
behavior should be faithful to the exact posterior predictive behavior.
This allows us to make meaningful posterior retrodictive checks. And
what beautiful checks they are.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_hist\_quantiles}\NormalTok{(samples\_dat2, }\StringTok{\textquotesingle{}log\_reading\_time\_pred\textquotesingle{}}\NormalTok{,}
                         \DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.1}\NormalTok{,}
                         \AttributeTok{baseline\_values=}\FunctionTok{log}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{reading\_time),}
                         \AttributeTok{xlab=}\StringTok{\textquotesingle{}Log Reading Time / 1 ms\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 73 predictive values (0.0%) fell above the binning.
\end{verbatim}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-45-1.pdf}

The posterior predictive reading times now match the observed reading
times, in both the peak and the tails. This agreement persists even if
we stratify by variant.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{names }\OtherTok{\textless{}{-}} \FunctionTok{sapply}\NormalTok{(}\FunctionTok{which}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{subj\_rel }\SpecialCharTok{==} \DecValTok{1}\NormalTok{),}
                \ControlFlowTok{function}\NormalTok{(n)}
                \FunctionTok{paste0}\NormalTok{(}\StringTok{\textquotesingle{}log\_reading\_time\_pred[\textquotesingle{}}\NormalTok{, n, }\StringTok{\textquotesingle{}]\textquotesingle{}}\NormalTok{))}
\NormalTok{filtered\_samples }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{filter\_expectands}\NormalTok{(samples\_dat2, names)}

\NormalTok{obs\_reading\_time }\OtherTok{\textless{}{-}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{reading\_time[data}\SpecialCharTok{$}\NormalTok{subj\_rel }\SpecialCharTok{==} \DecValTok{1}\NormalTok{]}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_hist\_quantiles}\NormalTok{(filtered\_samples,}
                         \StringTok{\textquotesingle{}log\_reading\_time\_pred\textquotesingle{}}\NormalTok{,}
                         \DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.1}\NormalTok{,}
                         \AttributeTok{baseline\_values=}\FunctionTok{log}\NormalTok{(obs\_reading\_time),}
                         \AttributeTok{xlab=}\StringTok{\textquotesingle{}Log Reading Time / 1 ms\textquotesingle{}}\NormalTok{,}
                         \AttributeTok{main=}\StringTok{\textquotesingle{}Subject Relative\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 43 predictive values (0.0%) fell above the binning.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{names }\OtherTok{\textless{}{-}} \FunctionTok{sapply}\NormalTok{(}\FunctionTok{which}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{subj\_rel }\SpecialCharTok{==} \DecValTok{0}\NormalTok{),}
                \ControlFlowTok{function}\NormalTok{(n)}
                \FunctionTok{paste0}\NormalTok{(}\StringTok{\textquotesingle{}log\_reading\_time\_pred[\textquotesingle{}}\NormalTok{, n, }\StringTok{\textquotesingle{}]\textquotesingle{}}\NormalTok{))}
\NormalTok{filtered\_samples }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{filter\_expectands}\NormalTok{(samples\_dat2, names)}

\NormalTok{obs\_reading\_time }\OtherTok{\textless{}{-}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{reading\_time[data}\SpecialCharTok{$}\NormalTok{subj\_rel }\SpecialCharTok{==} \DecValTok{0}\NormalTok{]}


\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_hist\_quantiles}\NormalTok{(filtered\_samples,}
                         \StringTok{\textquotesingle{}log\_reading\_time\_pred\textquotesingle{}}\NormalTok{,}
                         \DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.1}\NormalTok{,}
                         \AttributeTok{baseline\_values=}\FunctionTok{log}\NormalTok{(obs\_reading\_time),}
                         \AttributeTok{xlab=}\StringTok{\textquotesingle{}Log Reading Time / 1 ms\textquotesingle{}}\NormalTok{,}
                         \AttributeTok{main=}\StringTok{\textquotesingle{}Object Relative\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 30 predictive values (0.0%) fell above the binning.
\end{verbatim}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-46-1.pdf}

When we stratify by item and subject the observed behavior becomes a bit
spikier, perhaps at artifact of discretization. Overall, however, the
posterior predictive behavior is pretty consistent with the observed
behavior.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in}\NormalTok{ display\_items) \{}
\NormalTok{  names }\OtherTok{\textless{}{-}} \FunctionTok{sapply}\NormalTok{(}\FunctionTok{which}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{item }\SpecialCharTok{==}\NormalTok{ i),}
                  \ControlFlowTok{function}\NormalTok{(n)}
                  \FunctionTok{paste0}\NormalTok{(}\StringTok{\textquotesingle{}log\_reading\_time\_pred[\textquotesingle{}}\NormalTok{, n, }\StringTok{\textquotesingle{}]\textquotesingle{}}\NormalTok{))}
\NormalTok{  filtered\_samples }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{filter\_expectands}\NormalTok{(samples\_dat2, names)}

\NormalTok{  obs\_reading\_time }\OtherTok{\textless{}{-}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{reading\_time[data}\SpecialCharTok{$}\NormalTok{item }\SpecialCharTok{==}\NormalTok{ i]}

\NormalTok{  util}\SpecialCharTok{$}\FunctionTok{plot\_hist\_quantiles}\NormalTok{(filtered\_samples,}
                           \StringTok{\textquotesingle{}log\_reading\_time\_pred\textquotesingle{}}\NormalTok{,}
                           \DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.25}\NormalTok{,}
                           \AttributeTok{baseline\_values=}\FunctionTok{log}\NormalTok{(obs\_reading\_time),}
                           \AttributeTok{xlab=}\StringTok{\textquotesingle{}Log Reading Time / 1 ms\textquotesingle{}}\NormalTok{,}
                           \AttributeTok{main=}\FunctionTok{paste}\NormalTok{(}\StringTok{\textquotesingle{}Item\textquotesingle{}}\NormalTok{, i))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 5 predictive values (0.0%) fell above the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 4 predictive values (0.0%) fell above the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 5 predictive values (0.0%) fell above the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 6 predictive values (0.0%) fell above the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 5 predictive values (0.0%) fell above the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 4 predictive values (0.0%) fell above the binning.

Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 4 predictive values (0.0%) fell above the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 3 predictive values (0.0%) fell above the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 2 predictive values (0.0%) fell above the binning.
\end{verbatim}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-47-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\ControlFlowTok{for}\NormalTok{ (s }\ControlFlowTok{in}\NormalTok{ display\_subjects) \{}
  \ControlFlowTok{if}\NormalTok{ (s }\SpecialCharTok{==} \DecValTok{13}\NormalTok{) \{}
    \FunctionTok{plot.new}\NormalTok{()}
    \ControlFlowTok{next}
\NormalTok{  \}}

\NormalTok{  names }\OtherTok{\textless{}{-}} \FunctionTok{sapply}\NormalTok{(}\FunctionTok{which}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{subject }\SpecialCharTok{==}\NormalTok{ s),}
                  \ControlFlowTok{function}\NormalTok{(n)}
                  \FunctionTok{paste0}\NormalTok{(}\StringTok{\textquotesingle{}log\_reading\_time\_pred[\textquotesingle{}}\NormalTok{, n, }\StringTok{\textquotesingle{}]\textquotesingle{}}\NormalTok{))}
\NormalTok{  filtered\_samples }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{filter\_expectands}\NormalTok{(samples\_dat2, names)}

\NormalTok{  obs\_reading\_time }\OtherTok{\textless{}{-}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{reading\_time[data}\SpecialCharTok{$}\NormalTok{subject }\SpecialCharTok{==}\NormalTok{ s]}

\NormalTok{  util}\SpecialCharTok{$}\FunctionTok{plot\_hist\_quantiles}\NormalTok{(filtered\_samples,}
                          \StringTok{\textquotesingle{}log\_reading\_time\_pred\textquotesingle{}}\NormalTok{,}
                           \DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.25}\NormalTok{,}
                           \AttributeTok{baseline\_values=}\FunctionTok{log}\NormalTok{(obs\_reading\_time),}
                           \AttributeTok{xlab=}\StringTok{\textquotesingle{}Log Reading Time / 1 ms\textquotesingle{}}\NormalTok{,}
                           \AttributeTok{main=}\FunctionTok{paste}\NormalTok{(}\StringTok{\textquotesingle{}Subject\textquotesingle{}}\NormalTok{, s))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 1 predictive value (0.0%) fell above the binning.

Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 1 predictive value (0.0%) fell above the binning.

Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 1 predictive value (0.0%) fell above the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 3 predictive values (0.0%) fell above the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 5 predictive values (0.0%) fell above the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 2 predictive values (0.0%) fell above the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 1 predictive value (0.0%) fell above the binning.
\end{verbatim}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-48-1.pdf}

\subsubsection{Posterior Inferences}\label{posterior-inferences}

After all of this work, we finally have a model with no apparent
retrodictive tension to suggest model inadequacy. This doesn't imply
that our model is ``true'' in any real sense. Indeed if we collected
more data then we would likely be able to resolve even more detailed
behaviors that could invalidate this relatively simple model. Our
posterior inferences, however, will accurately describe the latent data
generating process as well as we can resolve it at the moment.

What do those posterior inferences have to say? Firstly, we can see that
the reading times after a failed initial hypothesis are clearly larger
than those after a successful initial hypothesis.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{), }\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_expectand\_pushforward}\NormalTok{(samples\_dat2[[}\StringTok{"nu"}\NormalTok{]], }\DecValTok{25}\NormalTok{,}
                                \AttributeTok{display\_name=}\StringTok{"nu"}\NormalTok{)}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_expectand\_pushforward}\NormalTok{(samples\_dat2[[}\StringTok{"omega"}\NormalTok{]],}
                                \DecValTok{75}\NormalTok{, }\AttributeTok{flim=}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.1}\NormalTok{ ,}\FloatTok{1.1}\NormalTok{),}
                                \AttributeTok{display\_name=}\StringTok{"omega"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-49-1.pdf}

Moreover, the initial failure component model is more diffuse than the
initial success component model.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{), }\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_expectand\_pushforward}\NormalTok{(samples\_dat2[[}\StringTok{"phi1"}\NormalTok{]],}
                                \DecValTok{100}\NormalTok{, }\AttributeTok{flim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.7}\NormalTok{),}
                                \AttributeTok{display\_name=}\StringTok{"phi1"}\NormalTok{)}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_expectand\_pushforward}\NormalTok{(samples\_dat2[[}\StringTok{"phi2"}\NormalTok{]],}
                                \DecValTok{50}\NormalTok{, }\AttributeTok{flim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.7}\NormalTok{),}
                                \AttributeTok{display\_name=}\StringTok{"phi2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-50-1.pdf}

Sentences with an object-extracted relative clause favor the faster
reading times of the initial success component model more than the
sentences with a subject-extracted relative clause. This suggests that
subject-extracted relative clauses take more attempts to correctly
parse. At least, that is, for Chinese.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_expectand\_pushforward}\NormalTok{(samples\_dat2[[}\StringTok{\textquotesingle{}lambda\_SR\textquotesingle{}}\NormalTok{]], }\DecValTok{25}\NormalTok{,}
                                \AttributeTok{flim=}\FunctionTok{c}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.4}\NormalTok{),}
                                \AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{25}\NormalTok{),}
                                \AttributeTok{display\_name=}\StringTok{"Initial Success Probability"}\NormalTok{,}
                                \AttributeTok{col=}\NormalTok{util}\SpecialCharTok{$}\NormalTok{c\_mid)}
\FunctionTok{text}\NormalTok{(}\FloatTok{0.35}\NormalTok{, }\DecValTok{15}\NormalTok{, }\StringTok{\textquotesingle{}Object}\SpecialCharTok{\textbackslash{}n}\StringTok{Relative\textquotesingle{}}\NormalTok{, }\AttributeTok{col=}\NormalTok{util}\SpecialCharTok{$}\NormalTok{c\_mid)}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_expectand\_pushforward}\NormalTok{(samples\_dat2[[}\StringTok{\textquotesingle{}lambda\_OR\textquotesingle{}}\NormalTok{]], }\DecValTok{25}\NormalTok{,}
                                \AttributeTok{flim=}\FunctionTok{c}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.4}\NormalTok{),}
                                \AttributeTok{col=}\NormalTok{util}\SpecialCharTok{$}\NormalTok{c\_dark,}
                                \AttributeTok{border=}\StringTok{"\#BBBBBB88"}\NormalTok{,}
                                \AttributeTok{add=}\ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{text}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\DecValTok{12}\NormalTok{, }\StringTok{\textquotesingle{}Subject}\SpecialCharTok{\textbackslash{}n}\StringTok{Relative\textquotesingle{}}\NormalTok{, }\AttributeTok{col=}\NormalTok{util}\SpecialCharTok{$}\NormalTok{c\_dark)}
\end{Highlighting}
\end{Shaded}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-51-1.pdf}

We can even quantify the preference for the first component model by
computing the posterior probability that \[
\lambda_{\text{SR}} < \lambda_{\text{OR}}.
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{var\_repl }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\StringTok{\textquotesingle{}l1\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}lambda\_SR\textquotesingle{}}\NormalTok{,}
                 \StringTok{\textquotesingle{}l2\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}lambda\_OR\textquotesingle{}}\NormalTok{)}

\NormalTok{p\_est }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{implicit\_subset\_prob}\NormalTok{(samples\_dat2,}
                                   \ControlFlowTok{function}\NormalTok{(l1, l2) l1 }\SpecialCharTok{\textless{}}\NormalTok{ l2,}
\NormalTok{                                   var\_repl)}

\NormalTok{format\_string }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\StringTok{"Posterior probability that lambda\_SR "}\NormalTok{,}
                        \StringTok{"\textless{} lambda\_OR = \%.3f +/{-} \%.3f."}\NormalTok{)}
\FunctionTok{cat}\NormalTok{(}\FunctionTok{sprintf}\NormalTok{(format\_string, p\_est[}\DecValTok{1}\NormalTok{], }\DecValTok{2} \SpecialCharTok{*}\NormalTok{ p\_est[}\DecValTok{2}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Posterior probability that lambda_SR < lambda_OR = 0.741 +/- 0.014.
\end{verbatim}

Finally, we can examine the inferred behavior of the individual items
and subjects. Let's start with the relative complexity of the items.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{names }\OtherTok{\textless{}{-}} \FunctionTok{sapply}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{data}\SpecialCharTok{$}\NormalTok{N\_items,}
                \ControlFlowTok{function}\NormalTok{(i) }\FunctionTok{paste0}\NormalTok{(}\StringTok{\textquotesingle{}delta[\textquotesingle{}}\NormalTok{, i, }\StringTok{\textquotesingle{}]\textquotesingle{}}\NormalTok{))}
\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_disc\_pushforward\_quantiles}\NormalTok{(samples\_dat2, names,}
                                     \AttributeTok{xlab=}\StringTok{"Item"}\NormalTok{,}
                                     \AttributeTok{xticklabs=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{data}\SpecialCharTok{$}\NormalTok{N\_items),}
                                     \AttributeTok{ylab=}\StringTok{"Relative Complexity"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-53-1.pdf}

Because Item 12 did not appear in any observations its inferences are
determined entirely by the prior model, resulting in much stronger
posterior uncertainties. Otherwise Item 8 appears to be the most complex
question with Item 7 the simplest.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_expectand\_pushforward}\NormalTok{(samples\_dat2[[}\StringTok{\textquotesingle{}delta[7]\textquotesingle{}}\NormalTok{]],}
                                \DecValTok{50}\NormalTok{, }\AttributeTok{flim=}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.25}\NormalTok{, }\FloatTok{0.35}\NormalTok{),}
                                \AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{15}\NormalTok{),}
                                \AttributeTok{display\_name=}\StringTok{"Relative Complexity"}\NormalTok{,}
                                \AttributeTok{col=}\NormalTok{util}\SpecialCharTok{$}\NormalTok{c\_mid)}
\FunctionTok{text}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.15}\NormalTok{, }\DecValTok{10}\NormalTok{, }\StringTok{\textquotesingle{}Item 7\textquotesingle{}}\NormalTok{, }\AttributeTok{col=}\NormalTok{util}\SpecialCharTok{$}\NormalTok{c\_mid)}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_expectand\_pushforward}\NormalTok{(samples\_dat2[[}\StringTok{\textquotesingle{}delta[8]\textquotesingle{}}\NormalTok{]],}
                                \DecValTok{50}\NormalTok{, }\AttributeTok{flim=}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.25}\NormalTok{, }\FloatTok{0.35}\NormalTok{),}
                                \AttributeTok{col=}\NormalTok{util}\SpecialCharTok{$}\NormalTok{c\_dark,}
                                \AttributeTok{border=}\StringTok{"\#BBBBBB88"}\NormalTok{,}
                                \AttributeTok{add=}\ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{text}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\DecValTok{10}\NormalTok{, }\StringTok{\textquotesingle{}Item 8\textquotesingle{}}\NormalTok{, }\AttributeTok{col=}\NormalTok{util}\SpecialCharTok{$}\NormalTok{c\_dark)}
\end{Highlighting}
\end{Shaded}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-54-1.pdf}

What about the relative subject skills?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{names }\OtherTok{\textless{}{-}} \FunctionTok{sapply}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{data}\SpecialCharTok{$}\NormalTok{N\_subjects,}
                \ControlFlowTok{function}\NormalTok{(s) }\FunctionTok{paste0}\NormalTok{(}\StringTok{\textquotesingle{}zeta[\textquotesingle{}}\NormalTok{, s, }\StringTok{\textquotesingle{}]\textquotesingle{}}\NormalTok{))}
\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_disc\_pushforward\_quantiles}\NormalTok{(samples\_dat2, names,}
                                     \AttributeTok{xlab=}\StringTok{"Subject"}\NormalTok{,}
                                     \AttributeTok{ylab=}\StringTok{"Relative Skill"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-55-1.pdf}

The lack of any observations involving Subjects 10, 12, and 25 once
again manifests in larger posterior uncertainties. For those subjects
whose reading times were recorded, Subject 19 seems to be about \[
\exp(1.25) \approx 3.5
\] times better at parsing the target texts than Subject 12.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_expectand\_pushforward}\NormalTok{(samples\_dat2[[}\StringTok{\textquotesingle{}zeta[12]\textquotesingle{}}\NormalTok{]],}
                                \DecValTok{100}\NormalTok{, }\AttributeTok{flim=}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{),}
                                \AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{),}
                                \AttributeTok{display\_name=}\StringTok{"Relative Skill"}\NormalTok{,}
                                \AttributeTok{col=}\NormalTok{util}\SpecialCharTok{$}\NormalTok{c\_mid)}
\FunctionTok{text}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.4}\NormalTok{, }\DecValTok{6}\NormalTok{, }\StringTok{\textquotesingle{}Subject 12\textquotesingle{}}\NormalTok{, }\AttributeTok{col=}\NormalTok{util}\SpecialCharTok{$}\NormalTok{c\_mid)}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_expectand\_pushforward}\NormalTok{(samples\_dat2[[}\StringTok{\textquotesingle{}zeta[19]\textquotesingle{}}\NormalTok{]],}
                                \DecValTok{100}\NormalTok{, }\AttributeTok{flim=}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{),}
                                \AttributeTok{col=}\NormalTok{util}\SpecialCharTok{$}\NormalTok{c\_dark,}
                                \AttributeTok{border=}\StringTok{"\#BBBBBB88"}\NormalTok{,}
                                \AttributeTok{add=}\ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{text}\NormalTok{(}\FloatTok{0.75}\NormalTok{, }\DecValTok{6}\NormalTok{, }\StringTok{\textquotesingle{}Subject 19\textquotesingle{}}\NormalTok{, }\AttributeTok{col=}\NormalTok{util}\SpecialCharTok{$}\NormalTok{c\_dark)}
\end{Highlighting}
\end{Shaded}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-56-1.pdf}

\subsection{Joint Dependency Locality and Direct-Access
Model}\label{joint-dependency-locality-and-direct-access-model}

We can from their respective posterior retrodictive behaviors that the
dependency locality model is less consistent with the observed reading
times than the direct-access model. That said, this comparison is a
mostly qualitative one. Many are satisfied by only more quantitative
comparisons.

There is no end of \textbf{predictive performance scores} that claim to
be able to quantify how much better of a fit one model is than other.
Unfortunately the theoretical construction and practical implementation
of these scores inherently results in fragile, if not entirely
untrustworthy, results. At least, that is, in the opinion of this
author. For a more detailed discussion of posterior predictive
performance scores and their limitations see Sections 1.4.1 and 1.4.2 of
my
\href{https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html\#14_Model_Adequacy}{workflow
chapter}, as well as the references therein.

Quantitative comparisons, however, become ordinary posterior inferences
if we can integrate both of these models into a single joint model. As
we saw in \href{@sec:joint}{Section 1.2.4}, this integration is
straightforward for the dependency locality and direct-access models.

\subsubsection{Observational Model}\label{observational-model-2}

To combine these two models we start with the direct-access mixture
model but then allow for clausal shifts in the location variables, \[
  \lambda_{\text{SR}} \,
  p_{1}( t_{ij} \mid \mu_{1, ij}^{\text{SR}}, \phi_{1})
+ (1 - \lambda_{\text{SR}}) \,
  p_{2}( t_{ij} \mid \mu_{2, ij}^{\text{SR}}, \phi_{2})
\] and \[
  \lambda_{\text{OR}}  \,
  p_{1}( t_{ij} \mid \mu_{1, ij}^{\text{OR}}, \phi_{1})
+ (1 - \lambda_{\text{OR}}) \,
  p_{2}( t_{ij} \mid \mu_{2, ij}^{\text{OR}}, \phi_{2}).
\] with \begin{align*}
\mu_{1, ij}^{\text{SR}}
&=
\exp \left(  \eta + \gamma^{\text{SR}}
           + \alpha_{i} - \beta_{j} \right)
\\
\mu_{2, ij}^{\text{SR}}
&=
\exp \left(  \eta + \gamma^{\text{SR}} + \omega
           + \alpha_{i} - \beta_{j} \right).
\end{align*} and \begin{align*}
\mu_{1, ij}^{\text{OR}}
&=
\exp \left(  \eta + \gamma^{\text{OR}}
           + \alpha_{i} - \beta_{j} \right)
\\
\mu_{2, ij}^{\text{OR}}
&=
\exp \left(  \eta + \gamma^{\text{OR}} + \omega
           + \alpha_{i} - \beta_{j} \right).
\end{align*}

If \[
\lambda_{\text{SR}} = \lambda_{\text{OR}} = 1
\] then the joint model reduces to the dependency locality model.
Similarly if \[
\gamma^{\text{SR}} = \gamma^{\text{OR}} = \gamma
\] then the joint model effectively reduces to the direct-access model
provided we absorb \(\gamma\) into \(\eta\).

Based on our previous results we'll jump past log normal component
models to inverse gamma component models.

\subsubsection{Anchoring}\label{anchoring-1}

At this point anchoring is becoming more routine. We start by selecting
a distinguished anchor item \(i'\) to define relative complexities, \[
\delta_{i} = \alpha_{i} - \alpha_{i'},
\] and then choosing a distinguished anchor subject \(j'\) to define
relative skills, \[
\zeta_{j} = \beta_{j} - \beta_{j'}.
\]

Because there are four different location variables in the joint model,
however, our next step will have to be a bit more careful. We start by
writing \begin{align*}
\mu_{1, ij}^{\text{SR}}
&=
\exp \left(   \eta + \gamma^{\text{SR}}
            + \alpha_{i} - \beta_{j} \right)
\\
&=
\exp \left(   \eta + \gamma^{\text{SR}}
            + \alpha_{i'} - \beta_{j'}
            + \delta_{i} - \zeta_{j} \right)
\\
&\equiv
\exp \left( \tau + \delta_{i} - \zeta_{j} \right)
\end{align*} and \begin{align*}
\mu_{2, ij}^{\text{SR}}
&=
\exp \left(   \eta + \gamma^{\text{SR}} + \omega
            + \alpha_{i} - \beta_{j} \right)
\\
&=
\exp \left(   \eta + \gamma^{\text{SR}}
            + \alpha_{i'} - \beta_{j'} + \omega
            + \delta_{i} - \zeta_{j} \right)
\\
&=
\exp \left(  \tau + \omega
           + \delta_{i} - \zeta_{j} \right).
\end{align*}

Then in contrast we can write \begin{align*}
\mu_{1, ij}^{\text{OR}}
&=
\exp \left(   \eta + \gamma^{\text{OR}}
            + \alpha_{i} - \beta_{j} \right)
\\
&=
\exp \left(   \eta + \gamma^{\text{SR}}
            + \alpha_{i'} - \beta_{j'}
            + \gamma^{\text{OR}} - \gamma^{\text{SR}}
            + \delta_{i} - \zeta_{j} \right)
\\
&\equiv
\exp \left( \tau + \chi + \delta_{i} - \zeta_{j} \right)
\end{align*} and \begin{align*}
\mu_{2, ij}^{\text{OR}}
&=
\exp \left(   \eta + \gamma^{\text{OR}} + \omega
            + \alpha_{i} - \beta_{j} \right)
\\
&=
\exp \left(   \eta + \gamma^{\text{SR}}
            + \alpha_{i'} - \beta_{j'}
            + \gamma^{\text{OR}} - \gamma^{\text{SR}} + \omega
            + \delta_{i} - \zeta_{j} \right)
\\
&=
\exp \left(  \tau + \chi + \omega
           + \delta_{i} - \zeta_{j} \right),
\end{align*}

The new baseline \[
\tau = \eta + \gamma^{\text{SR}} + \alpha_{i'} - \beta_{j'}.
\] quantifies the nominal behavior of a successful initial hypothesis
parsing subject-extracted relative clauses. The deviations \[
\chi = \gamma^{\text{OR}} - \gamma^{\text{SR}}
\] and \(\omega\) are the same parameters that appear in the dependency
locality and direct-access models, respectively.

\subsubsection{Prior Model}\label{prior-model-3}

Except for \(\tau\), all of the parameters in the joint model directly
correspond to parameters in either the dependency locality model or the
direct-access model. Consequently we can use the same component prior
models that we had used previously when building a prior for the joint
model.

Because we used the same component prior models for the baselines
\(\kappa\) and \(\nu\) in those two models, it's also reasonable to
continue that choice for \(\tau\). We're effectively assuming that the
slight difference in these quantities is negligible compared to the
uncertainties in our domain expertise elicitation. Again, more precise
domain expertise is always welcome.

To be diligent we should check the prior predictive behavior of this new
joint model. Fortunately, we don't see any new awkward behaviors that
clash with our domain expertise.

\begin{codelisting}

\caption{\texttt{joint\textbackslash\_prior.stan}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{functions}\NormalTok{ \{}
  \CommentTok{// Mean{-}dispersion parameterization of inverse gamma family}
  \DataTypeTok{real}\NormalTok{ inv\_gamma\_md\_lpdf(}\DataTypeTok{real}\NormalTok{ x, }\DataTypeTok{real}\NormalTok{ log\_mu, }\DataTypeTok{real}\NormalTok{ psi) \{}
    \ControlFlowTok{return}\NormalTok{ inv\_gamma\_lpdf(x | inv(psi) + }\DecValTok{2}\NormalTok{,}
\NormalTok{                              exp(log\_mu) * (inv(psi) + }\DecValTok{1}\NormalTok{));}
\NormalTok{  \}}

  \DataTypeTok{real}\NormalTok{ inv\_gamma\_md\_rng(}\DataTypeTok{real}\NormalTok{ log\_mu, }\DataTypeTok{real}\NormalTok{ psi) \{}
    \ControlFlowTok{return}\NormalTok{ inv\_gamma\_rng(inv(psi) + }\DecValTok{2}\NormalTok{,}
\NormalTok{                         exp(log\_mu) * (inv(psi) + }\DecValTok{1}\NormalTok{));}
\NormalTok{  \}}
\NormalTok{\}}

\KeywordTok{data}\NormalTok{ \{}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N; }\CommentTok{// Number of observations}

  \CommentTok{// Item configuration}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N\_items;}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{, }\KeywordTok{upper}\NormalTok{=N\_items\textgreater{} item;}

  \CommentTok{// Subject configuration}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N\_subjects;}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{, }\KeywordTok{upper}\NormalTok{=N\_subjects\textgreater{} subject;}

  \CommentTok{// Item variant}
  \CommentTok{//   Object Relative:  subj\_rel = 0}
  \CommentTok{//   Subject Relative: subj\_rel = 1}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} subj\_rel;}

  \DataTypeTok{vector}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{}[N] reading\_time; }\CommentTok{// Reading times (ms)}
\NormalTok{\}}

\KeywordTok{generated quantities}\NormalTok{ \{}
  \CommentTok{// Log reading time baseline}
  \DataTypeTok{real}\NormalTok{ tau = normal\_rng(}\FloatTok{5.76}\NormalTok{, }\FloatTok{0.50}\NormalTok{);}

  \CommentTok{// Relative item difficulties}
  \DataTypeTok{vector}\NormalTok{[N\_items {-} }\DecValTok{1}\NormalTok{] delta\_free}
\NormalTok{    = to\_vector(normal\_rng(zeros\_vector(N\_items {-} }\DecValTok{1}\NormalTok{), }\FloatTok{0.99}\NormalTok{));}

  \CommentTok{// Relative subject skills}
  \DataTypeTok{vector}\NormalTok{[N\_subjects {-} }\DecValTok{1}\NormalTok{] zeta\_free}
\NormalTok{    = to\_vector(normal\_rng(zeros\_vector(N\_subjects {-} }\DecValTok{1}\NormalTok{), }\FloatTok{0.99}\NormalTok{));}

  \CommentTok{// Initial failure difference}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} omega = abs(normal\_rng(}\DecValTok{0}\NormalTok{, }\FloatTok{0.90}\NormalTok{));}

  \CommentTok{// Subject Relative Difference}
  \DataTypeTok{real}\NormalTok{ chi = normal\_rng(}\DecValTok{0}\NormalTok{, }\FloatTok{0.99}\NormalTok{);}

  \CommentTok{// Measurement scales}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} phi1 = abs(normal\_rng(}\DecValTok{0}\NormalTok{, }\FloatTok{3.89}\NormalTok{));}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} phi2 = abs(normal\_rng(}\DecValTok{0}\NormalTok{, }\FloatTok{3.89}\NormalTok{));}

  \CommentTok{// Initial failure probabilities}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} lambda\_SR = beta\_rng(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{);}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} lambda\_OR = beta\_rng(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{);}

  \CommentTok{// Relative skills for all items and subjects}
  \DataTypeTok{vector}\NormalTok{[N\_items] delta}
\NormalTok{    = append\_row([}\DecValTok{0}\NormalTok{]\textquotesingle{}, delta\_free);}
  \DataTypeTok{vector}\NormalTok{[N\_subjects] zeta}
\NormalTok{    = append\_row([}\DecValTok{0}\NormalTok{]\textquotesingle{}, zeta\_free);}

  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{real}\NormalTok{ log\_reading\_time\_pred;}

  \ControlFlowTok{for}\NormalTok{ (n }\ControlFlowTok{in} \DecValTok{1}\NormalTok{:N) \{}
    \DataTypeTok{int}\NormalTok{ i = item[n];}
    \DataTypeTok{int}\NormalTok{ s = subject[n];}

    \ControlFlowTok{if}\NormalTok{ (subj\_rel[n] == }\DecValTok{1}\NormalTok{) \{}
      \DataTypeTok{real}\NormalTok{ log\_mu = tau + delta[i] {-} zeta[s];}

      \ControlFlowTok{if}\NormalTok{ (bernoulli\_rng(lambda\_SR)) \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(inv\_gamma\_md\_rng(log\_mu        , phi1));}
\NormalTok{      \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(inv\_gamma\_md\_rng(log\_mu + omega, phi2));}
\NormalTok{      \}}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
      \DataTypeTok{real}\NormalTok{ log\_mu = tau + delta[i] {-} zeta[s] + chi;}

      \ControlFlowTok{if}\NormalTok{ (bernoulli\_rng(lambda\_OR)) \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(inv\_gamma\_md\_rng(log\_mu        , phi1));}
\NormalTok{      \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(inv\_gamma\_md\_rng(log\_mu + omega, phi2));}
\NormalTok{      \}}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{stan}\NormalTok{(}\AttributeTok{file=}\StringTok{"stan\_programs/joint\_prior.stan"}\NormalTok{,}
            \AttributeTok{data=}\NormalTok{data, }\AttributeTok{seed=}\DecValTok{8438338}\NormalTok{, }\AttributeTok{algorithm=}\StringTok{"Fixed\_param"}\NormalTok{,}
            \AttributeTok{warmup=}\DecValTok{0}\NormalTok{, }\AttributeTok{iter=}\DecValTok{1024}\NormalTok{, }\AttributeTok{refresh=}\DecValTok{0}\NormalTok{)}

\NormalTok{samples }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{extract\_expectand\_vals}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_hist\_quantiles}\NormalTok{(samples, }\StringTok{\textquotesingle{}log\_reading\_time\_pred\textquotesingle{}}\NormalTok{,}
                         \DecValTok{0}\NormalTok{, }\DecValTok{20}\NormalTok{, }\FloatTok{0.25}\NormalTok{,}
                         \AttributeTok{xlab=}\StringTok{\textquotesingle{}Log Reading Time / 1 ms\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 8270 predictive values (0.1%) fell below the binning.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v=}\FunctionTok{log}\NormalTok{(}\FloatTok{1e1}\NormalTok{), }\AttributeTok{lwd=}\DecValTok{2}\NormalTok{, }\AttributeTok{lty=}\DecValTok{3}\NormalTok{, }\AttributeTok{col=}\StringTok{"\#DDDDDD"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v=}\FunctionTok{log}\NormalTok{(}\FloatTok{1e5}\NormalTok{), }\AttributeTok{lwd=}\DecValTok{2}\NormalTok{, }\AttributeTok{lty=}\DecValTok{3}\NormalTok{, }\AttributeTok{col=}\StringTok{"\#DDDDDD"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-58-1.pdf}

\subsubsection{Posterior
Quantification}\label{posterior-quantification-2}

Now we hope Hamiltonian Monte Carlo is up to the task of exploring the
posterior distribution derived from this joint model.

\begin{codelisting}

\caption{\texttt{joint.stan}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{functions}\NormalTok{ \{}
  \CommentTok{// Mean{-}dispersion parameterization of inverse gamma family}
  \DataTypeTok{real}\NormalTok{ inv\_gamma\_md\_lpdf(}\DataTypeTok{real}\NormalTok{ x, }\DataTypeTok{real}\NormalTok{ log\_mu, }\DataTypeTok{real}\NormalTok{ psi) \{}
    \ControlFlowTok{return}\NormalTok{ inv\_gamma\_lpdf(x | inv(psi) + }\DecValTok{2}\NormalTok{,}
\NormalTok{                              exp(log\_mu) * (inv(psi) + }\DecValTok{1}\NormalTok{));}
\NormalTok{  \}}

  \DataTypeTok{real}\NormalTok{ inv\_gamma\_md\_rng(}\DataTypeTok{real}\NormalTok{ log\_mu, }\DataTypeTok{real}\NormalTok{ psi) \{}
    \ControlFlowTok{return}\NormalTok{ inv\_gamma\_rng(inv(psi) + }\DecValTok{2}\NormalTok{,}
\NormalTok{                         exp(log\_mu) * (inv(psi) + }\DecValTok{1}\NormalTok{));}
\NormalTok{  \}}
\NormalTok{\}}

\KeywordTok{data}\NormalTok{ \{}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N; }\CommentTok{// Number of observations}

  \CommentTok{// Item configuration}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N\_items;}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{, }\KeywordTok{upper}\NormalTok{=N\_items\textgreater{} item;}

  \CommentTok{// Subject configuration}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N\_subjects;}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{, }\KeywordTok{upper}\NormalTok{=N\_subjects\textgreater{} subject;}

  \CommentTok{// Item variant}
  \CommentTok{//   Object Relative:  subj\_rel = 0}
  \CommentTok{//   Subject Relative: subj\_rel = 1}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} subj\_rel;}

  \DataTypeTok{vector}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{}[N] reading\_time; }\CommentTok{// Reading times (ms)}
\NormalTok{\}}

\KeywordTok{parameters}\NormalTok{ \{}
  \CommentTok{// Log reading time baseline for successful retrieval}
  \DataTypeTok{real}\NormalTok{ tau;}

  \CommentTok{// Relative item difficulties}
  \DataTypeTok{vector}\NormalTok{[N\_items {-} }\DecValTok{1}\NormalTok{] delta\_free;}

  \CommentTok{// Relative subject skills}
  \DataTypeTok{vector}\NormalTok{[N\_subjects {-} }\DecValTok{1}\NormalTok{] zeta\_free;}

  \CommentTok{// Log offset for initial failure before successful retrieval}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} omega;}

  \CommentTok{// Subject Relative Difference}
  \DataTypeTok{real}\NormalTok{ chi;}

  \CommentTok{// Measurement scales}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} phi1;}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} phi2;}

  \CommentTok{// Mixture probabilities}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} lambda\_SR;}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} lambda\_OR;}
\NormalTok{\}}

\KeywordTok{transformed parameters}\NormalTok{ \{}
  \CommentTok{// Relative skills for all items and subjects}
  \DataTypeTok{vector}\NormalTok{[N\_items] delta}
\NormalTok{    = append\_row([}\DecValTok{0}\NormalTok{]\textquotesingle{}, delta\_free);}
  \DataTypeTok{vector}\NormalTok{[N\_subjects] zeta}
\NormalTok{    = append\_row([}\DecValTok{0}\NormalTok{]\textquotesingle{}, zeta\_free);}
\NormalTok{\}}

\KeywordTok{model}\NormalTok{ \{}
  \CommentTok{// Prior model}

\CommentTok{// 100 \textless{}\textasciitilde{} exp(tau) \textless{}\textasciitilde{} 1000}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(tau | }\FloatTok{5.76}\NormalTok{, }\FloatTok{0.50}\NormalTok{);}

  \CommentTok{// 0.1 \textless{}\textasciitilde{} exp(delta) \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(delta\_free | }\DecValTok{0}\NormalTok{, }\FloatTok{0.99}\NormalTok{);}

  \CommentTok{// 0.1 \textless{}\textasciitilde{} exp(zeta) \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(zeta\_free| }\DecValTok{0}\NormalTok{, }\FloatTok{0.99}\NormalTok{);}

  \CommentTok{// 1 \textless{}\textasciitilde{} exp(omega) \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(omega | }\DecValTok{0}\NormalTok{, }\FloatTok{0.90}\NormalTok{);}

  \CommentTok{// 0.1 \textless{}\textasciitilde{} exp(chi) \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(chi | }\DecValTok{0}\NormalTok{, }\FloatTok{0.99}\NormalTok{);}

  \CommentTok{// 0 \textless{}\textasciitilde{} phi \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(phi1 | }\DecValTok{0}\NormalTok{, }\FloatTok{3.89}\NormalTok{);}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(phi2 | }\DecValTok{0}\NormalTok{, }\FloatTok{3.89}\NormalTok{);}

  \CommentTok{// Uniform prior density functions}
  \KeywordTok{target +=}\NormalTok{ beta\_lpdf(lambda\_SR | }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{);}
  \KeywordTok{target +=}\NormalTok{ beta\_lpdf(lambda\_OR | }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{);}

  \CommentTok{// Observational model}
  \ControlFlowTok{for}\NormalTok{ (n }\ControlFlowTok{in} \DecValTok{1}\NormalTok{:N) \{}
    \DataTypeTok{int}\NormalTok{ i = item[n];}
    \DataTypeTok{int}\NormalTok{ s = subject[n];}

    \ControlFlowTok{if}\NormalTok{ (subj\_rel[n] == }\DecValTok{1}\NormalTok{) \{}
      \DataTypeTok{real}\NormalTok{ log\_mu = tau + delta[i] {-} zeta[s];}

      \DataTypeTok{real}\NormalTok{ lpd1}
\NormalTok{        =   log(lambda\_SR)}
\NormalTok{          + inv\_gamma\_md\_lpdf(reading\_time[n] | log\_mu        , phi1);}
      \DataTypeTok{real}\NormalTok{ lpd2}
\NormalTok{        =   log(}\DecValTok{1}\NormalTok{ {-} lambda\_SR)}
\NormalTok{          + inv\_gamma\_md\_lpdf(reading\_time[n] | log\_mu + omega, phi2);}

      \KeywordTok{target +=}\NormalTok{ log\_sum\_exp(lpd1, lpd2);}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
      \DataTypeTok{real}\NormalTok{ log\_mu = tau + delta[i] {-} zeta[s] + chi;}

      \DataTypeTok{real}\NormalTok{ lpd1}
\NormalTok{        =   log(lambda\_OR)}
\NormalTok{          + inv\_gamma\_md\_lpdf(reading\_time[n] | log\_mu        , phi1);}
      \DataTypeTok{real}\NormalTok{ lpd2}
\NormalTok{        =   log(}\DecValTok{1}\NormalTok{ {-} lambda\_OR)}
\NormalTok{          + inv\_gamma\_md\_lpdf(reading\_time[n] | log\_mu + omega, phi2);}

      \KeywordTok{target +=}\NormalTok{ log\_sum\_exp(lpd1, lpd2);}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\}}

\KeywordTok{generated quantities}\NormalTok{ \{}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{real}\NormalTok{ log\_reading\_time\_pred;}

  \ControlFlowTok{for}\NormalTok{ (n }\ControlFlowTok{in} \DecValTok{1}\NormalTok{:N) \{}
    \DataTypeTok{int}\NormalTok{ i = item[n];}
    \DataTypeTok{int}\NormalTok{ s = subject[n];}

    \ControlFlowTok{if}\NormalTok{ (subj\_rel[n] == }\DecValTok{1}\NormalTok{) \{}
      \DataTypeTok{real}\NormalTok{ log\_mu = tau + delta[i] {-} zeta[s];}

      \ControlFlowTok{if}\NormalTok{ (bernoulli\_rng(lambda\_SR)) \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(inv\_gamma\_md\_rng(log\_mu        , phi1));}
\NormalTok{      \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(inv\_gamma\_md\_rng(log\_mu + omega, phi2));}
\NormalTok{      \}}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
      \DataTypeTok{real}\NormalTok{ log\_mu = tau + delta[i] {-} zeta[s] + chi;}

      \ControlFlowTok{if}\NormalTok{ (bernoulli\_rng(lambda\_OR)) \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(inv\_gamma\_md\_rng(log\_mu        , phi1));}
\NormalTok{      \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(inv\_gamma\_md\_rng(log\_mu + omega, phi2));}
\NormalTok{      \}}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{stan}\NormalTok{(}\AttributeTok{file=}\StringTok{\textquotesingle{}stan\_programs/joint.stan\textquotesingle{}}\NormalTok{,}
            \AttributeTok{data=}\NormalTok{data, }\AttributeTok{seed=}\DecValTok{8438338}\NormalTok{,}
            \AttributeTok{warmup=}\DecValTok{1000}\NormalTok{, }\AttributeTok{iter=}\DecValTok{2024}\NormalTok{, }\AttributeTok{refresh=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Well the diagnostics are almost completely quiet. This suggests that the
exploration of the Markov chains was exhaustive.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diagnostics }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{extract\_hmc\_diagnostics}\NormalTok{(fit)}
\NormalTok{util}\SpecialCharTok{$}\FunctionTok{check\_all\_hmc\_diagnostics}\NormalTok{(diagnostics)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  All Hamiltonian Monte Carlo diagnostics are consistent with reliable
Markov chain Monte Carlo.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{samples\_joint }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{extract\_expectand\_vals}\NormalTok{(fit)}
\NormalTok{base\_samples }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{filter\_expectands}\NormalTok{(samples\_joint,}
                                       \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}tau\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}delta\_free\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}zeta\_free\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}omega\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}chi\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}phi1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}phi2\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}lambda\_SR\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}lambda\_OR\textquotesingle{}}\NormalTok{),}
                                       \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{util}\SpecialCharTok{$}\FunctionTok{check\_all\_expectand\_diagnostics}\NormalTok{(base\_samples)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
tau:
  Chain 1: hat{ESS} (87.566) is smaller than desired (100).
  Chain 4: hat{ESS} (99.453) is smaller than desired (100).

zeta_free[2]:
  Chain 2: hat{ESS} (88.870) is smaller than desired (100).

zeta_free[8]:
  Chain 2: hat{ESS} (97.598) is smaller than desired (100).

zeta_free[10]:
  Chain 2: hat{ESS} (86.756) is smaller than desired (100).

zeta_free[14]:
  Chain 2: hat{ESS} (94.936) is smaller than desired (100).

zeta_free[18]:
  Chain 2: hat{ESS} (94.783) is smaller than desired (100).

zeta_free[32]:
  Chain 2: hat{ESS} (92.060) is smaller than desired (100).

zeta_free[33]:
  Chain 2: hat{ESS} (92.746) is smaller than desired (100).

zeta_free[34]:
  Chain 2: hat{ESS} (98.846) is smaller than desired (100).


Small empirical effective sample sizes result in imprecise Markov chain
Monte Carlo estimators.
\end{verbatim}

\subsubsection{Retrodictive Checks}\label{retrodictive-checks-1}

Because the joint model is a strict generalization of the direct-access
model, we should be able to achieve the same posterior retrodictive
performance. Indeed the retrodictive checks are similarly gorgeous for
all of the summary statistics that we have considered.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_hist\_quantiles}\NormalTok{(samples\_joint, }\StringTok{\textquotesingle{}log\_reading\_time\_pred\textquotesingle{}}\NormalTok{,}
                         \DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.1}\NormalTok{,}
                         \AttributeTok{baseline\_values=}\FunctionTok{log}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{reading\_time),}
                         \AttributeTok{xlab=}\StringTok{\textquotesingle{}Log Reading Time / 1 ms\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 73 predictive values (0.0%) fell above the binning.
\end{verbatim}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-61-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{names }\OtherTok{\textless{}{-}} \FunctionTok{sapply}\NormalTok{(}\FunctionTok{which}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{subj\_rel }\SpecialCharTok{==} \DecValTok{1}\NormalTok{),}
                \ControlFlowTok{function}\NormalTok{(n)}
                \FunctionTok{paste0}\NormalTok{(}\StringTok{\textquotesingle{}log\_reading\_time\_pred[\textquotesingle{}}\NormalTok{, n, }\StringTok{\textquotesingle{}]\textquotesingle{}}\NormalTok{))}
\NormalTok{filtered\_samples }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{filter\_expectands}\NormalTok{(samples\_joint, names)}

\NormalTok{obs\_reading\_time }\OtherTok{\textless{}{-}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{reading\_time[data}\SpecialCharTok{$}\NormalTok{subj\_rel }\SpecialCharTok{==} \DecValTok{1}\NormalTok{]}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_hist\_quantiles}\NormalTok{(filtered\_samples,}
                         \StringTok{\textquotesingle{}log\_reading\_time\_pred\textquotesingle{}}\NormalTok{,}
                         \DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.1}\NormalTok{,}
                         \AttributeTok{baseline\_values=}\FunctionTok{log}\NormalTok{(obs\_reading\_time),}
                         \AttributeTok{xlab=}\StringTok{\textquotesingle{}Log Reading Time / 1 ms\textquotesingle{}}\NormalTok{,}
                         \AttributeTok{main=}\StringTok{\textquotesingle{}Subject Relative\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 37 predictive values (0.0%) fell above the binning.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{names }\OtherTok{\textless{}{-}} \FunctionTok{sapply}\NormalTok{(}\FunctionTok{which}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{subj\_rel }\SpecialCharTok{==} \DecValTok{0}\NormalTok{),}
                \ControlFlowTok{function}\NormalTok{(n)}
                \FunctionTok{paste0}\NormalTok{(}\StringTok{\textquotesingle{}log\_reading\_time\_pred[\textquotesingle{}}\NormalTok{, n, }\StringTok{\textquotesingle{}]\textquotesingle{}}\NormalTok{))}
\NormalTok{filtered\_samples }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{filter\_expectands}\NormalTok{(samples\_joint, names)}

\NormalTok{obs\_reading\_time }\OtherTok{\textless{}{-}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{reading\_time[data}\SpecialCharTok{$}\NormalTok{subj\_rel }\SpecialCharTok{==} \DecValTok{0}\NormalTok{]}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_hist\_quantiles}\NormalTok{(filtered\_samples,}
                         \StringTok{\textquotesingle{}log\_reading\_time\_pred\textquotesingle{}}\NormalTok{,}
                         \DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.1}\NormalTok{,}
                         \AttributeTok{baseline\_values=}\FunctionTok{log}\NormalTok{(obs\_reading\_time),}
                         \AttributeTok{xlab=}\StringTok{\textquotesingle{}Log Reading Time / 1 ms\textquotesingle{}}\NormalTok{,}
                         \AttributeTok{main=}\StringTok{\textquotesingle{}Object Relative\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 36 predictive values (0.0%) fell above the binning.
\end{verbatim}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-62-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in}\NormalTok{ display\_items) \{}
\NormalTok{  names }\OtherTok{\textless{}{-}} \FunctionTok{sapply}\NormalTok{(}\FunctionTok{which}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{item }\SpecialCharTok{==}\NormalTok{ i),}
                  \ControlFlowTok{function}\NormalTok{(n)}
                  \FunctionTok{paste0}\NormalTok{(}\StringTok{\textquotesingle{}log\_reading\_time\_pred[\textquotesingle{}}\NormalTok{, n, }\StringTok{\textquotesingle{}]\textquotesingle{}}\NormalTok{))}
\NormalTok{  filtered\_samples }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{filter\_expectands}\NormalTok{(samples\_joint, names)}

\NormalTok{  obs\_reading\_time }\OtherTok{\textless{}{-}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{reading\_time[data}\SpecialCharTok{$}\NormalTok{item }\SpecialCharTok{==}\NormalTok{ i]}

\NormalTok{  util}\SpecialCharTok{$}\FunctionTok{plot\_hist\_quantiles}\NormalTok{(filtered\_samples,}
                           \StringTok{\textquotesingle{}log\_reading\_time\_pred\textquotesingle{}}\NormalTok{,}
                           \DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.25}\NormalTok{,}
                           \AttributeTok{baseline\_values=}\FunctionTok{log}\NormalTok{(obs\_reading\_time),}
                           \AttributeTok{xlab=}\StringTok{\textquotesingle{}Log Reading Time / 1 ms\textquotesingle{}}\NormalTok{,}
                           \AttributeTok{main=}\FunctionTok{paste}\NormalTok{(}\StringTok{\textquotesingle{}Item\textquotesingle{}}\NormalTok{, i))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 3 predictive values (0.0%) fell above the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 7 predictive values (0.0%) fell above the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 3 predictive values (0.0%) fell above the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 4 predictive values (0.0%) fell above the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 3 predictive values (0.0%) fell above the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 6 predictive values (0.0%) fell above the binning.

Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 6 predictive values (0.0%) fell above the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 3 predictive values (0.0%) fell above the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 8 predictive values (0.0%) fell above the binning.
\end{verbatim}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-63-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\ControlFlowTok{for}\NormalTok{ (s }\ControlFlowTok{in}\NormalTok{ display\_subjects) \{}
  \ControlFlowTok{if}\NormalTok{ (s }\SpecialCharTok{==} \DecValTok{13}\NormalTok{) \{}
    \FunctionTok{plot.new}\NormalTok{()}
    \ControlFlowTok{next}
\NormalTok{  \}}

\NormalTok{  names }\OtherTok{\textless{}{-}} \FunctionTok{sapply}\NormalTok{(}\FunctionTok{which}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{subject }\SpecialCharTok{==}\NormalTok{ s),}
                  \ControlFlowTok{function}\NormalTok{(n)}
                  \FunctionTok{paste0}\NormalTok{(}\StringTok{\textquotesingle{}log\_reading\_time\_pred[\textquotesingle{}}\NormalTok{, n, }\StringTok{\textquotesingle{}]\textquotesingle{}}\NormalTok{))}
\NormalTok{  filtered\_samples }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{filter\_expectands}\NormalTok{(samples\_joint, names)}

\NormalTok{  obs\_reading\_time }\OtherTok{\textless{}{-}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{reading\_time[data}\SpecialCharTok{$}\NormalTok{subject }\SpecialCharTok{==}\NormalTok{ s]}

\NormalTok{  util}\SpecialCharTok{$}\FunctionTok{plot\_hist\_quantiles}\NormalTok{(filtered\_samples,}
                          \StringTok{\textquotesingle{}log\_reading\_time\_pred\textquotesingle{}}\NormalTok{,}
                           \DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.25}\NormalTok{,}
                           \AttributeTok{baseline\_values=}\FunctionTok{log}\NormalTok{(obs\_reading\_time),}
                           \AttributeTok{xlab=}\StringTok{\textquotesingle{}Log Reading Time / 1 ms\textquotesingle{}}\NormalTok{,}
                           \AttributeTok{main=}\FunctionTok{paste}\NormalTok{(}\StringTok{\textquotesingle{}Subject\textquotesingle{}}\NormalTok{, s))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 1 predictive value (0.0%) fell above the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 2 predictive values (0.0%) fell above the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 3 predictive values (0.0%) fell above the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 1 predictive value (0.0%) fell above the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 2 predictive values (0.0%) fell above the binning.
\end{verbatim}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-64-1.pdf}

\subsubsection{Posterior Inferences}\label{posterior-inferences-1}

We can now use posterior inferences to quantify the contribution from
the dependency locality and direct-access model behaviors.

The initial success probabilities for the subject-extracted and
object-extracted relative clauses are both far from unity, indicating
that the direct-access model is doing a lot of work. While the
difference between the two appears to be smaller than what we saw in the
inverse gamma direct-access model alone, this is an artifact of slightly
larger posterior uncertainties caused by the increased flexibility of
the joint model.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_expectand\_pushforward}\NormalTok{(samples\_joint[[}\StringTok{\textquotesingle{}lambda\_SR\textquotesingle{}}\NormalTok{]], }\DecValTok{25}\NormalTok{,}
                                \AttributeTok{flim=}\FunctionTok{c}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.4}\NormalTok{),}
                                \AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{25}\NormalTok{),}
                                \AttributeTok{display\_name=}\StringTok{"Initial Success Probability"}\NormalTok{,}
                                \AttributeTok{col=}\NormalTok{util}\SpecialCharTok{$}\NormalTok{c\_mid)}
\FunctionTok{text}\NormalTok{(}\FloatTok{0.35}\NormalTok{, }\DecValTok{15}\NormalTok{, }\StringTok{\textquotesingle{}Object}\SpecialCharTok{\textbackslash{}n}\StringTok{Relative\textquotesingle{}}\NormalTok{, }\AttributeTok{col=}\NormalTok{util}\SpecialCharTok{$}\NormalTok{c\_mid)}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_expectand\_pushforward}\NormalTok{(samples\_joint[[}\StringTok{\textquotesingle{}lambda\_OR\textquotesingle{}}\NormalTok{]], }\DecValTok{25}\NormalTok{,}
                                \AttributeTok{flim=}\FunctionTok{c}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.4}\NormalTok{),}
                                \AttributeTok{col=}\NormalTok{util}\SpecialCharTok{$}\NormalTok{c\_dark,}
                                \AttributeTok{border=}\StringTok{"\#BBBBBB88"}\NormalTok{,}
                                \AttributeTok{add=}\ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{text}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\DecValTok{12}\NormalTok{, }\StringTok{\textquotesingle{}Subject}\SpecialCharTok{\textbackslash{}n}\StringTok{Relative\textquotesingle{}}\NormalTok{, }\AttributeTok{col=}\NormalTok{util}\SpecialCharTok{$}\NormalTok{c\_dark)}
\end{Highlighting}
\end{Shaded}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-65-1.pdf}

This doesn't, however, imply that the behaviors from the dependency
locality model aren't also contributing. To quantify that we need to
look at our posterior inferences for \(\chi\).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_expectand\_pushforward}\NormalTok{(samples\_joint[[}\StringTok{"chi"}\NormalTok{]], }\DecValTok{25}\NormalTok{,}
                                \AttributeTok{display\_name=}\StringTok{"chi"}\NormalTok{,}
                                \AttributeTok{baseline=}\DecValTok{0}\NormalTok{,}
                                \AttributeTok{baseline\_col=}\NormalTok{util}\SpecialCharTok{$}\NormalTok{c\_mid\_teal)}
\end{Highlighting}
\end{Shaded}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-66-1.pdf}

Because the marginal posterior distribution for \(\chi\) strongly
concentrates around zero, the contribution from the dependency locality
model appears to be negligible. These behaviors just don't help the
joint model fit the observed data.

If we can specify thresholds \(t_{l}\) and \(t_{u}\) that quantify what
a ``non-negligible'' contribution is, then we can summarize the
suppression of the dependency locality model with the posterior
probability that \[
t_{l} < \chi < t_{u}.
\] For example if 5\% proportional changes to the location behavior are
considered negligible for a given application then we could compute
\begin{align*}
\log \left( \frac{1}{1.05} \right) < &\, \chi < \log \left( 1.05 \right)
\\
-\log \left( 1.05 \right) < &\, \chi < \log \left( 1.05 \right)
\\
-\log \left( 1.05 \right) < &\, \chi < \log \left( 1.05 \right)
\\
-0.049 < &\, \chi < 0.049.
\end{align*}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p\_est }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{implicit\_subset\_prob}\NormalTok{(samples\_joint,}
                                   \ControlFlowTok{function}\NormalTok{(x)}
                                   \SpecialCharTok{{-}}\FloatTok{0.049} \SpecialCharTok{\textless{}}\NormalTok{ x }\SpecialCharTok{\&\&}\NormalTok{ x }\SpecialCharTok{\textless{}} \SpecialCharTok{+}\FloatTok{0.049}\NormalTok{,}
                                   \FunctionTok{list}\NormalTok{(}\StringTok{\textquotesingle{}x\textquotesingle{}} \OtherTok{=} \StringTok{\textquotesingle{}chi\textquotesingle{}}\NormalTok{))}

\NormalTok{format\_string }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\StringTok{"Posterior probability that {-}0.049 \textless{} chi "}\NormalTok{,}
                        \StringTok{"\textless{} +0.049 = \%.3f +/{-} \%.3f."}\NormalTok{)}
\FunctionTok{cat}\NormalTok{(}\FunctionTok{sprintf}\NormalTok{(format\_string, p\_est[}\DecValTok{1}\NormalTok{], }\DecValTok{2} \SpecialCharTok{*}\NormalTok{ p\_est[}\DecValTok{2}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Posterior probability that -0.049 < chi < +0.049 = 0.997 +/- 0.002.
\end{verbatim}

In this case, almost \emph{all} of the posterior probability
concentrates on negligible model configurations.

Finally the joint baseline \(\tau\) is consistent with the direct-access
baseline \(\nu\).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_expectand\_pushforward}\NormalTok{(samples\_dlt2[[}\StringTok{"kappa"}\NormalTok{]],}
                                \DecValTok{50}\NormalTok{, }\AttributeTok{flim=}\FunctionTok{c}\NormalTok{(}\FloatTok{5.5}\NormalTok{, }\FloatTok{6.75}\NormalTok{),}
                                \AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{),}
                                \AttributeTok{display\_name=}\StringTok{""}\NormalTok{,}
                                \AttributeTok{col=}\NormalTok{util}\SpecialCharTok{$}\NormalTok{c\_light)}
\FunctionTok{text}\NormalTok{(}\FloatTok{6.6}\NormalTok{, }\DecValTok{8}\NormalTok{, }\StringTok{\textquotesingle{}kappa\textquotesingle{}}\NormalTok{, }\AttributeTok{col=}\NormalTok{util}\SpecialCharTok{$}\NormalTok{c\_light)}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_expectand\_pushforward}\NormalTok{(samples\_dat2[[}\StringTok{"nu"}\NormalTok{]],}
                                \DecValTok{50}\NormalTok{, }\AttributeTok{flim=}\FunctionTok{c}\NormalTok{(}\FloatTok{5.5}\NormalTok{, }\FloatTok{6.75}\NormalTok{),}
                                \AttributeTok{col=}\NormalTok{util}\SpecialCharTok{$}\NormalTok{c\_mid,}
                                \AttributeTok{border=}\StringTok{"\#BBBBBB88"}\NormalTok{,}
                                \AttributeTok{add=}\ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{text}\NormalTok{(}\FloatTok{5.8}\NormalTok{, }\DecValTok{8}\NormalTok{, }\StringTok{\textquotesingle{}nu\textquotesingle{}}\NormalTok{, }\AttributeTok{col=}\NormalTok{util}\SpecialCharTok{$}\NormalTok{c\_mid)}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_expectand\_pushforward}\NormalTok{(samples\_joint[[}\StringTok{"tau"}\NormalTok{]],}
                                \DecValTok{50}\NormalTok{, }\AttributeTok{flim=}\FunctionTok{c}\NormalTok{(}\FloatTok{5.5}\NormalTok{, }\FloatTok{6.75}\NormalTok{),}
                                \AttributeTok{col=}\NormalTok{util}\SpecialCharTok{$}\NormalTok{c\_dark,}
                                \AttributeTok{border=}\StringTok{"\#BBBBBB88"}\NormalTok{,}
                                \AttributeTok{add=}\ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{text}\NormalTok{(}\FloatTok{5.6}\NormalTok{, }\DecValTok{8}\NormalTok{, }\StringTok{\textquotesingle{}tau\textquotesingle{}}\NormalTok{, }\AttributeTok{col=}\NormalTok{util}\SpecialCharTok{$}\NormalTok{c\_dark)}
\end{Highlighting}
\end{Shaded}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-68-1.pdf}

\subsection{Modeling Discretization}\label{sec:mod_disc}

Once we adopted an inverse gamma direct-access model, the posterior
predictive distribution spanned the range of observed behaviors. In
particular, the spikes in the observed data that I speculated might be
discretization artifacts don't really transcend the posterior predictive
uncertainties until we start stratifying the reading times into small
groups. Consequently it appears that, in this case, a continuous model
is sufficient to learn from the discrete observations.

What would we have done, however, if potential discretization artifacts
in the observed data were less amenable with the posterior predictive
distribution? In this section we'll review a few techniques for
accommodating discrete observations.

\subsubsection{Modeling Rounding}\label{modeling-rounding}

One strategy that we might consider is to treat the discretization of
the observed reading times as part of the data generating process
itself.

Let's say, for example, that continuous reading times are rounded to the
nearest integer. More formally, for any integer \(k \in \mathbb{Z}\) all
continuous reading times in the interval \[
\mathsf{r}_{k} = [ k - 0.5, k + 0.5 )
\] are mapped to \(k\).

To model this censoring process we compute the probability of each
interval, and hence each integer value, from the latent continuous
model, \begin{align*}
q_{k}
&=
\pi ( \mathsf{r}_{k} \mid \theta )
\\
&=
\int_{k - 0.5}^{k + 0.5} \mathrm{d} t \, p( t \mid \theta ).
\end{align*} The observational model for any discrete observation \(k\)
is then just a categorical model, \[
p ( k )
= \text{categorical} ( k \mid q_{1}, \ldots )
= q_{k}.
\]

We can use this basic approach to also model reading times that have
been rounded down to the next smallest integer or up to the next highest
integer. All we have to do is change the continuous interval that maps
to each integer reading time. For example, when modeling continuous
reading times that are rounded up to the next highest integer we would
map all reading times in the interval \[
( k - 1, k ]
\] to the integer \(k \in \mathbb{Z}\).

These censored models are straightforward to implement if we can
directly calculate the interval probabilities. This, in turn, usually
requires being able to evaluate all of the relevant cumulative
distribution functions.

Consider, for instance, the direct-access model \[
p( t_{ij} \mid \theta)
       \lambda  \, p_{1}( t_{ij} \mid \mu_{1, ij}, \phi_{1})
+ (1 - \lambda) \, p_{2}( t_{ij} \mid \mu_{2, ij}, \phi_{2})
\] where \[
\theta = ( \lambda, \mu_{1, ij}, \phi_{1} \mu_{2, ij}, \phi_{2}).
\] The probability allocated to the interval \([ k - 0.5, k + 0.5 )\)
can be evaluated as \begin{align*}
q_{k}
&=
\int_{k - 0.5}^{k + 0.5} \mathrm{d} t_{ij} \,
p( t_{ij} \mid \theta )
\\
&=\quad
\lambda
\int_{k - 0.5}^{k + 0.5} \mathrm{d} t_{ij} \,
p_{1}( t_{ij} \mid \mu_{1, ij}, \phi_{1})
\\
&\quad+
(1 - \lambda)
\int_{k - 0.5}^{k + 0.5} \mathrm{d} t_{ij} \,
p_{2}( t_{ij} \mid \mu_{2, ij}, \phi_{2})
\\
&=\quad
\quad\quad \lambda \;\, \,
\big[   \Pi_{1} ( k + 0.5 \mid \mu_{1, ij}, \phi_{1} )
       - \Pi_{1} ( k - 0.5 \mid \mu_{1, ij}, \phi_{1} ) \big]
\\
&\quad+
(1 - \lambda) \,
\big[   \Pi_{2} ( k + 0.5 \mid \mu_{2, ij}, \phi_{2} )
       - \Pi_{2} ( k - 0.5 \mid \mu_{2, ij}, \phi_{2} ) \big].
\end{align*}

If \(p_{1}\) and \(p_{2}\) are both inverse gamma models then
\(\Pi_{1}\) and \(\Pi_{2}\) are given by different configurations of the
inverse gamma cumulative distribution function. Conveniently this
function is available in the \texttt{Stan} modeling language, making it
straightforward to implement the censored models as Stan programs.

\begin{codelisting}

\caption{\texttt{dat2\textbackslash\_rounding.stan}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{functions}\NormalTok{ \{}
  \CommentTok{// Mean{-}dispersion parameterization of inverse gamma family}
  \DataTypeTok{real}\NormalTok{ inv\_gamma\_md\_lpdf(}\DataTypeTok{real}\NormalTok{ x, }\DataTypeTok{real}\NormalTok{ log\_mu, }\DataTypeTok{real}\NormalTok{ psi) \{}
    \ControlFlowTok{return}\NormalTok{ inv\_gamma\_lpdf(x | inv(psi) + }\DecValTok{2}\NormalTok{,}
\NormalTok{                              exp(log\_mu) * (inv(psi) + }\DecValTok{1}\NormalTok{));}
\NormalTok{  \}}

  \DataTypeTok{real}\NormalTok{ inv\_gamma\_md\_cdf(}\DataTypeTok{real}\NormalTok{ x, }\DataTypeTok{real}\NormalTok{ log\_mu, }\DataTypeTok{real}\NormalTok{ psi) \{}
    \ControlFlowTok{return}\NormalTok{ inv\_gamma\_cdf(x | inv(psi) + }\DecValTok{2}\NormalTok{,}
\NormalTok{                             exp(log\_mu) * (inv(psi) + }\DecValTok{1}\NormalTok{));}
\NormalTok{  \}}

  \DataTypeTok{real}\NormalTok{ inv\_gamma\_md\_rng(}\DataTypeTok{real}\NormalTok{ log\_mu, }\DataTypeTok{real}\NormalTok{ psi) \{}
    \ControlFlowTok{return}\NormalTok{ inv\_gamma\_rng(inv(psi) + }\DecValTok{2}\NormalTok{,}
\NormalTok{                         exp(log\_mu) * (inv(psi) + }\DecValTok{1}\NormalTok{));}
\NormalTok{  \}}
\NormalTok{\}}

\KeywordTok{data}\NormalTok{ \{}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N; }\CommentTok{// Number of observations}

  \CommentTok{// Item configuration}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N\_items;}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{, }\KeywordTok{upper}\NormalTok{=N\_items\textgreater{} item;}

  \CommentTok{// Subject configuration}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N\_subjects;}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{, }\KeywordTok{upper}\NormalTok{=N\_subjects\textgreater{} subject;}

  \CommentTok{// Item variant}
  \CommentTok{//   Object Relative:  subj\_rel = 0}
  \CommentTok{//   Subject Relative: subj\_rel = 1}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} subj\_rel;}

  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} reading\_time; }\CommentTok{// Reading times (ms)}
\NormalTok{\}}

\KeywordTok{parameters}\NormalTok{ \{}
  \CommentTok{// Log reading time baseline for successful retrieval}
  \DataTypeTok{real}\NormalTok{ nu;}

  \CommentTok{// Relative item difficulties}
  \DataTypeTok{vector}\NormalTok{[N\_items {-} }\DecValTok{1}\NormalTok{] delta\_free;}

  \CommentTok{// Relative subject skills}
  \DataTypeTok{vector}\NormalTok{[N\_subjects {-} }\DecValTok{1}\NormalTok{] zeta\_free;}

  \CommentTok{// Log offset for initial failure before successful retrieval}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} omega;}

  \CommentTok{// Measurement scales}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} phi1;}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} phi2;}

  \CommentTok{// Mixture probabilities}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} lambda\_SR;}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} lambda\_OR;}
\NormalTok{\}}

\KeywordTok{transformed parameters}\NormalTok{ \{}
  \CommentTok{// Relative skills for all items and subjects}
  \DataTypeTok{vector}\NormalTok{[N\_items] delta}
\NormalTok{    = append\_row([}\DecValTok{0}\NormalTok{]\textquotesingle{}, delta\_free);}
  \DataTypeTok{vector}\NormalTok{[N\_subjects] zeta}
\NormalTok{    = append\_row([}\DecValTok{0}\NormalTok{]\textquotesingle{}, zeta\_free);}
\NormalTok{\}}

\KeywordTok{model}\NormalTok{ \{}
  \CommentTok{// Prior model}

  \CommentTok{// 100 \textless{}\textasciitilde{} exp(nu) \textless{}\textasciitilde{} 10000}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(nu | }\FloatTok{6.91}\NormalTok{, }\FloatTok{0.99}\NormalTok{);}

  \CommentTok{// 0.1 \textless{}\textasciitilde{} exp(delta) \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(delta\_free | }\DecValTok{0}\NormalTok{, }\FloatTok{0.99}\NormalTok{);}

  \CommentTok{// 0.1 \textless{}\textasciitilde{} exp(zeta) \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(zeta\_free| }\DecValTok{0}\NormalTok{, }\FloatTok{0.99}\NormalTok{);}

  \CommentTok{// 1 \textless{}\textasciitilde{} exp(omega) \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(omega | }\DecValTok{0}\NormalTok{, }\FloatTok{0.90}\NormalTok{);}

  \CommentTok{// 0 \textless{}\textasciitilde{} phi \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(phi1 | }\DecValTok{0}\NormalTok{, }\FloatTok{3.89}\NormalTok{);}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(phi2 | }\DecValTok{0}\NormalTok{, }\FloatTok{3.89}\NormalTok{);}

  \CommentTok{// Uniform prior density functions}
  \KeywordTok{target +=}\NormalTok{ beta\_lpdf(lambda\_SR | }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{);}
  \KeywordTok{target +=}\NormalTok{ beta\_lpdf(lambda\_OR | }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{);}

  \CommentTok{// Observational model}
  \ControlFlowTok{for}\NormalTok{ (n }\ControlFlowTok{in} \DecValTok{1}\NormalTok{:N) \{}
    \DataTypeTok{int}\NormalTok{ i = item[n];}
    \DataTypeTok{int}\NormalTok{ s = subject[n];}
    \DataTypeTok{real}\NormalTok{ log\_mu = nu + delta[i] {-} zeta[s];}

    \DataTypeTok{real}\NormalTok{ p1}
\NormalTok{      =   inv\_gamma\_md\_cdf(reading\_time[n] + }\FloatTok{0.5}\NormalTok{ | log\_mu,         phi1)}
\NormalTok{        {-} inv\_gamma\_md\_cdf(reading\_time[n] {-} }\FloatTok{0.5}\NormalTok{ | log\_mu,         phi1);}
    \DataTypeTok{real}\NormalTok{ p2}
\NormalTok{      =   inv\_gamma\_md\_cdf(reading\_time[n] + }\FloatTok{0.5}\NormalTok{ | log\_mu + omega, phi2)}
\NormalTok{        {-} inv\_gamma\_md\_cdf(reading\_time[n] {-} }\FloatTok{0.5}\NormalTok{ | log\_mu + omega, phi2);}

    \ControlFlowTok{if}\NormalTok{ (subj\_rel[n] == }\DecValTok{1}\NormalTok{) \{}
      \KeywordTok{target +=}\NormalTok{ log(lambda\_SR * p1 + (}\DecValTok{1}\NormalTok{ {-} lambda\_SR) * p2);}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
      \KeywordTok{target +=}\NormalTok{ log(lambda\_OR * p1 + (}\DecValTok{1}\NormalTok{ {-} lambda\_OR) * p2);}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\}}

\KeywordTok{generated quantities}\NormalTok{ \{}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{real}\NormalTok{ log\_reading\_time\_pred;}

  \ControlFlowTok{for}\NormalTok{ (n }\ControlFlowTok{in} \DecValTok{1}\NormalTok{:N) \{}
    \DataTypeTok{int}\NormalTok{ i = item[n];}
    \DataTypeTok{int}\NormalTok{ s = subject[n];}
    \DataTypeTok{real}\NormalTok{ log\_mu = nu + delta[i] {-} zeta[s];}

    \ControlFlowTok{if}\NormalTok{ (subj\_rel[n] == }\DecValTok{1}\NormalTok{) \{}
      \ControlFlowTok{if}\NormalTok{ (bernoulli\_rng(lambda\_SR)) \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(round(inv\_gamma\_md\_rng(log\_mu,         phi1)));}
\NormalTok{      \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(round(inv\_gamma\_md\_rng(log\_mu + omega, phi2)));}
\NormalTok{      \}}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
       \ControlFlowTok{if}\NormalTok{ (bernoulli\_rng(lambda\_OR)) \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(round(inv\_gamma\_md\_rng(log\_mu,         phi1)));}
\NormalTok{      \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(round(inv\_gamma\_md\_rng(log\_mu + omega, phi2)));}
\NormalTok{      \}}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

The only problem with this approach is that many cumulative distribution
functions are expensive to evaluate. Moreover, they are not always the
most numerically stable functions. Stan's implementation of the inverse
gamma function, for example, can be problematic at larger inputs. The
resulting numerical errors then frustrate accurate Hamiltonian Monte
Carlo.

To demonstrate the implementation of a censored model we'll instead go
back to our first direct-access model. The log normal cumulative
distribution function is much more computationally robust.

\begin{codelisting}

\caption{\texttt{dat1\textbackslash\_rounding.stan}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{ \{}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N; }\CommentTok{// Number of observations}

  \CommentTok{// Item configuration}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N\_items;}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{, }\KeywordTok{upper}\NormalTok{=N\_items\textgreater{} item;}

  \CommentTok{// Subject configuration}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N\_subjects;}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{, }\KeywordTok{upper}\NormalTok{=N\_subjects\textgreater{} subject;}

  \CommentTok{// Item variant}
  \CommentTok{//   Object Relative:  subj\_rel = 0}
  \CommentTok{//   Subject Relative: subj\_rel = 1}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} subj\_rel;}

  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} reading\_time; }\CommentTok{// Reading times (ms)}
\NormalTok{\}}

\KeywordTok{parameters}\NormalTok{ \{}
  \CommentTok{// Log reading time baseline for successful retrieval}
  \DataTypeTok{real}\NormalTok{ nu;}

  \CommentTok{// Relative item difficulties}
  \DataTypeTok{vector}\NormalTok{[N\_items {-} }\DecValTok{1}\NormalTok{] delta\_free;}

  \CommentTok{// Relative subject skills}
  \DataTypeTok{vector}\NormalTok{[N\_subjects {-} }\DecValTok{1}\NormalTok{] zeta\_free;}

  \CommentTok{// Log offset for initial failure before successful retrieval}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} omega;}

  \CommentTok{// Measurement scales}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} phi1;}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} phi2;}

  \CommentTok{// Mixture probabilities}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} lambda\_SR;}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} lambda\_OR;}
\NormalTok{\}}

\KeywordTok{transformed parameters}\NormalTok{ \{}
  \CommentTok{// Relative skills for all items and subjects}
  \DataTypeTok{vector}\NormalTok{[N\_items] delta}
\NormalTok{    = append\_row([}\DecValTok{0}\NormalTok{]\textquotesingle{}, delta\_free);}
  \DataTypeTok{vector}\NormalTok{[N\_subjects] zeta}
\NormalTok{    = append\_row([}\DecValTok{0}\NormalTok{]\textquotesingle{}, zeta\_free);}
\NormalTok{\}}

\KeywordTok{model}\NormalTok{ \{}
  \CommentTok{// Prior model}

  \CommentTok{// 100 \textless{}\textasciitilde{} exp(nu) \textless{}\textasciitilde{} 1000}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(nu | }\FloatTok{5.76}\NormalTok{, }\FloatTok{0.50}\NormalTok{);}

  \CommentTok{// 0.1 \textless{}\textasciitilde{} exp(delta) \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(delta\_free | }\DecValTok{0}\NormalTok{, }\FloatTok{0.99}\NormalTok{);}

  \CommentTok{// 0.1 \textless{}\textasciitilde{} exp(zeta) \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(zeta\_free| }\DecValTok{0}\NormalTok{, }\FloatTok{0.99}\NormalTok{);}

  \CommentTok{// 1 \textless{}\textasciitilde{} exp(omega) \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(omega | }\DecValTok{0}\NormalTok{, }\FloatTok{0.90}\NormalTok{);}

  \CommentTok{// 0 \textless{}\textasciitilde{} phi \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(phi1 | }\DecValTok{0}\NormalTok{, }\FloatTok{3.89}\NormalTok{);}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(phi2 | }\DecValTok{0}\NormalTok{, }\FloatTok{3.89}\NormalTok{);}

  \CommentTok{// Uniform prior density functions}
  \KeywordTok{target +=}\NormalTok{ beta\_lpdf(lambda\_SR | }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{);}
  \KeywordTok{target +=}\NormalTok{ beta\_lpdf(lambda\_OR | }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{);}

  \CommentTok{// Observational model}
  \ControlFlowTok{for}\NormalTok{ (n }\ControlFlowTok{in} \DecValTok{1}\NormalTok{:N) \{}
    \DataTypeTok{int}\NormalTok{ i = item[n];}
    \DataTypeTok{int}\NormalTok{ s = subject[n];}
    \DataTypeTok{real}\NormalTok{ log\_mu = nu + delta[i] {-} zeta[s];}

    \DataTypeTok{real}\NormalTok{ p1}
\NormalTok{      =   lognormal\_cdf( reading\_time[n] + }\FloatTok{0.5}\NormalTok{ | log\_mu,        phi1)}
\NormalTok{        {-} lognormal\_cdf( reading\_time[n] {-} }\FloatTok{0.5}\NormalTok{ | log\_mu,        phi1);}
    \DataTypeTok{real}\NormalTok{ p2}
\NormalTok{      =   lognormal\_cdf( reading\_time[n] + }\FloatTok{0.5}\NormalTok{ | log\_mu + omega, phi2)}
\NormalTok{        {-} lognormal\_cdf( reading\_time[n] {-} }\FloatTok{0.5}\NormalTok{ | log\_mu + omega, phi2);}

    \ControlFlowTok{if}\NormalTok{ (subj\_rel[n] == }\DecValTok{1}\NormalTok{) \{}
      \KeywordTok{target +=}\NormalTok{ log(lambda\_SR * p1 + (}\DecValTok{1}\NormalTok{ {-} lambda\_SR) * p2);}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
      \KeywordTok{target +=}\NormalTok{ log(lambda\_OR * p1 + (}\DecValTok{1}\NormalTok{ {-} lambda\_OR) * p2);}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\}}

\KeywordTok{generated quantities}\NormalTok{ \{}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{real}\NormalTok{ log\_reading\_time\_pred;}

  \ControlFlowTok{for}\NormalTok{ (n }\ControlFlowTok{in} \DecValTok{1}\NormalTok{:N) \{}
    \DataTypeTok{int}\NormalTok{ i = item[n];}
    \DataTypeTok{int}\NormalTok{ s = subject[n];}
    \DataTypeTok{real}\NormalTok{ log\_mu = nu + delta[i] {-} zeta[s];}

    \ControlFlowTok{if}\NormalTok{ (subj\_rel[n] == }\DecValTok{0}\NormalTok{) \{}
      \ControlFlowTok{if}\NormalTok{ (bernoulli\_rng(lambda\_SR)) \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(round(lognormal\_rng(log\_mu        , phi1)));}
\NormalTok{      \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(round(lognormal\_rng(log\_mu + omega, phi2)));}
\NormalTok{      \}}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
       \ControlFlowTok{if}\NormalTok{ (bernoulli\_rng(lambda\_OR)) \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(round(lognormal\_rng(log\_mu        , phi1)));}
\NormalTok{      \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(round(lognormal\_rng(log\_mu + omega, phi2)));}
\NormalTok{      \}}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{stan}\NormalTok{(}\AttributeTok{file=}\StringTok{\textquotesingle{}stan\_programs/dat1\_rounding.stan\textquotesingle{}}\NormalTok{,}
            \AttributeTok{data=}\NormalTok{data, }\AttributeTok{seed=}\DecValTok{8438338}\NormalTok{,}
            \AttributeTok{warmup=}\DecValTok{1000}\NormalTok{, }\AttributeTok{iter=}\DecValTok{2024}\NormalTok{, }\AttributeTok{refresh=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Interestingly, we see similar split \(\hat{r}\) diagnostic warnings that
we encountered when trying to explore the uncensored log normal
direct-access model. In addition a stray divergence has popped up.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diagnostics }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{extract\_hmc\_diagnostics}\NormalTok{(fit)}
\NormalTok{util}\SpecialCharTok{$}\FunctionTok{check\_all\_hmc\_diagnostics}\NormalTok{(diagnostics)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  All Hamiltonian Monte Carlo diagnostics are consistent with reliable
Markov chain Monte Carlo.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{samples\_dat1r }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{extract\_expectand\_vals}\NormalTok{(fit)}
\NormalTok{base\_samples }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{filter\_expectands}\NormalTok{(samples\_dat1r,}
                                       \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}nu\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}delta\_free\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}zeta\_free\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}omega\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}phi1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}phi2\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}lambda\_SR\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}lambda\_OR\textquotesingle{}}\NormalTok{),}
                                       \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{util}\SpecialCharTok{$}\FunctionTok{summarize\_expectand\_diagnostics}\NormalTok{(base\_samples)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The expectands nu, delta_free[6], delta_free[7], delta_free[9],
delta_free[14], delta_free[15], zeta_free[1], zeta_free[2],
zeta_free[3], zeta_free[4], zeta_free[5], zeta_free[6], zeta_free[7],
zeta_free[8], zeta_free[10], zeta_free[11], zeta_free[13],
zeta_free[14], zeta_free[15], zeta_free[16], zeta_free[17],
zeta_free[18], zeta_free[19], zeta_free[20], zeta_free[22],
zeta_free[23], zeta_free[25], zeta_free[27], zeta_free[29],
zeta_free[30], zeta_free[32], zeta_free[33], zeta_free[34],
zeta_free[35], zeta_free[36], zeta_free[38], zeta_free[39], omega,
phi1, lambda_OR triggered diagnostic warnings.

The expectands nu, delta_free[6], delta_free[7], delta_free[9],
delta_free[14], delta_free[15], zeta_free[5], zeta_free[8],
zeta_free[13], zeta_free[14], zeta_free[16], zeta_free[18],
zeta_free[25], zeta_free[27], zeta_free[30], zeta_free[32],
zeta_free[38], zeta_free[39], omega, phi1, lambda_OR triggered hat{R}
warnings.

Split Rhat larger than 1.1 suggests that at least one of the Markov
chains has not reached an equilibrium.

The expectands nu, zeta_free[1], zeta_free[2], zeta_free[3],
zeta_free[4], zeta_free[6], zeta_free[7], zeta_free[8], zeta_free[10],
zeta_free[11], zeta_free[14], zeta_free[15], zeta_free[16],
zeta_free[17], zeta_free[18], zeta_free[19], zeta_free[20],
zeta_free[22], zeta_free[23], zeta_free[25], zeta_free[27],
zeta_free[29], zeta_free[32], zeta_free[33], zeta_free[34],
zeta_free[35], zeta_free[36], zeta_free[38], zeta_free[39] triggered
hat{ESS} warnings.

Small empirical effective sample sizes result in imprecise Markov chain
Monte Carlo estimators.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_rhats}\NormalTok{(base\_samples, }\AttributeTok{B=}\DecValTok{50}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-71-1.pdf}

Indeed the posterior multimodality is nearly identical.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_pairs\_by\_chain}\NormalTok{(samples\_dat1r[[}\StringTok{\textquotesingle{}omega\textquotesingle{}}\NormalTok{]], }\StringTok{\textquotesingle{}omega\textquotesingle{}}\NormalTok{,}
\NormalTok{                         samples\_dat1r[[}\StringTok{\textquotesingle{}lambda\_SR\textquotesingle{}}\NormalTok{]], }\StringTok{\textquotesingle{}lambda\_SR\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-72-1.pdf}

Moreover, the posterior retrodictive behavior seems to be unaffected by
the rounding.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_hist\_quantiles}\NormalTok{(}\FunctionTok{lapply}\NormalTok{(samples\_dat1,}
                                \ControlFlowTok{function}\NormalTok{(s) s[}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{),]),}
                         \StringTok{\textquotesingle{}log\_reading\_time\_pred\textquotesingle{}}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.1}\NormalTok{,}
                         \AttributeTok{baseline\_values=}\FunctionTok{log}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{reading\_time),}
                         \AttributeTok{xlab=}\StringTok{\textquotesingle{}Log Reading Time / 1 ms\textquotesingle{}}\NormalTok{,}
                         \AttributeTok{main=}\StringTok{\textquotesingle{}Markov Chains 1 and 3\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 1437 predictive values (0.0%) fell below the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 23 predictive values (0.0%) fell above the binning.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_hist\_quantiles}\NormalTok{(}\FunctionTok{lapply}\NormalTok{(samples\_dat1,}
                                \ControlFlowTok{function}\NormalTok{(s) s[}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{),]),}
                         \StringTok{\textquotesingle{}log\_reading\_time\_pred\textquotesingle{}}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.1}\NormalTok{,}
                         \AttributeTok{baseline\_values=}\FunctionTok{log}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{reading\_time),}
                         \AttributeTok{xlab=}\StringTok{\textquotesingle{}Log Reading Time / 1 ms\textquotesingle{}}\NormalTok{,}
                         \AttributeTok{main=}\StringTok{\textquotesingle{}Markov Chains 2 and 4\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 18 predictive values (0.0%) fell below the binning.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_hist\_quantiles}\NormalTok{(}\FunctionTok{lapply}\NormalTok{(samples\_dat1r,}
                                \ControlFlowTok{function}\NormalTok{(s) s[}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{),]),}
                         \StringTok{\textquotesingle{}log\_reading\_time\_pred\textquotesingle{}}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.1}\NormalTok{,}
                         \AttributeTok{baseline\_values=}\FunctionTok{log}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{reading\_time),}
                         \AttributeTok{xlab=}\StringTok{\textquotesingle{}Log Reading Time / 1 ms\textquotesingle{}}\NormalTok{,}
                         \AttributeTok{main=}\StringTok{\textquotesingle{}Markov Chains 1 and 2 (Censored)\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 21 predictive values (0.0%) fell below the binning.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_hist\_quantiles}\NormalTok{(}\FunctionTok{lapply}\NormalTok{(samples\_dat1r,}
                                \ControlFlowTok{function}\NormalTok{(s) s[}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{),]),}
                         \StringTok{\textquotesingle{}log\_reading\_time\_pred\textquotesingle{}}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.1}\NormalTok{,}
                         \AttributeTok{baseline\_values=}\FunctionTok{log}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{reading\_time),}
                         \AttributeTok{xlab=}\StringTok{\textquotesingle{}Log Reading Time / 1 ms\textquotesingle{}}\NormalTok{,}
                         \AttributeTok{main=}\StringTok{\textquotesingle{}Markov Chains 3 and 4 (Censored)\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 1505 predictive values (0.0%) fell below the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 18 predictive values (0.0%) fell above the binning.
\end{verbatim}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-73-1.pdf}

In this case rounding doesn't appreciably change the behavior of the
continuous model.

\subsubsection{Modeling Discretization
Directly}\label{modeling-discretization-directly}

Another approach that we might consider is to replace any family of
continuous probability models with a qualitatively similar family of
discrete probability models.

For example, in the direct-access model, \[
p( t_{ij} \mid \theta)
       \lambda  \, p_{1}( t_{ij} \mid \mu_{1, ij}, \phi_{1})
+ (1 - \lambda) \, p_{2}( t_{ij} \mid \mu_{2, ij}, \phi_{2})
\] we might consider using a negative binomial model for \(p_{1}\) and
\(p_{2}\) instead of an inverse gamma model. Like the inverse gamma
model, the negative binomial model skews towards larger values. Unlike
the inverse gamma model, however, the negative binomial model is defined
over the positive integers.

The only issue with the negative binomial model is that it tends to be
implemented differently in every software package. To mimic the
location-scale parameterization of the log normal and inverse gamma
models we'll need to use

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{neg\_binomial\_2\_lpdf(y | mu, }\DecValTok{1}\NormalTok{ / phi)}
\end{Highlighting}
\end{Shaded}

or, equivalently,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{neg\_binomial\_2\_log\_lpdf(y | log\_mu, }\DecValTok{1}\NormalTok{ / phi)}
\end{Highlighting}
\end{Shaded}

in the \texttt{Stan} modeling language. This ensures that
\(\phi \rightarrow 0\) configures the narrowest, most symmetric models
while \(\phi \rightarrow \infty\) configures the widest, most skewed
models.

\begin{codelisting}

\caption{\texttt{dat\textbackslash\_nb.stan}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{ \{}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N; }\CommentTok{// Number of observations}

  \CommentTok{// Item configuration}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N\_items;}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{, }\KeywordTok{upper}\NormalTok{=N\_items\textgreater{} item;}

  \CommentTok{// Subject configuration}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} N\_subjects;}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{1}\NormalTok{, }\KeywordTok{upper}\NormalTok{=N\_subjects\textgreater{} subject;}

  \CommentTok{// Item variant}
  \CommentTok{//   Object Relative:  subj\_rel = 0}
  \CommentTok{//   Subject Relative: subj\_rel = 1}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} subj\_rel;}

  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} reading\_time; }\CommentTok{// Reading times (ms)}
\NormalTok{\}}

\KeywordTok{parameters}\NormalTok{ \{}
  \CommentTok{// Log reading time baseline for successful retrieval}
  \DataTypeTok{real}\NormalTok{ nu;}

  \CommentTok{// Relative item difficulties}
  \DataTypeTok{vector}\NormalTok{[N\_items {-} }\DecValTok{1}\NormalTok{] delta\_free;}

  \CommentTok{// Relative subject skills}
  \DataTypeTok{vector}\NormalTok{[N\_subjects {-} }\DecValTok{1}\NormalTok{] zeta\_free;}

  \CommentTok{// Log offset for initial failure before successful retrieval}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} omega;}

  \CommentTok{// Measurement scales}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} phi1;}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} phi2;}

  \CommentTok{// Mixture probabilities}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} lambda\_SR;}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} lambda\_OR;}
\NormalTok{\}}

\KeywordTok{transformed parameters}\NormalTok{ \{}
  \CommentTok{// Relative skills for all items and subjects}
  \DataTypeTok{vector}\NormalTok{[N\_items] delta}
\NormalTok{    = append\_row([}\DecValTok{0}\NormalTok{]\textquotesingle{}, delta\_free);}
  \DataTypeTok{vector}\NormalTok{[N\_subjects] zeta}
\NormalTok{    = append\_row([}\DecValTok{0}\NormalTok{]\textquotesingle{}, zeta\_free);}
\NormalTok{\}}

\KeywordTok{model}\NormalTok{ \{}
  \CommentTok{// Prior model}

  \CommentTok{// 100 \textless{}\textasciitilde{} exp(nu) \textless{}\textasciitilde{} 1000}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(nu | }\FloatTok{5.76}\NormalTok{, }\FloatTok{0.50}\NormalTok{);}

  \CommentTok{// 0.1 \textless{}\textasciitilde{} exp(delta) \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(delta\_free | }\DecValTok{0}\NormalTok{, }\FloatTok{0.99}\NormalTok{);}

  \CommentTok{// 0.1 \textless{}\textasciitilde{} exp(zeta) \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(zeta\_free| }\DecValTok{0}\NormalTok{, }\FloatTok{0.99}\NormalTok{);}

  \CommentTok{// 1 \textless{}\textasciitilde{} exp(omega) \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(omega | }\DecValTok{0}\NormalTok{, }\FloatTok{0.90}\NormalTok{);}

  \CommentTok{// 0 \textless{}\textasciitilde{} phi \textless{}\textasciitilde{} 10}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(phi1 | }\DecValTok{0}\NormalTok{, }\FloatTok{3.89}\NormalTok{);}
  \KeywordTok{target +=}\NormalTok{ normal\_lpdf(phi2 | }\DecValTok{0}\NormalTok{, }\FloatTok{3.89}\NormalTok{);}

  \CommentTok{// Uniform prior density functions}
  \KeywordTok{target +=}\NormalTok{ beta\_lpdf(lambda\_SR | }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{);}
  \KeywordTok{target +=}\NormalTok{ beta\_lpdf(lambda\_OR | }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{);}

  \CommentTok{// Observational model}
  \ControlFlowTok{for}\NormalTok{ (n }\ControlFlowTok{in} \DecValTok{1}\NormalTok{:N) \{}
    \DataTypeTok{int}\NormalTok{ i = item[n];}
    \DataTypeTok{int}\NormalTok{ s = subject[n];}
    \DataTypeTok{real}\NormalTok{ log\_mu = nu + delta[i] {-} zeta[s];}

    \ControlFlowTok{if}\NormalTok{ (subj\_rel[n] == }\DecValTok{1}\NormalTok{) \{}
      \DataTypeTok{real}\NormalTok{ lpd1}
\NormalTok{        =   log(lambda\_SR)}
\NormalTok{          + neg\_binomial\_2\_log\_lpmf(reading\_time[n] |}
\NormalTok{                                    log\_mu        , }\DecValTok{1}\NormalTok{ / phi1);}
      \DataTypeTok{real}\NormalTok{ lpd2}
\NormalTok{        =   log(}\DecValTok{1}\NormalTok{ {-} lambda\_SR)}
\NormalTok{          + neg\_binomial\_2\_log\_lpmf(reading\_time[n] |}
\NormalTok{                                    log\_mu + omega, }\DecValTok{1}\NormalTok{ / phi2);}

      \KeywordTok{target +=}\NormalTok{ log\_sum\_exp(lpd1, lpd2);}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
      \DataTypeTok{real}\NormalTok{ lpd1}
\NormalTok{        =   log(lambda\_OR)}
\NormalTok{          + neg\_binomial\_2\_log\_lpmf(reading\_time[n] |}
\NormalTok{                                    log\_mu        , }\DecValTok{1}\NormalTok{ / phi1);}
      \DataTypeTok{real}\NormalTok{ lpd2}
\NormalTok{        =   log(}\DecValTok{1}\NormalTok{ {-} lambda\_OR)}
\NormalTok{          + neg\_binomial\_2\_log\_lpmf(reading\_time[n] |}
\NormalTok{                                    log\_mu + omega, }\DecValTok{1}\NormalTok{ / phi2);}

      \KeywordTok{target +=}\NormalTok{ log\_sum\_exp(lpd1, lpd2);}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\}}

\KeywordTok{generated quantities}\NormalTok{ \{}
  \DataTypeTok{array}\NormalTok{[N] }\DataTypeTok{real}\NormalTok{ log\_reading\_time\_pred;}

  \ControlFlowTok{for}\NormalTok{ (n }\ControlFlowTok{in} \DecValTok{1}\NormalTok{:N) \{}
    \DataTypeTok{int}\NormalTok{ i = item[n];}
    \DataTypeTok{int}\NormalTok{ s = subject[n];}
    \DataTypeTok{real}\NormalTok{ log\_mu = nu + delta[i] {-} zeta[s];}

    \ControlFlowTok{if}\NormalTok{ (subj\_rel[n] == }\DecValTok{1}\NormalTok{) \{}
      \ControlFlowTok{if}\NormalTok{ (bernoulli\_rng(lambda\_SR)) \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(neg\_binomial\_2\_log\_rng(log\_mu        , }\DecValTok{1}\NormalTok{ / phi1));}
\NormalTok{      \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(neg\_binomial\_2\_log\_rng(log\_mu + omega, }\DecValTok{1}\NormalTok{ / phi2));}
\NormalTok{      \}}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
       \ControlFlowTok{if}\NormalTok{ (bernoulli\_rng(lambda\_OR)) \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(neg\_binomial\_2\_log\_rng(log\_mu        , }\DecValTok{1}\NormalTok{ / phi1));}
\NormalTok{      \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{        log\_reading\_time\_pred[n]}
\NormalTok{          = log(neg\_binomial\_2\_log\_rng(log\_mu + omega, }\DecValTok{1}\NormalTok{ / phi2));}
\NormalTok{      \}}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{stan}\NormalTok{(}\AttributeTok{file=}\StringTok{\textquotesingle{}stan\_programs/dat\_nb.stan\textquotesingle{}}\NormalTok{,}
            \AttributeTok{data=}\NormalTok{data, }\AttributeTok{seed=}\DecValTok{8438338}\NormalTok{,}
            \AttributeTok{warmup=}\DecValTok{1000}\NormalTok{, }\AttributeTok{iter=}\DecValTok{2024}\NormalTok{, }\AttributeTok{refresh=}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Unfortunately, the computational diagnostics show strong signs of
posterior multimodality.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diagnostics }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{extract\_hmc\_diagnostics}\NormalTok{(fit)}
\NormalTok{util}\SpecialCharTok{$}\FunctionTok{check\_all\_hmc\_diagnostics}\NormalTok{(diagnostics)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  All Hamiltonian Monte Carlo diagnostics are consistent with reliable
Markov chain Monte Carlo.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{samples\_dat\_nb }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{extract\_expectand\_vals}\NormalTok{(fit)}
\NormalTok{base\_samples }\OtherTok{\textless{}{-}}\NormalTok{ util}\SpecialCharTok{$}\FunctionTok{filter\_expectands}\NormalTok{(samples\_dat\_nb,}
                                       \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}nu\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}delta\_free\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}zeta\_free\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}omega\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}phi1\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}phi2\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}lambda\_SR\textquotesingle{}}\NormalTok{,}
                                         \StringTok{\textquotesingle{}lambda\_OR\textquotesingle{}}\NormalTok{),}
                                       \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{util}\SpecialCharTok{$}\FunctionTok{summarize\_expectand\_diagnostics}\NormalTok{(base\_samples)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The expectands nu, delta_free[5], delta_free[6], delta_free[7],
delta_free[9], delta_free[14], delta_free[15], zeta_free[1],
zeta_free[2], zeta_free[4], zeta_free[5], zeta_free[8], zeta_free[13],
zeta_free[14], zeta_free[15], zeta_free[16], zeta_free[18],
zeta_free[19], zeta_free[22], zeta_free[25], zeta_free[26],
zeta_free[27], zeta_free[28], zeta_free[29], zeta_free[30],
zeta_free[31], zeta_free[32], zeta_free[33], zeta_free[37],
zeta_free[39], omega, phi1, lambda_SR, lambda_OR triggered diagnostic
warnings.

The expectands nu, delta_free[5], delta_free[6], delta_free[7],
delta_free[9], delta_free[14], delta_free[15], zeta_free[1],
zeta_free[2], zeta_free[5], zeta_free[8], zeta_free[13], zeta_free[14],
zeta_free[16], zeta_free[18], zeta_free[22], zeta_free[25],
zeta_free[26], zeta_free[27], zeta_free[28], zeta_free[29],
zeta_free[30], zeta_free[31], zeta_free[32], zeta_free[33],
zeta_free[37], zeta_free[39], omega, phi1, lambda_SR, lambda_OR
triggered hat{R} warnings.

Split Rhat larger than 1.1 suggests that at least one of the Markov
chains has not reached an equilibrium.

The expectands nu, zeta_free[1], zeta_free[2], zeta_free[4],
zeta_free[8], zeta_free[15], zeta_free[18], zeta_free[19],
zeta_free[25], zeta_free[27], zeta_free[32], zeta_free[33] triggered
hat{ESS} warnings.

Small empirical effective sample sizes result in imprecise Markov chain
Monte Carlo estimators.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_rhats}\NormalTok{(base\_samples, }\AttributeTok{B=}\DecValTok{50}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-76-1.pdf}

The multimodality we see here is very similar to the multimodalities we
saw in the uncensored and censored log normal direct-access models. In
one of the modes \(\omega \rightarrow 0\) so that the two component
models collapse on top of each other to form a single peak.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_pairs\_by\_chain}\NormalTok{(samples\_dat\_nb[[}\StringTok{\textquotesingle{}omega\textquotesingle{}}\NormalTok{]], }\StringTok{\textquotesingle{}omega\textquotesingle{}}\NormalTok{,}
\NormalTok{                         samples\_dat\_nb[[}\StringTok{\textquotesingle{}lambda\_SR\textquotesingle{}}\NormalTok{]], }\StringTok{\textquotesingle{}lambda\_SR\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-77-1.pdf}

It appears that the negative binomial model is not as skewed as the
inverse gamma model. Instead it's retrodictive performance is more
similar to the log normal model.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), }\AttributeTok{mar=}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_hist\_quantiles}\NormalTok{(}\FunctionTok{lapply}\NormalTok{(samples\_dat1,}
                                \ControlFlowTok{function}\NormalTok{(s) s[}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{),]),}
                         \StringTok{\textquotesingle{}log\_reading\_time\_pred\textquotesingle{}}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.1}\NormalTok{,}
                         \AttributeTok{baseline\_values=}\FunctionTok{log}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{reading\_time),}
                         \AttributeTok{xlab=}\StringTok{\textquotesingle{}Log Reading Time / 1 ms\textquotesingle{}}\NormalTok{,}
                         \AttributeTok{main=}\StringTok{\textquotesingle{}Markov Chains 1 and 3 (Log Normal)\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 1437 predictive values (0.0%) fell below the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 23 predictive values (0.0%) fell above the binning.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_hist\_quantiles}\NormalTok{(}\FunctionTok{lapply}\NormalTok{(samples\_dat1,}
                                \ControlFlowTok{function}\NormalTok{(s) s[}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{),]),}
                         \StringTok{\textquotesingle{}log\_reading\_time\_pred\textquotesingle{}}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.1}\NormalTok{,}
                         \AttributeTok{baseline\_values=}\FunctionTok{log}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{reading\_time),}
                         \AttributeTok{xlab=}\StringTok{\textquotesingle{}Log Reading Time / 1 ms\textquotesingle{}}\NormalTok{,}
                         \AttributeTok{main=}\StringTok{\textquotesingle{}Markov Chains 2 and 4 (Log Normal)\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 18 predictive values (0.0%) fell below the binning.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_hist\_quantiles}\NormalTok{(}\FunctionTok{lapply}\NormalTok{(samples\_dat\_nb,}
                                \ControlFlowTok{function}\NormalTok{(s) s[}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{),]),}
                         \StringTok{\textquotesingle{}log\_reading\_time\_pred\textquotesingle{}}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.1}\NormalTok{,}
                         \AttributeTok{baseline\_values=}\FunctionTok{log}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{reading\_time),}
                         \AttributeTok{xlab=}\StringTok{\textquotesingle{}Log Reading Time / 1 ms\textquotesingle{}}\NormalTok{,}
                         \AttributeTok{main=}\StringTok{\textquotesingle{}Markov Chains 1 and 4 (Negative Binomial)\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 2649 predictive values (0.0%) fell below the binning.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_hist\_quantiles}\NormalTok{(}\FunctionTok{lapply}\NormalTok{(samples\_dat\_nb,}
                                \ControlFlowTok{function}\NormalTok{(s) s[}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{),]),}
                         \StringTok{\textquotesingle{}log\_reading\_time\_pred\textquotesingle{}}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.1}\NormalTok{,}
                         \AttributeTok{baseline\_values=}\FunctionTok{log}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{reading\_time),}
                         \AttributeTok{xlab=}\StringTok{\textquotesingle{}Log Reading Time / 1 ms\textquotesingle{}}\NormalTok{,}
                         \AttributeTok{main=}\StringTok{\textquotesingle{}Markov Chains 2 and 3 (Negative Binomial)\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 28651 predictive values (0.5%) fell below the binning.
\end{verbatim}

\begin{verbatim}
Warning in check_bin_containment(bin_min, bin_max, collapsed_values,
"predictive value"): 1 predictive value (0.0%) fell above the binning.
\end{verbatim}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-78-1.pdf}

Whenever a model appears to be too rigid to adequately fit the observed
data we may want to double check that the prior model isn't excluding
useful model configurations. In this case, the marginal posterior
distributions all contract well within the scope of the component prior
models. Consequently the negative binomial model really does appear to
be too rigid.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), }\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_expectand\_pushforward}\NormalTok{(samples\_dat\_nb[[}\StringTok{"nu"}\NormalTok{]],}
                                \DecValTok{50}\NormalTok{, }\AttributeTok{flim=}\FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\FloatTok{7.5}\NormalTok{),}
                                \AttributeTok{display\_name=}\StringTok{"nu"}\NormalTok{)}
\NormalTok{xs }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{4}\NormalTok{, }\FloatTok{7.5}\NormalTok{, }\FloatTok{0.01}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(xs, }\FunctionTok{dnorm}\NormalTok{(xs, }\FloatTok{5.76}\NormalTok{, }\FloatTok{0.50}\NormalTok{),}
      \AttributeTok{lwd=}\DecValTok{2}\NormalTok{, }\AttributeTok{col=}\NormalTok{util}\SpecialCharTok{$}\NormalTok{c\_light\_teal)}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_expectand\_pushforward}\NormalTok{(samples\_dat\_nb[[}\StringTok{"omega"}\NormalTok{]],}
                                \DecValTok{100}\NormalTok{, }\AttributeTok{flim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{2.5}\NormalTok{),}
                                \AttributeTok{display\_name=}\StringTok{"omega"}\NormalTok{)}
\NormalTok{xs }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{2.5}\NormalTok{, }\FloatTok{0.01}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(xs, }\FunctionTok{dnorm}\NormalTok{(xs, }\DecValTok{0}\NormalTok{, }\FloatTok{0.90}\NormalTok{),}
      \AttributeTok{lwd=}\DecValTok{2}\NormalTok{, }\AttributeTok{col=}\NormalTok{util}\SpecialCharTok{$}\NormalTok{c\_light\_teal)}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_expectand\_pushforward}\NormalTok{(samples\_dat\_nb[[}\StringTok{"phi1"}\NormalTok{]],}
                                \DecValTok{50}\NormalTok{, }\AttributeTok{flim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{15}\NormalTok{),}
                                \AttributeTok{display\_name=}\StringTok{"phi1"}\NormalTok{)}
\NormalTok{xs }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{15}\NormalTok{, }\FloatTok{0.01}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(xs, }\FunctionTok{dnorm}\NormalTok{(xs, }\DecValTok{0}\NormalTok{, }\FloatTok{3.89}\NormalTok{),}
      \AttributeTok{lwd=}\DecValTok{2}\NormalTok{, }\AttributeTok{col=}\NormalTok{util}\SpecialCharTok{$}\NormalTok{c\_light\_teal)}

\NormalTok{util}\SpecialCharTok{$}\FunctionTok{plot\_expectand\_pushforward}\NormalTok{(samples\_dat\_nb[[}\StringTok{"phi2"}\NormalTok{]],}
                                \DecValTok{100}\NormalTok{, }\AttributeTok{flim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{15}\NormalTok{),}
                                \AttributeTok{display\_name=}\StringTok{"phi2"}\NormalTok{)}
\NormalTok{xs }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{15}\NormalTok{, }\FloatTok{0.01}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(xs, }\FunctionTok{dnorm}\NormalTok{(xs, }\DecValTok{0}\NormalTok{, }\FloatTok{3.89}\NormalTok{),}
      \AttributeTok{lwd=}\DecValTok{2}\NormalTok{, }\AttributeTok{col=}\NormalTok{util}\SpecialCharTok{$}\NormalTok{c\_light\_teal)}
\end{Highlighting}
\end{Shaded}

\includegraphics{reading_times_files/figure-pdf/unnamed-chunk-79-1.pdf}

Over the decades statisticians have developed a wealth of discrete
models that we might consider instead of the negative binomial model. As
we move beyond more common models, however, these alternatives become
more obscure and require more statistical expertise to identify and then
implement robustly.

This leaves us with one last demonstration of the practicalities of
probabilistic model building. When our probabilistic vocabulary is
limited we can tell only simple stories. Sometimes these stories are
sufficient for the analysis at hand, and sometimes they're not. As we
become more fluent with probabilistic modeling techniques, however, we
can tell richer stories that allow us to implement increasing more
sophisticated analyses.

\section{Next Steps}\label{next-steps}

Although we were eventually able to develop an adequate model for the
observe reading times, our analysis does not have to end here.

For example the pairwise-comparison structure of our models is ripe for
generalization. If we wanted to account for poorly written items, for
example, then we could allow for item-specific \textbf{discrimination
parameters}, \begin{align*}
\mu_{2, ij}
=
\exp \left( \eta + \rho_{i} \cdot (
            \omega + \alpha_{i} - \beta_{j} ) \right).
\end{align*} The smaller \(\rho_{i}\) is, the less sensitive the reading
times of passages based on that template will be to the individual
reader skills.

Another potential direction for expanding our model is to couple the
individual item complexities and reader skills together into
\textbf{hierarchical models}. Provided that the individual behaviors are
approximately
\href{https://betanalpha.github.io/assets/case_studies/hierarchical_modeling.html\#2_Can_You_Take_Me_Hier(archical)}{exchangeable},
hierarchical models might allow us to extract more precise inferences
for the quantities of interest.

The opportunities only increase when we look beyond the reading time
data alone. The answers to the comprehension questions, for instance,
could be modeled jointly with the reading times. This would allow us to
capture how reading time behaviors are coupled to reader comprehension
and avoid excluding any subjects from the analysis at all. Sadly the
these responses have replaced by a placeholder character in the
available data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(raw\_data}\SpecialCharTok{$}\NormalTok{correct)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

   - 
2735 
\end{verbatim}

To proceed in this direction we would first need to do a bit of
statistical archaeology.

\section{Conclusion}\label{conclusion}

When an experiment is carefully designed, the resulting data generating
process might be reasonably well-approximated by black-box analysis
tools such as regression estimators. We can also derive well-behaved
inferences in these ideal settings by modeling the data generating
process directly.

More importantly, probabilistic modeling allows us to derive
well-behaved inferences when the experimental design or its
implementation falters and when the system that we're studying is more
complex then we might have initially assumed. In other words,
probabilistic modeling in uniquely suited to adapt to meet our
scientific goals. This way we don't have to adapt our scientific goals
to meet our analysis techniques.

\section*{Acknowledgements}\label{acknowledgements}
\addcontentsline{toc}{section}{Acknowledgements}

I thank Ted Gibson for graciously making the data available and Shravan
Vasishth for generously sharing his domain expertise as well as many
helpful comments.

A very special thanks to everyone supporting me on Patreon: Adam
Fleischhacker, Alejandro Morales, Alessandro Varacca, Alex D, Alexander
Noll, Amit, Andrea Serafino, Andrew Mascioli, Andrew Rouillard, Ara
Winter, Ari Holtzman, Austin Rochford, Aviv Keshet, Avraham Adler, Ben
Matthews, Ben Swallow, Benoit Essiambre, boot, Brendan Galdo, Bryan
Chang, Brynjolfur Gauti Jónsson, Cameron Smith, Canaan Breiss, Cat
Shark, Cathy Oliveri, Charles Naylor, Chase Dwelle, Chris Jones,
Christina Van Heer, Christopher Mehrvarzi, Colin Carroll, Colin
McAuliffe, Damien Mannion, dan mackinlay, Dan W Joyce, Dan Waxman, Dan
Weitzenfeld, Danny Van Nest, David Burdelski, Doug Rivers, Dr.~Jobo,
Dr.~Omri Har Shemesh, Dylan Maher, Dylan Spielman, Ebriand, Ed Cashin,
Edgar Merkle, Edoardo Marcora, Eric LaMotte, Erik Banek, Eugene O'Friel,
Felipe González, Fergus Chadwick, Finn Lindgren, Francesco Corona, Geoff
Rollins, Granville Matheson, Gregor Gorjanc, Guilherme Marthe, Hamed
Bastan-Hagh, haubur, Hector Munoz, Henri Wallen, hs, Hugo Botha, Håkan
Johansson, Ian Costley, idontgetoutmuch, Ignacio Vera, Ilaria
Prosdocimi, Isaac Vock, Isidor Belic, jacob pine, Jair Andrade, James C,
James Hodgson, James Wade, Janek Berger, Jarrett Byrnes, Jason Martin,
Jason Pekos, Jason Wong, jd, Jeff Burnett, Jeff Dotson, Jeff Helzner,
Jeffrey Erlich, Jerry Lin , Jessica Graves, Joe Sloan, John Flournoy,
Jonathan H. Morgan, Josh Knecht, JU, Julian Lee, June, Justin Bois,
Karim Naguib, Karim Osman, Konstantin Shakhbazov, Kristian Gårdhus
Wichmann, Kádár András, Lars Barquist, lizzie , LOU ODETTE, Luís F, Mads
Christian Hansen, Marek Kwiatkowski, Mariana Carmona, Mark Donoghoe,
Markus P., Daniel Edward Marthaler, Matthew, Matthew Kay, Matthieu
LEROY, Mattia Arsendi, Matěj, Maurits van der Meer, Max, Michael
Colaresi, Michael DeWitt, Michael Dillon, Michael Lerner, Mick Cooney,
MisterMentat , Márton Vaitkus, N Sanders, N.S. , Nathaniel Burbank,
Nicholas Cowie, Nick S, Nikita Karetnikov, Octavio Medina, Ole Rogeberg,
Oliver Crook, Patrick Kelley, Patrick Boehnke, Pau Pereira Batlle, Peter
Johnson, Pieter van den Berg , ptr, quasar, Ramiro Barrantes Reynolds,
Ravin Kumar, Raúl Peralta Lozada, Rex, Riccardo Fusaroli, Richard
Nerland, Robert Frost, Robert Goldman, Robert kohn, Robin Taylor, Ryan
Gan, Ryan Grossman, Ryan Kelly, S Hong, Sean Wilson, Sergiy Protsiv,
Seth Axen, shira, Simon Duane, Simon Lilburn, Simon Steiger, Simone,
Spencer Boucher, sssz, Stefan Lorenz, Stephen Lienhard, Steve Harris,
Steven Forrest, Stew Watts, Stone Chen, Susan Holmes, Svilup, Tate
Tunstall, Tatsuo Okubo, Teresa Ortiz, Theodore Dasher, Thomas Siegert,
Thomas Vladeck, Tobychev , Tomas Capretto, Tony Wuersch, Virgile
Andreani, Virginia Fisher, Vitalie Spinu, Vladimir Markov, VO2 Maximus
Decius, Will Farr, Will Lowe, Will Wen, woejozney, yolhaj , yureq , Zach
A, and Zhengchen Cai.

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-BoxEtAl:2005}
Box, George E. P., J. Stuart Hunter, and William G. Hunter. 2005.
\emph{Statistics for Experimenters}. Second. Wiley Series in Probability
and Statistics. Wiley-Interscience {[}John Wiley \& Sons{]}, Hoboken,
NJ.

\bibitem[\citeproctext]{ref-GibsonEtAl:2013}
Gibson, Edward, and H.-H. Iris Wu. 2013. {``Processing Chinese Relative
Clauses in Context.''} \emph{Language and Cognitive Processes} 28 (1-2):
125--55.

\bibitem[\citeproctext]{ref-NicenboimEtAl:2025}
Nicenboim, Bruno, Daniel J Schad, and Shravan Vasishth. 2025.
\emph{Introduction to Bayesian Data Analysis for Cognitive Science}. 1st
ed. Chapman \& Hall/CRC Statistics in the Social and Behavioral
Sciences. London, England: Chapman; Hall.

\bibitem[\citeproctext]{ref-NicenboimEtAl:2018}
Nicenboim, Bruno, and Shravan Vasishth. 2018. {``Models of Retrieval in
Sentence Comprehension: A Computational Evaluation Using Bayesian
Hierarchical Modeling.''} \emph{Journal of Memory and Language} 99:
1--34.

\bibitem[\citeproctext]{ref-VasishthEtAl:2017}
Vasishth, Shravan, Nicolas Chopin, Robin Ryder, and Bruno Nicenboim.
2017. {``Modelling Dependency Completion in Sentence Comprehension as a
Bayesian Hierarchical Mixture Process: A Case Study Involving Chinese
Relative Clauses,''} February.

\end{CSLReferences}

\section*{License}\label{license}
\addcontentsline{toc}{section}{License}

A repository containing all of the files used to generate this chapter
is available on
\href{https://github.com/betanalpha/quarto_case_studies/tree/main/reading_times}{GitHub}.

The code in this case study is copyrighted by Michael Betancourt and
licensed under the new BSD (3-clause) license:

\url{https://opensource.org/licenses/BSD-3-Clause}

The text and figures in this chapter are copyrighted by Michael
Betancourt and licensed under the CC BY-NC 4.0 license:

\url{https://creativecommons.org/licenses/by-nc/4.0/}

\section*{Original Computing
Environment}\label{original-computing-environment}
\addcontentsline{toc}{section}{Original Computing Environment}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{writeLines}\NormalTok{(}\FunctionTok{readLines}\NormalTok{(}\FunctionTok{file.path}\NormalTok{(}\FunctionTok{Sys.getenv}\NormalTok{(}\StringTok{"HOME"}\NormalTok{), }\StringTok{".R/Makevars"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
CC=clang

CXXFLAGS=-O3 -mtune=native -march=native -Wno-unused-variable -Wno-unused-function -Wno-macro-redefined -Wno-unneeded-internal-declaration
CXX=clang++ -arch x86_64 -ftemplate-depth-256

CXX14FLAGS=-O3 -mtune=native -march=native -Wno-unused-variable -Wno-unused-function -Wno-macro-redefined -Wno-unneeded-internal-declaration -Wno-unknown-pragmas
CXX14=clang++ -arch x86_64 -ftemplate-depth-256
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sessionInfo}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
R version 4.3.2 (2023-10-31)
Platform: x86_64-apple-darwin20 (64-bit)
Running under: macOS 15.7.3

Matrix products: default
BLAS:   /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib 
LAPACK: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

time zone: America/New_York
tzcode source: internal

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] colormap_0.1.4     rstan_2.32.6       StanHeaders_2.32.7

loaded via a namespace (and not attached):
 [1] gtable_0.3.4       jsonlite_1.8.8     compiler_4.3.2     Rcpp_1.0.11       
 [5] stringr_1.5.1      parallel_4.3.2     gridExtra_2.3      scales_1.3.0      
 [9] yaml_2.3.8         fastmap_1.1.1      ggplot2_3.4.4      R6_2.6.1          
[13] curl_5.2.0         knitr_1.45         tibble_3.2.1       munsell_0.5.0     
[17] pillar_1.9.0       rlang_1.1.2        utf8_1.2.4         V8_4.4.1          
[21] stringi_1.8.3      inline_0.3.19      xfun_0.41          RcppParallel_5.1.7
[25] cli_3.6.2          magrittr_2.0.3     digest_0.6.33      grid_4.3.2        
[29] lifecycle_1.0.4    vctrs_0.6.5        evaluate_0.23      glue_1.6.2        
[33] QuickJSR_1.0.8     codetools_0.2-19   stats4_4.3.2       pkgbuild_1.4.3    
[37] fansi_1.0.6        colorspace_2.1-0   rmarkdown_2.25     matrixStats_1.2.0 
[41] tools_4.3.2        loo_2.6.0          pkgconfig_2.0.3    htmltools_0.5.7   
\end{verbatim}



\end{document}
